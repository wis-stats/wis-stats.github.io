[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference at WIS",
    "section": "",
    "text": "About the course\nWelcome to Statistical Inference, an two-week summer course to train biologists in data science and statistical inference. The course runs August 24–September 4, 2025. This webpage has all of the materials for the course.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#personnel",
    "href": "index.html#personnel",
    "title": "Statistical Inference at WIS",
    "section": "Personnel",
    "text": "Personnel\n\nCourse instructor: Justin Bois is a teaching professor in the Division of Biology and Biological Engineering at Caltech, where he teaches a variety of courses, including courses on data analysis in the biological sciences.\nTeaching assistant: Niv Cohen is a Ph.D. student in the Systems Immunology Department at Weizmann.\nTeaching assistant: Hamish Pike is a postdoc in the Systems Immunology Department at Weizmann.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#course-structure-and-schedule",
    "href": "index.html#course-structure-and-schedule",
    "title": "Statistical Inference at WIS",
    "section": "Course structure and schedule",
    "text": "Course structure and schedule\nWe meet every day, Sunday through Thursday, 9:00–13:00, in the Drory Auditorium in the Weissman Building for a total of ten days. Each day is split into an instructional and practical section. In the instructional section, topics are introduced and discussion in a lecture and/or follow-along format. In the practical sections, students apply the concepts in exercises. The topics of the sessions are below.\n\n\nData practicalities\n\nSun, August 24: What are we doing?\nSun, August 24: Polars and split-apply-combine\nSun, August 24: Data display\n\nTheory\n\nMon, August 25: Probability review\nMon, August 25: Sampling out of probability distributions\nTue, August 26: Sampling with Markov chain Monte Carlo\nWed, August 27: Bayesian modeling and inference (with prior predictive checks)\n\nTechniques\n\nWed, August 27: Statistical inference by Markov chain Monte Carlo\nThu, August 28: Model assessment and principled workflows\nSun, August 31: Summarizing posteriors with optimization\n\nSpecific models\n\nMon, September 1: Mixture models\nMon, September 1: Variate-covariate models\nTue, September 2: Hierarchical models\nWed, September 3: Principal component analysis\nWed, September 3: Probabilistic PCA and factor analysis\nThu, September 4: Hidden Markov models\nThu, September 4: Generalized linear models",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#copyright-and-license",
    "href": "index.html#copyright-and-license",
    "title": "Statistical Inference at WIS",
    "section": "Copyright and License",
    "text": "Copyright and License\nCopyright 2025, Justin Bois.\nWith the exception of pasted graphics, where the source is noted, this work is licensed under a Creative Commons Attribution License CC BY-NC-SA 4.0. All code contained herein is licensed under an MIT license.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "lessons/what_are_we_doing/what_are_we_doing.html",
    "href": "lessons/what_are_we_doing/what_are_we_doing.html",
    "title": "1  What are we doing?",
    "section": "",
    "text": "1.1 Data analysis pipelines\nIt is always good to start a course thinking about exactly what it is we are doing. Toward that end, we start with a question. What is the goal of doing (biological) experiments? There are many answers you may have for this. Some examples:\nMore obnoxious answers are\nThis question might be better addressed if we zoom out a bit and think about the scientific process as a whole. Below, we have a sketch of the scientific processes. This cycle repeats itself as we explore nature and learn more. In the boxes are milestones, and along the arrows in orange text are the tasks that get us to these milestones.\nWe start to the left with a hypothesis of how a biological system works, often expressed as a sketch, or cartoon, which is something we can have intuition about. Indeed, it is often a cartoon of how a system functions that we have in mind. From these cartoons, we can use deductive inference to move our model from a cartoon to a prediction. We then perform experiments to test our predictions, that is to assault our hypotheses, and the output of experiments is data. Once we have our data, we make inferences from them that enable us to update our hypothesis, thereby pushing human knowledge forward.\nLet’s consider the tasks and their milestones in a little more depth. We start in the lower left.\nLet’s consider the lower right arrow in the figure above, going from a data set to updated hypotheses and parameter estimates, the heart of this course. We zoom in on that arrow, there are several steps, which we will call a data analysis pipeline, depicted below.\nLet’s look at each step.\nIn this course, we will learn about all of these steps. We will spend roughly a third of the class on data validation and wrangling and exploratory data analysis, and the rest of the class on statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are we doing?</span>"
    ]
  },
  {
    "objectID": "lessons/what_are_we_doing/what_are_we_doing.html#data-analysis-pipelines",
    "href": "lessons/what_are_we_doing/what_are_we_doing.html#data-analysis-pipelines",
    "title": "1  What are we doing?",
    "section": "",
    "text": "Figure 1.2: Data analysis pipeline. Each block represents a milestone along the pipeline and each arrow a task to bring you there.\n\n\n\n\n\nData validation. We always have assumptions about data coming from a data source into the pipeline. We have assumptions about file formats, data organization, etc. We also have assumptions about the data themselves. For example, fluorescence measurements must all be nonnegative. Before proceeding through the pipeline, we need to make sure the data comply with all of these assumptions, lest we have issues down the pipeline. This process is called data validation.\nData wrangling. This is the process of converting the format of the validated data from the source to a format that is easier to work with. This could involved restructuring tabular data or extracting useful information out of images, etc. There are countless examples.\nExploratory data analysis. Once data sets are tidy and easy to work with, we can start exploring them. Generally, this involves making informative graphics looking at the data set from different angles. Sometimes, this is sufficient to learn from the data, and we can proceed directly to updated hypotheses and publication, but we often also need to perform statistical inference, which is in the next two steps of the pipeline.\nParameter estimation. This is the practice of quantifying the observed effects in the data, and in particular quantifying the uncertainty in our estimates of the effect sizes. This falls under the umbrella of statistical inference.\nHypothesis testing. Also a technique of statistical inference, hypothesis testing involves evaluation of hypotheses about how the data were generated. This usually gives some quantitative measure about the hypotheses, but there are many issues with quantifying the “truth” of a hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are we doing?</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/polars.html",
    "href": "lessons/exploratory/polars.html",
    "title": "Polars and split-apply-combine",
    "section": "",
    "text": "Throughout your research career, you will undoubtedly need to handle data, possibly lots of data. Once in a usable form, you are empowered to make graphics, perform statistical inference, and use your data in AI applications. Tidy data is an important format which we will define and work with in this lesson.\nIn an ideal world, data sets would be stored in tidy format and be ready to use. But for practical considerations (and sadly also due to experimenter and instrument manufacturer thoughtlessness) data comes in lots of formats, and you may have to spend much of your time wrangling the data to get it into a usable format. The process of getting data into useful formats is called wrangling. We will not dive into that topic, but will assume data is in tidy, or at least easily useable, format.\nIn this lesson, we will use Polars to manipulate tidy data sets.",
    "crumbs": [
      "Polars and split-apply-combine"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html",
    "href": "lessons/exploratory/intro_to_polars.html",
    "title": "2  Introduction to data frames and Polars",
    "section": "",
    "text": "2.1 The data set\n| Download notebook\nData set download\nThroughout your research career, you will undoubtedly need to handle data, possibly lots of data. Once in a usable form, you are empowered to rapidly make graphics and perform statistical inference. Tidy data is an important format and we will discuss that in subsequent sections of this lesson. In an ideal world, data sets would be stored in tidy format and be ready to use. The data comes in lots of formats, and you may have to spend much of your time wrangling the data to get it into a usable format. Wrangling is the topic of the next lesson; for now all data sets will be in tidy format from the get-go.\nWe will explore using data frames with a real data set. We will use a data set published in Beattie, et al., Perceptual impairment in face identification with poor sleep, Royal Society Open Science, 3, 160321, 2016. In this paper, researchers used the Glasgow Facial Matching Test (GMFT) to investigate how sleep deprivation affects a subject’s ability to match faces, as well as the confidence the subject has in those matches. Briefly, the test works by having subjects look at a pair of faces. Two such pairs are shown below.\nThe top two pictures are the same person, the bottom two pictures are different people. For each pair of faces, the subject gets as much time as he or she needs and then says whether or not they are the same person. The subject then rates his or her confidence in the choice.\nIn this study, subjects also took surveys to determine properties about their sleep. The Sleep Condition Indicator (SCI) is a measure of insomnia disorder over the past month (scores of 16 and below indicate insomnia). The Pittsburgh Sleep Quality Index (PSQI) quantifies how well a subject sleeps in terms of interruptions, latency, etc. A higher score indicates poorer sleep. The Epworth Sleepiness Scale (ESS) assesses daytime drowsiness.\nThe data set can be downloaded here. The contents of this file were adapted from the Excel file posted on the public Dryad repository. (Note this: if you want other people to use and explore your data, make it publicly available.) I’ll say it more boldly.\nThe data file is a CSV file, where CSV stands for comma-separated value. This is a text file that is easily read into data structures in many programming languages. You should generally always store your data in such a format, not necessarily CSV, but a format that is open, has a well-defined specification, and is readable in many contexts. Excel files do not meet these criteria. Neither do .mat files. There are other good ways to store data, such as JSON, but we will almost exclusively use CSV files in this class.\nLet’s take a look at the CSV file. We will use the command line program head to look at the first 20 lines of the file.\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\n\n# This will not work in Colab because the file is not local\n!head {fname}\n\n﻿participant number,gender,age,correct hit percentage,correct reject percentage,percent correct,confidence when correct hit,confidence incorrect hit,confidence correct reject,confidence incorrect reject,confidence when correct,confidence when incorrect,sci,psqi,ess\n8,f,39,65,80,72.5,91,90,93,83.5,93,90,9,13,2\n16,m,42,90,90,90,75.5,55.5,70.5,50,75,50,4,11,7\n18,f,31,90,95,92.5,89.5,90,86,81,89,88,10,9,3\n22,f,35,100,75,87.5,89.5,*,71,80,88,80,13,8,20\n27,f,74,60,65,62.5,68.5,49,61,49,65,49,13,9,12\n28,f,61,80,20,50,71,63,31,72.5,64.5,70.5,15,14,2\n30,m,32,90,75,82.5,67,56.5,66,65,66,64,16,9,3\n33,m,62,45,90,67.5,54,37,65,81.5,62,61,14,9,9\n34,f,33,80,100,90,70.5,76.5,64.5,*,68,76.5,14,12,10\nThe first line contains the headers for each column. They are participant number, gender, age, etc. The data follow. There are two important things to note here. First, notice that the gender column has string data (m or f), while the rest of the data are numeric. Note also that there are some missing data, denoted by the *s in the file.\nGiven the file I/O skills you recently learned, you could write some functions to parse this file and extract the data you want. You can imagine that this might be kind of painful. However, if the file format is nice and clean, like we more or less have here, we can use pre-built tools to read in the data from the file and put it in a convenient data structure. Those structures are data frames.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#the-data-set",
    "href": "lessons/exploratory/intro_to_polars.html#the-data-set",
    "title": "2  Introduction to data frames and Polars",
    "section": "",
    "text": "Figure 2.1: Two pairs of faces from the Glasgow Facial Matching Test (GFMT).\n\n\n\n\n\n\n\nIf at all possible, share your data freely.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#data-frames",
    "href": "lessons/exploratory/intro_to_polars.html#data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.2 Data frames",
    "text": "2.2 Data frames\nThough we will use the word “data frame” over and over again, what a data frame is is actually a bit nebulous. Our working definition of a data frame is that it is a representation of two-dimensional tabular data where each column has a label. We will restrict ourselves to the case where each column has a specific data type (e.g., strings, ints, floats, or even lists).\nOne can think of a data frame as a collection of labeled columns, each one called a series. (A series may be thought of as a single column of data.) Alternatively, it is sometimes convenient to think of a data frame as a collection of rows, where each entry in the row is labeled with the column heading.\nFor more reading on the history of data frames and an attempt (in my opinion a very good attempt) at clearly defining them see section 4 of this paper by Petersohn, et al.\n\n2.2.1 Pandas\nPandas is one of the most widely used tools in the Python ecosystem for handling data. It is worth knowing about. We will, however, not be using Pandas, but instead will use Polars. I prefer Polars because its API is cleaner, in my opinion, but it has the added benefit of generally being much faster than Pandas.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#loading-the-data",
    "href": "lessons/exploratory/intro_to_polars.html#loading-the-data",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.3 Loading the data",
    "text": "2.3 Loading the data\n\n2.3.1 Using polars.read_csv() to read in data\nWe have imported Polars with the alias pl as is customary. We will use pl.read_csv() to load the data set. The data are stored in a data frame (data type DataFrame), which is one of the data types that makes Polars so convenient for use in data analysis. Data frames offer mixed data types, including incomplete columns, and convenient slicing, among many, many other convenient features. We will use the data frame to look at the data, at the same time demonstrating some of the power of data frames. They are like spreadsheets, only a lot better.\n\ndf = pl.read_csv(fname, null_values=\"*\")\n\nNotice that we used the kwarg null_values=* to specify that entries marked with a * are missing. The resulting data frame is populated with null, wherever this character is present in the file. In this case, we want null_values='*'. So, let’s load in the data set.\nIf you check out the doc string for pl.read_csv(), you will see there are lots of options for reading in the data.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#exploring-the-dataframe",
    "href": "lessons/exploratory/intro_to_polars.html#exploring-the-dataframe",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.4 Exploring the DataFrame",
    "text": "2.4 Exploring the DataFrame\nLet’s jump right in and look at the contents of the DataFrame. We can look at the first several rows using the df.head() method.\n\n# Look at the contents (first 5 rows)\ndf.head()\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nWe see that the column headings were automatically assigned, as have the data types of the columns, where i64, f64, and str respectively denote integers, floats and strings. Also note (in row 3) that the missing data are denoted as null.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#indexing-data-frames",
    "href": "lessons/exploratory/intro_to_polars.html#indexing-data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.5 Indexing data frames",
    "text": "2.5 Indexing data frames\nThe data frame is a convenient data structure for many reasons that will become clear as we start exploring. Let’s start by looking at how data frames are indexed. The rows in Polars data frames are indexed by integers, starting with zero as usual for Python. So, the first row of the data frame may be accessed as follows.\n\ndf[0]\n\n\nshape: (1, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n\n\n\n\nIn practice you will almost never use row indices, but rather use Boolean indexing, which is accomplished using Polars’s filter() method.\nBecause row indices in Polars data frames are always integers and column indices are not allowed to be integers (they must be strings), columns are accessed in the same way. If you choose to index with a string, Polars knows you are asking for a column.\n\ndf['percent correct']\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5\n\n\n\n\n\n\nFor accessing a single column, I prefer the get_column() method.\n\ndf.get_column('percent correct')\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#boolean-indexing-of-data-frames",
    "href": "lessons/exploratory/intro_to_polars.html#boolean-indexing-of-data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.6 Boolean indexing of data frames",
    "text": "2.6 Boolean indexing of data frames\nLet’s say I wanted the record for participant number 42. I can use Boolean indexing to specify the row. This is accomplished using the filter() method.\n\ndf.filter(pl.col(\"participant number\") == 42)\n\n\nshape: (1, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n42\n\"m\"\n29\n100\n70\n85.0\n75.0\nnull\n64.5\n43.0\n74.0\n43.0\n32\n1\n6\n\n\n\n\n\n\nThe argument of the filter() method is an expression, pl.col('participant number') == 42, which gives the rows (in this case, one row) for which the value of the 'participant number' column is 42.\nIf I just wanted the percent correct, I can first filter to get the row I want, then extract the 'percent correct' column, and then use the item() method to extract the scalar value.\n\n(\n    df\n    .filter(pl.col('participant number') == 42)\n    .get_column('percent correct')\n    .item()\n)\n\n85.0\n\n\nNote how I expressed this code snippet stylistically. I am doing method chaining, and having each method on its own line adds readability.\nNow, let’s pull out all records of females under the age of 21. We can again use Boolean indexing, but we need to use an & operator, taken to mean logical AND. We did not cover this bitwise operator before, but the syntax is self-explanatory in the example below. Note that it is important that each Boolean operation you are doing is in parentheses because of the precedence of the operators involved. The other bitwise operators you may wish to use for Boolean indexing in data frames are | for OR and ~ for NOT.\n\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n3\n\"f\"\n16\n70\n80\n75.0\n70.0\n57.0\n54.0\n53.0\n57.0\n54.5\n23\n1\n3\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n58\n\"f\"\n16\n85\n85\n85.0\n55.0\n30.0\n50.0\n40.0\n52.5\n35.0\n29\n2\n11\n\n\n72\n\"f\"\n18\n80\n75\n77.5\n67.5\n51.5\n66.0\n57.0\n67.0\n53.0\n29\n4\n6\n\n\n85\n\"f\"\n18\n85\n85\n85.0\n93.0\n92.0\n91.0\n89.0\n91.5\n91.0\n25\n4\n21\n\n\n\n\n\n\nWe can do something even more complicated, like pull out all females under 30 who got more than 85% of the face matching tasks correct.\n\ndf.filter(\n      (pl.col('gender') == 'f') \n    & (pl.col('age') &lt; 30) \n    & (pl.col('percent correct') &gt; 85.0)\n)\n\n\nshape: (8, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n93\n\"f\"\n28\n100\n75\n87.5\n89.5\nnull\n67.0\n60.0\n80.0\n60.0\n16\n7\n4\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n6\n\"f\"\n28\n95\n80\n87.5\n100.0\n85.0\n94.0\n61.0\n99.0\n65.0\n19\n7\n12\n\n\n10\n\"f\"\n25\n100\n100\n100.0\n90.0\nnull\n85.0\nnull\n90.0\nnull\n17\n10\n11\n\n\n44\n\"f\"\n21\n85\n90\n87.5\n66.0\n29.0\n70.0\n29.0\n67.0\n29.0\n26\n7\n18\n\n\n48\n\"f\"\n23\n90\n85\n87.5\n67.0\n47.0\n69.0\n40.0\n67.0\n40.0\n18\n6\n8\n\n\n51\n\"f\"\n24\n85\n95\n90.0\n97.0\n41.0\n74.0\n73.0\n83.0\n55.5\n29\n1\n7\n\n\n67\n\"f\"\n25\n100\n100\n100.0\n61.5\nnull\n58.5\nnull\n60.5\nnull\n28\n8\n9\n\n\n\n\n\n\nOf interest in this exercise in Boolean indexing is that we never had to write a loop. To produce our indices, we could have done the following.\n\n# Initialize array of Boolean indices\ninds = []\n\n# Iterate over the rows of the DataFrame to check if the row should be included\nfor row in df.iter_rows(named=True):\n    inds.append(    \n            row[\"age\"] &lt; 30 \n        and row[\"gender\"] == \"f\" \n        and row[\"percent correct\"] &gt; 85\n    )\n\n# Look at inds\nprint(inds)\n\n[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n\n\nIf we wanted, we could use this inds list of Trues and Falses to filter our values.\n\ndf.filter(inds)\n\n\nshape: (8, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n93\n\"f\"\n28\n100\n75\n87.5\n89.5\nnull\n67.0\n60.0\n80.0\n60.0\n16\n7\n4\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n6\n\"f\"\n28\n95\n80\n87.5\n100.0\n85.0\n94.0\n61.0\n99.0\n65.0\n19\n7\n12\n\n\n10\n\"f\"\n25\n100\n100\n100.0\n90.0\nnull\n85.0\nnull\n90.0\nnull\n17\n10\n11\n\n\n44\n\"f\"\n21\n85\n90\n87.5\n66.0\n29.0\n70.0\n29.0\n67.0\n29.0\n26\n7\n18\n\n\n48\n\"f\"\n23\n90\n85\n87.5\n67.0\n47.0\n69.0\n40.0\n67.0\n40.0\n18\n6\n8\n\n\n51\n\"f\"\n24\n85\n95\n90.0\n97.0\n41.0\n74.0\n73.0\n83.0\n55.5\n29\n1\n7\n\n\n67\n\"f\"\n25\n100\n100\n100.0\n61.5\nnull\n58.5\nnull\n60.5\nnull\n28\n8\n9\n\n\n\n\n\n\nThis feature, where the looping is done automatically on Polars objects like data frames, is very powerful and saves us writing lots of lines of code. This example also showed how to use the iter_rows() method of a data frame. It is actually rare that you will need to do that, and you should generally avoid it, since it is also slow.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#contexts-and-expressions",
    "href": "lessons/exploratory/intro_to_polars.html#contexts-and-expressions",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.7 Contexts and expressions",
    "text": "2.7 Contexts and expressions\nWe will now be a bit more formal in discussing how to work with Polars data frames. Specifically, Polars features the concepts of expressions and contexts.\n\n2.7.1 The filter context\nAs an example, let us consider our above task of filtering the data frame to extract females under the age of 21. The syntax was\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\nConsider first pl.col('gender') == 'f'. This is an example of an expression. An expression consists of a calculation or transformation that can be applied to a series and returns a series. In this case, we are taking a column called 'gender' and we are evaluating whether each element in that column is 'f'. Indeed, if we ask the Python interpreter to tell us the type of the above expression, it is a Polars Expr.\n\ntype(pl.col('gender') == 'f')\n\npolars.expr.expr.Expr\n\n\nSimilarly, pl.col('age') &lt; 21 is also an expression, as is the result when we apply the & bitwise operator.\n\ntype((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\npolars.expr.expr.Expr\n\n\nSo, an expression says what we want to do to data. But now we ask, in what way, i.e., in what context, do we want to use the result of the expression? One way we may wish to use the above expression is to filter the rows in a data frame. The filter context is established by df.filter(). The argument of df.filter() is an expression (or expressions) that evaluate to Booleans. That is how we got our result; in the context of filtering, the expression is evaluated and only entries where the expression gives True are retained.\n\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n3\n\"f\"\n16\n70\n80\n75.0\n70.0\n57.0\n54.0\n53.0\n57.0\n54.5\n23\n1\n3\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n58\n\"f\"\n16\n85\n85\n85.0\n55.0\n30.0\n50.0\n40.0\n52.5\n35.0\n29\n2\n11\n\n\n72\n\"f\"\n18\n80\n75\n77.5\n67.5\n51.5\n66.0\n57.0\n67.0\n53.0\n29\n4\n6\n\n\n85\n\"f\"\n18\n85\n85\n85.0\n93.0\n92.0\n91.0\n89.0\n91.5\n91.0\n25\n4\n21\n\n\n\n\n\n\n\n\n2.7.2 The selection context\nThe simplest way we can use an expression is simply to evaluate the expression and give its result as a new data frame. This is the selection context, in which we get the output of the expression. It can be invoked with df.select().\n\ndf.select((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (102, 1)\n\n\n\ngender\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nNote that this is a data frame and not a series; it is a data frame containing one column. In this case, the column is named after the first column used in our Boolean expression. We can adjust the column label by applying the alias() method, which does a renaming transformation.\n\nf_under_21 = (pl.col('gender') == 'f') & (pl.col('age') &lt; 21)\n\ndf.select(f_under_21.alias('female under 21'))\n\n\nshape: (102, 1)\n\n\n\nfemale under 21\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nIf we wanted a series instead of a new data frame, we can apply the get_column() method to the data frame returned by df.select().\n\n# Result of expression as a series\ndf.select(f_under_21.alias('female under 21')).get_column('female under 21')\n\n\nshape: (102,)\n\n\n\nfemale under 21\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nWe can also select with multiple expressions. For example, let’s say we additionally wanted to compute the ratio of confidence when correct to confidence when incorrect. First, we can make an expression for that.\n\nconf_ratio = pl.col('confidence when correct') / pl.col('confidence when incorrect')\n\nNow, we can select that as well as the 'female under 21' column.\n\ndf.select(\n    f_under_21.alias('female under 21'),\n    conf_ratio.alias('confidence ratio')\n)\n\n\nshape: (102, 2)\n\n\n\nfemale under 21\nconfidence ratio\n\n\nbool\nf64\n\n\n\n\nfalse\n1.033333\n\n\nfalse\n1.5\n\n\nfalse\n1.011364\n\n\nfalse\n1.1\n\n\nfalse\n1.326531\n\n\n…\n…\n\n\nfalse\n1.040541\n\n\nfalse\n0.925\n\n\nfalse\n0.802469\n\n\nfalse\n1.588235\n\n\nfalse\n1.109589\n\n\n\n\n\n\nNotice that df.select() returns a new data frame containing only the columns that are given by the expressions and the original data frame is discarded. If we want the results of the expressions to instead be added to the data frame (keeping all of its original columns), we use the df.with_columns() method. This is still a selection context; the output is just different, comprising of the original data frame with added columns. (In the output of the cell below, you will find the columns added to the far right of the data frame.)\n\ndf.with_columns(\n    f_under_21.alias('female under 21'),\n    conf_ratio.alias('confidence ratio')\n)\n\n\nshape: (102, 17)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\nfemale under 21\nconfidence ratio\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\nf64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\nfalse\n1.033333\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\nfalse\n1.5\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\nfalse\n1.011364\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\nfalse\n1.1\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\nfalse\n1.326531\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n97\n\"f\"\n23\n70\n85\n77.5\n77.0\n66.5\n77.0\n77.5\n77.0\n74.0\n20\n8\n10\nfalse\n1.040541\n\n\n98\n\"f\"\n70\n90\n85\n87.5\n65.5\n85.5\n87.0\n80.0\n74.0\n80.0\n19\n8\n7\nfalse\n0.925\n\n\n99\n\"f\"\n24\n70\n80\n75.0\n61.5\n81.0\n70.0\n61.0\n65.0\n81.0\n31\n2\n15\nfalse\n0.802469\n\n\n102\n\"f\"\n40\n75\n65\n70.0\n53.0\n37.0\n84.0\n52.0\n81.0\n51.0\n22\n4\n7\nfalse\n1.588235\n\n\n103\n\"f\"\n33\n85\n40\n62.5\n80.0\n27.0\n31.0\n82.5\n81.0\n73.0\n24\n5\n7\nfalse\n1.109589\n\n\n\n\n\n\nFinally, we will do something we’ll want to use going forward. Recall that a subject is said to suffer from insomnia if he or she has an SCI of 16 or below. We might like to add a column to the data frame that specifies whether or not the subject suffers from insomnia, which we do in the code cell below. Note that until now, we have not updated our data frame. To actually update the data frame, we need an assignment operation, df = ....\n\n# Add a column with `True` for insomnia\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias('insomnia'))\n\n# Take a look (.head() gives first five rows)\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#polars-selectors",
    "href": "lessons/exploratory/intro_to_polars.html#polars-selectors",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.8 Polars selectors",
    "text": "2.8 Polars selectors\nWe have seen that we can choose which columns we want to work with in expressions using pl.col(). Thus far, we have used a single string as an argument, but pl.col() is more capable than that. For example, to select three columns of interest, we can do the following.\n\ndf.select(pl.col('age', 'gender', 'percent correct')).head()\n\n\nshape: (5, 3)\n\n\n\nage\ngender\npercent correct\n\n\ni64\nstr\nf64\n\n\n\n\n39\n\"f\"\n72.5\n\n\n42\n\"m\"\n90.0\n\n\n31\n\"f\"\n92.5\n\n\n35\n\"f\"\n87.5\n\n\n74\n\"f\"\n62.5\n\n\n\n\n\n\nWe can also pass regular expressions into pl.col(). If we want all columns, for example, we can use pl.col('*'). To get all columns containing the string 'confidence', we can use pl.col('^.*confidence.*$').\nPersonally, I always struggle with regular expressions. Fortunately, Polars has powerful selectors which help specify which columns are of interest. In addition to facilitating selection based on the names of the columns, selectors allow selection based on the data type of the column as well (actually, so does pl.col(), but it is simplified with selectors).\nWe have to import the selectors separately, which we have done in the top cell of this notebook via import polars.selectors as cs. The cs alias is suggested by the Polars developers.\nAs an example, we can select all columns that have a column heading containing the string 'confidence'.\n\ndf.select(cs.contains('confidence')).head()\n\n\nshape: (5, 6)\n\n\n\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nOr, perhaps we want to exclude all columns that are not indicators of performance on a test (participant number, age, and gender). We can specifically exclude columns with cs.exclude().\n\ndf.select(cs.exclude('gender', 'age', 'participant number')).head()\n\n\nshape: (5, 13)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n\n\n\n\n\n\nWhat if we want columns with only a float data type?\n\ndf.select(cs.float()).head()\n\n\nshape: (5, 7)\n\n\n\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nNote that the sleep measures were omitted because they are integer data types. We could select everything that is numeric if we want to include those.\n\ndf.select(cs.numeric()).head()\n\n\nshape: (5, 14)\n\n\n\nparticipant number\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n16\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n18\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n22\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n27\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nUnfortunately, this still gave us participant number and age again. We could exclude those explicitly by chaining methods.\n\ndf.select(cs.numeric().exclude('age', 'participant number')).head()\n\n\nshape: (5, 12)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nSelectors also allow for set algebra. As a (contrived) example, let say we want all columns that are not of string data type that have spaces in the column heading and that we also want to exclude the participant number.\n\ndf.select((~cs.string() & cs.contains(' ')).exclude('participant number')).head()\n\n\nshape: (5, 9)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nNotice that we used the complement operator ~ and the intersection operator &. Selectors also support the union operator | and the difference operator -.\nTo close our discussion on selectors, we note that selectors are their own data type:\n\ntype(cs.string())\n\npolars.selectors._selector_proxy_\n\n\nSelectors can be converted to expressions so you can continue computing with the as_expr() method.\n\ntype(cs.string().as_expr())\n\npolars.expr.expr.Expr",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#computing-summary-statistics",
    "href": "lessons/exploratory/intro_to_polars.html#computing-summary-statistics",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.9 Computing summary statistics",
    "text": "2.9 Computing summary statistics\nTo demonstrate a use of what we have learned so far, we can compute the mean percent correct for insomniacs and normal sleepers. We can filter the data frame according to the insomnia column, select the percent correct column and compute the mean.\nTo put them together in a new data frame, we make a dictionary and convert it to a data frame, which is one of the ways to make a Polars data frame. E.g.,\n\npl.DataFrame(dict(a=[1,2,3], b=[4.5, 5.5, 6.5], c=['one', 'two', 'three']))\n\n\nshape: (3, 3)\n\n\n\na\nb\nc\n\n\ni64\nf64\nstr\n\n\n\n\n1\n4.5\n\"one\"\n\n\n2\n5.5\n\"two\"\n\n\n3\n6.5\n\"three\"\n\n\n\n\n\n\nNow to make our data frame of means for insomniacs and normal sleepers.\n\npl.DataFrame(\n    {\n        'insomniacs': df.filter(pl.col('insomnia')).get_column('percent correct').mean(),\n        'normal sleepers': df.filter(~pl.col('insomnia')).get_column('percent correct').mean()\n    }\n)\n\n\nshape: (1, 2)\n\n\n\ninsomniacs\nnormal sleepers\n\n\nf64\nf64\n\n\n\n\n76.1\n81.461039\n\n\n\n\n\n\nNotice that I used the ~ operator, which is a bit switcher, to get the normal sleepers from the insomnia column. It changes all Trues to Falses and vice versa. In this case, it functions like NOT.\nIt appears as though normal sleepers score better than insomniacs. We will learn techniques to more quantitatively assess that claim when we learn about statistical inference.\nAs we will soon see, what we have done is a split-apply-combine operation, for which there are more elegant and efficient methods using Polars. You will probably never use the approach in the above code cell again.\nWe will do a lot more computing with Polars data frames as the course goes on. For a nifty demonstration demonstration in this lesson, we can quickly compute summary statistics about each column of a data frame using its describe() method.\n\ndf.describe()\n\n\nshape: (9, 17)\n\n\n\nstatistic\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\nstr\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n102.0\n\"102\"\n102.0\n102.0\n102.0\n102.0\n102.0\n84.0\n102.0\n93.0\n102.0\n99.0\n102.0\n102.0\n102.0\n102.0\n\n\n\"null_count\"\n0.0\n\"0\"\n0.0\n0.0\n0.0\n0.0\n0.0\n18.0\n0.0\n9.0\n0.0\n3.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n52.04902\nnull\n37.921569\n83.088235\n77.205882\n80.147059\n74.990196\n58.565476\n71.137255\n61.22043\n74.642157\n61.979798\n22.245098\n5.27451\n7.294118\n0.245098\n\n\n\"std\"\n30.020909\nnull\n14.02945\n15.09121\n17.569854\n12.047881\n14.165916\n19.560653\n14.987479\n17.671283\n13.619725\n15.92167\n7.547128\n3.404007\n4.426715\nnull\n\n\n\"min\"\n1.0\n\"f\"\n16.0\n35.0\n20.0\n40.0\n29.5\n7.0\n19.0\n17.0\n24.0\n24.5\n0.0\n0.0\n0.0\n0.0\n\n\n\"25%\"\n26.0\nnull\n26.0\n75.0\n70.0\n72.5\n66.0\n47.0\n64.5\n50.0\n66.0\n51.0\n17.0\n3.0\n4.0\nnull\n\n\n\"50%\"\n53.0\nnull\n37.0\n90.0\n80.0\n85.0\n75.0\n56.5\n71.5\n61.0\n76.0\n61.5\n24.0\n5.0\n7.0\nnull\n\n\n\"75%\"\n78.0\nnull\n45.0\n95.0\n90.0\n87.5\n87.0\n73.0\n80.0\n74.0\n82.5\n73.0\n29.0\n7.0\n10.0\nnull\n\n\n\"max\"\n103.0\n\"m\"\n74.0\n100.0\n100.0\n100.0\n100.0\n92.0\n100.0\n100.0\n100.0\n100.0\n32.0\n15.0\n21.0\n1.0\n\n\n\n\n\n\nThis gives us a data frame with summary statistics.\n\n2.9.1 Outputting a new CSV file\nNow that we added the insomniac column, we might like to save our data frame as a new CSV that we can reload later. We use df.write_csv() for this.\n\ndf.write_csv(\"gfmt_sleep_with_insomnia.csv\")\n\nLet’s take a look at what this file looks like.\n\n!head gfmt_sleep_with_insomnia.csv\n\nparticipant number,gender,age,correct hit percentage,correct reject percentage,percent correct,confidence when correct hit,confidence incorrect hit,confidence correct reject,confidence incorrect reject,confidence when correct,confidence when incorrect,sci,psqi,ess,insomnia\n8,f,39,65,80,72.5,91.0,90.0,93.0,83.5,93.0,90.0,9,13,2,true\n16,m,42,90,90,90.0,75.5,55.5,70.5,50.0,75.0,50.0,4,11,7,true\n18,f,31,90,95,92.5,89.5,90.0,86.0,81.0,89.0,88.0,10,9,3,true\n22,f,35,100,75,87.5,89.5,,71.0,80.0,88.0,80.0,13,8,20,true\n27,f,74,60,65,62.5,68.5,49.0,61.0,49.0,65.0,49.0,13,9,12,true\n28,f,61,80,20,50.0,71.0,63.0,31.0,72.5,64.5,70.5,15,14,2,true\n30,m,32,90,75,82.5,67.0,56.5,66.0,65.0,66.0,64.0,16,9,3,true\n33,m,62,45,90,67.5,54.0,37.0,65.0,81.5,62.0,61.0,14,9,9,true\n34,f,33,80,100,90.0,70.5,76.5,64.5,,68.0,76.5,14,12,10,true\n\n\nVery nice. Notice that by default Polars leaves an empty field for null values, and we do not need the null_values kwarg when we load in this CSV file.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#renaming-columns",
    "href": "lessons/exploratory/intro_to_polars.html#renaming-columns",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.10 Renaming columns",
    "text": "2.10 Renaming columns\nYou may be annoyed with the rather lengthy syntax of access column names and wish to change them. Actually, you probably do not want to do this. Explicit is better than implicit! And furthermore, high level plotting libraries, as we will soon see, often automatically use column names for axis labels. So, let’s instead lengthen a column name. Say we keep forgetting what ESS stands for an want to rename the ess column to “Epworth Sleepiness Scale.”\nData frames have a nice rename method to do this. To rename the columns, we provide a dictionary where the keys are current column names and the corresponding values are the names we which to check them to. While we are at it, we will choose descriptive names for all three of the sleep quality indices.\n\n# Make a dictionary to rename columns\nrename_dict = {\n    \"ess\": \"Epworth Sleepiness Scale\",\n    \"sci\": \"Sleep Condition Indicator\",\n    \"psqi\": \"Pittsburgh Sleep Quality Index\",\n}\n\n# Rename the columns\ndf = df.rename(rename_dict)\n\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nSleep Condition Indicator\nPittsburgh Sleep Quality Index\nEpworth Sleepiness Scale\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#a-note-on-indexing-and-speed",
    "href": "lessons/exploratory/intro_to_polars.html#a-note-on-indexing-and-speed",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.11 A note on indexing and speed",
    "text": "2.11 A note on indexing and speed\nAs we have seen Boolean indexing is very convenient and fits nicely into a logical framework which allows us to extract data according to criteria we want. The trade-off is speed. Slicing by Boolean indexing is essentially doing a reverse lookup in a dictionary. We have to loop through all values to find keys that match. This is much slower than directly indexing. Compare the difference in speed for indexing the percent correct by participant number 42 by Boolean indexing versus direct indexing (it’s row 54).\n\nprint(\"Boolean indexing:\")\n%timeit df.filter(pl.col('participant number') == 42)['percent correct'].item()\n\nprint(\"\\nDirect indexing:\")\n%timeit df[54, 'percent correct']\n\nBoolean indexing:\n100 μs ± 471 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nDirect indexing:\n533 ns ± 5.07 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe speed difference is stark, differing by two orders of magnitude. For larger data sets, or for analyses that require repeated indexing, this speed consideration may be important. However, Polars does optimization that takes full advantage of parallelization that will accelerate Boolean indexing.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/intro_to_polars.html#computing-environment",
    "href": "lessons/exploratory/intro_to_polars.html#computing-environment",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.12 Computing environment",
    "text": "2.12 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,polars,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\njupyterlab: 4.3.6",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html",
    "href": "lessons/exploratory/tidy_data.html",
    "title": "3  Tidy data and split-apply-combine",
    "section": "",
    "text": "3.1 Tidy data\n| Download notebook\nData set download\nWe have dipped our toe into Polars to see its power. In this lesson, we will continue to harness the power of Polars to pull out subsets of data we are interested in, and of vital importance, will introduce the concept of tidy data. I suspect this will be a demarcation in your life. You will have the times in your life before tidy data and after. Welcome to your bright tomorrow.\nHadley Wickham wrote a great article in favor of “tidy data.” Tidy data frames follow the rules: 1. Each variable is a column. 2. Each observation is a row. 3. Each type of observation has its own separate data frame.\nThis is less pretty to visualize as a table, but we rarely look at data in tables. Indeed, the representation of data which is convenient for visualization is different from that which is convenient for analysis. A tidy data frame is almost always much easier to work with than non-tidy formats.\nYou may raise some objections about tidy data. Here are a few, and my responses.\nObjection: Looking at a table of tidy data is ugly. It is not intuitively organized. I would almost never display a tidy data table in a publication.\nResponse: Correct! Having tabular data in a format that is easy to read as a human studying a table is a very different thing than having it in a format that is easy to explore and work with using a computer. As Daniel Chen put it, “There are data formats that are better for reporting and data formats that are better for analysis.” We are using the tidy data frames for analysis, not reporting (though we will see in the coming lessons that having the data in a tidy format makes making plots much easier, and plots are a key medium for reporting.)\nObjection: Isn’t it better to sometimes have data arranged in other ways? Say in a matrix?\nResponse: This is certainly true for things like images, or raster-style data in general. It makes more sense to organize an image in a 2D matrix than to have it organized as a data frame with three columns (row in image, column in image, intensity of pixel), where each row corresponds to a single pixel. For an image, indexing it by row and column is always unambiguous, my_image[i, j] means the pixel at row i and column j.\nFor other data, though, the matrix layout suffers from the fact that there may be more than one way to construct a matrix. If you know a data frame is tidy, you already know its structure. You need only to ask what the columns are, and then you immediately know how to access data using Boolean indexing. In other formats, you might have to read and write extensive comments to understand the structure of the data. Of course, you can read and write comments, but it opens the door for the possibility of misinterpretation or mistakes.\nObjection: But what about time series? Clearly, that can be in matrix format. One column is time, and then subsequent columns are observations made at that time.\nResponse: Yes, that is true. But then the matrix-style described could be considered tidy, since each row is a single observation (time point) that has many facets.\nObjection: Isn’t this an inefficient use of memory? There tend to be lots of repeated entries in tidy data frames.\nResponse: Yes, there are more efficient ways of storing and accessing data. But for data sets that are not “big data,” this is seldom a real issue. The extra expense in memory, as well as the extra expense in access, are small prices to pay for the simplicity and speed of the human user in accessing the data.\nObjection: Once it’s tidy, we pretty much have to use Boolean indexing to get what we want, and that can be slower than other methods of accessing data. What about performance?\nResponse: See the previous response. Speed of access really only becomes a problem with big, high-throughput data sets. In those cases, there are often many things you need to be clever about beyond organization of your data.\nConclusion: I really think that tidying a data set allows for fluid exploration. We will focus on tidy data sets going forward. Bringing untidy data into tidy format can be an annoying challenge that often involves lots of file I/O and text parsing operations. As such, it is wise to arrange your data in tidy format starting at acquisition. If your instrumentation prevents you from doing so, you should develop functions and scripts to parse them and convert them into tidy format.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#the-data-set",
    "href": "lessons/exploratory/tidy_data.html#the-data-set",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.2 The data set",
    "text": "3.2 The data set\nWe will again use the data set from the Beattie, et al. paper on facial matching under sleep deprivation. Let’s load in the original data set and add the column on insomnia as we did in previous part of this lesson.\n\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias(\"insomnia\"))\n\n# Take a look\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n\n\n\n\n\n\nThis data set is in tidy format. Each row represents a single test on a single participant. The aspects of that person’s test are given in each column. We already saw the power of having the data in this format when we did Boolean indexing. Now, we will see how this format allows use to easily do an operation we do again and again with data sets, split-apply-combine.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#split-apply-combine",
    "href": "lessons/exploratory/tidy_data.html#split-apply-combine",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.3 Split-apply-combine",
    "text": "3.3 Split-apply-combine\nLet’s say we want to compute the median percent correct face matchings for subjects with insomnia and the median percent correct face matchings for those without. Ignoring for the second the mechanics of how we would do this with Python, let’s think about it in English. What do we need to do?\n\nSplit the data set up according to the 'insomnia' field, i.e., split it up so we have a separate data set for the two classes of subjects, those with insomnia and those without.\nApply a median function to the activity in these split data sets.\nCombine the results of these averages on the split data set into a new, summary data set that contains the two classes (insomniac and not) and means for each.\n\nWe see that the strategy we want is a split-apply-combine strategy. This idea was put forward by Hadley Wickham in this paper. It turns out that this is a strategy we want to use very often. Split the data in terms of some criterion. Apply some function to the split-up data. Combine the results into a new data frame.\nNote that if the data are tidy, this procedure makes a lot of sense. Choose the column or columns you want to use to split by. All rows with like entries in the splitting column(s) are then grouped into a new data set. You can then apply any function you want into these new data sets. You can then combine the results into a new data frame.\nPolars’s split-apply-combine operations are achieved in a group by/aggregation context, achieved with df.group_by().agg(). You can think of group_by() as the splitting part. The apply-combine part is done by passing expressions into agg(). The result is a data frame with as many rows as there are groups that we split the data frame into.\n\n3.3.1 Median percent correct\nLet’s go ahead and do our first split-apply-combine on this tidy data set. First, we will split the data set up by insomnia condition and then apply a median function.\n\ndf.group_by('insomnia').median()\n\n\nshape: (2, 16)\n\n\n\ninsomnia\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\ntrue\n46.0\nnull\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\nfalse\n54.0\nnull\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\n\n\n\n\nNote that we used the median() method of the GroupBy object. This computes the median of all columns. This is equivalent the following using agg() with the expression pl.col('*').median().\n\ndf.group_by('insomnia').agg(pl.col('*').median())\n\n\nshape: (2, 16)\n\n\n\ninsomnia\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\nfalse\n54.0\nnull\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\ntrue\n46.0\nnull\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\n\n\n\n\nThe median doesn’t make sense for gender. If we only wanted to compute medians of quantities for which it makes sense to do so, we could use selectors.\n\ndf.group_by('insomnia').agg(cs.numeric().exclude('participant number').median())\n\n\nshape: (2, 14)\n\n\n\ninsomnia\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\ntrue\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\nfalse\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\n\n\n\n\nIf instead we only wanted the median of the percent correct and confidence when correct, we could do the following.\n\n(\n    df\n    .group_by('insomnia')\n    .agg(\n        pl.col('percent correct', 'confidence when correct').median(), \n    )\n)\n\n\nshape: (2, 3)\n\n\n\ninsomnia\npercent correct\nconfidence when correct\n\n\nbool\nf64\nf64\n\n\n\n\ntrue\n75.0\n77.0\n\n\nfalse\n85.0\n75.0\n\n\n\n\n\n\nWe can also use multiple columns in our group_by() operation. For example, we may wish to look at four groups, male insomniacs, female insomniacs, male non-insomniacs, and female non-insomniacs. To do this, we simply pass in a list of columns into df.group_by().\n\ndf.group_by([\"gender\", \"insomnia\"]).median()\n\n\nshape: (4, 16)\n\n\n\ngender\ninsomnia\nparticipant number\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nstr\nbool\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"m\"\nfalse\n41.0\n38.5\n90.0\n80.0\n82.5\n76.0\n57.75\n74.25\n54.75\n76.25\n59.25\n29.0\n3.0\n6.0\n\n\n\"m\"\ntrue\n55.5\n37.0\n95.0\n82.5\n83.75\n83.75\n55.5\n75.75\n73.25\n81.25\n62.5\n14.0\n9.0\n8.0\n\n\n\"f\"\nfalse\n58.0\n36.0\n85.0\n80.0\n85.0\n74.0\n55.0\n70.5\n60.0\n74.0\n58.75\n26.0\n4.0\n7.0\n\n\n\"f\"\ntrue\n46.0\n39.0\n80.0\n75.0\n72.5\n76.5\n73.75\n71.0\n68.5\n77.0\n70.5\n14.0\n9.0\n7.0",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#window-functions",
    "href": "lessons/exploratory/tidy_data.html#window-functions",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.4 Window functions",
    "text": "3.4 Window functions\nIn the above examples, we split the data frame into groups and applied an aggregating function that gave us back a data frame with as many rows as groups. But what if we do not want to aggregate? For example, say we want to compute the ranking of each participant according to percent correct within each group of insomniacs and normal sleepers. First, let’s see what happens if we use a group by/aggregation context. When we do the grouping, we will retain the order of the entries so that the series of ranks that we acquire match the original order in the data frame. We will also use the 'ordinal' method for ranking, which gives a distinct rank to each entry even in the event of ties.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nlist[u32]\n\n\n\n\ntrue\n[11, 21, … 10]\n\n\nfalse\n[13, 35, … 7]\n\n\n\n\n\n\nWe see that we indeed get a data frame that has the two rows, one for each group. The 'percent correct' columns has a new data type, a Polars list (not the same as a Python list). If we wanted to convert each entry in the list into a new row of the data frame, we can use the explode() method of a data frame.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n    )\n    .explode('percent correct')\n)\n\n\nshape: (102, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nu32\n\n\n\n\ntrue\n11\n\n\ntrue\n21\n\n\ntrue\n23\n\n\ntrue\n19\n\n\ntrue\n3\n\n\n…\n…\n\n\nfalse\n29\n\n\nfalse\n57\n\n\nfalse\n20\n\n\nfalse\n12\n\n\nfalse\n7\n\n\n\n\n\n\nThere are a few annoyances with doing this. First, in the group by/aggregation context, there is no way to maintain the other columns of the data frame as there is in the selection context via the with_columns() method. Second, we have to make a column with a list data type and then explode it.\nHere is where window functions come into play. A window function only operates on a subset of the data, ignoring everything outside of that subset. Since we are applying a function (in our example a rank function) to a subset (group) of the data, we can think of doing a group by followed by a calculation that has the same number of rows in the output as there are rows in the data as a window function.\nWindow functions are implemented in Polars expressions using the over() method. The argument(s) of the over() specify which columns specify groups. The expression is then evaluated individually for each group. To add a column with rankings based on percent correct within each group (insomnia or regular sleeper), we can do the following.\n\n(\n    df\n    .with_columns(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n        .over('insomnia')\n        .alias('percent correct ranked within insomnia groups')\n    )\n)\n\n\nshape: (102, 17)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\npercent correct ranked within insomnia groups\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\nu32\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n11\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n21\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n23\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n19\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n3\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n97\n\"f\"\n23\n70\n85\n77.5\n77.0\n66.5\n77.0\n77.5\n77.0\n74.0\n20\n8\n10\nfalse\n29\n\n\n98\n\"f\"\n70\n90\n85\n87.5\n65.5\n85.5\n87.0\n80.0\n74.0\n80.0\n19\n8\n7\nfalse\n57\n\n\n99\n\"f\"\n24\n70\n80\n75.0\n61.5\n81.0\n70.0\n61.0\n65.0\n81.0\n31\n2\n15\nfalse\n20\n\n\n102\n\"f\"\n40\n75\n65\n70.0\n53.0\n37.0\n84.0\n52.0\n81.0\n51.0\n22\n4\n7\nfalse\n12\n\n\n103\n\"f\"\n33\n85\n40\n62.5\n80.0\n27.0\n31.0\n82.5\n81.0\n73.0\n24\n5\n7\nfalse\n7\n\n\n\n\n\n\nWindow functions and group by operations are similar. Consider computing the median percent correct for each group, insomniacs and normal sleepers. Using a group by/aggregation context, this is accomplished as follows.\n\n(\n    df\n    .group_by('insomnia')\n    .agg(\n        pl.col('percent correct')\n        .median()\n        .alias('median percent correct')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nmedian percent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\n\n\n\n\n\n\nWe can achieve the same result using a window function with in a select context.\n\n(\n    df\n    .select(\n        pl.col('insomnia'), \n        pl.col('percent correct')\n        .median().over('insomnia')\n        .alias('median percent correct')\n    )\n    .unique('median percent correct')\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nmedian percent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n75.0\n\n\nfalse\n85.0\n\n\n\n\n\n\nA group by/aggregation context is preferred in this case. As a loose rule of thumb, use a group by/aggregation context when you want a resulting data frame with the number of rows being equal to the number of groups. Use a window function in a selection context when you want a resulting data frame with the same number of rows as your input data frame.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#custom-aggregation-and-window-functions",
    "href": "lessons/exploratory/tidy_data.html#custom-aggregation-and-window-functions",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.5 Custom aggregation and window functions",
    "text": "3.5 Custom aggregation and window functions\nLet’s say we want to compute the coefficient of variation (CoV, the standard deviation divided by the mean) of data in columns of groups in the data frame. There is no built-in function in Polars to do this, but we could construct an expression that does it.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        (pl.col('percent correct').std(ddof=0) / pl.col('percent correct').mean())\n        .alias('coeff of var percent correct')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\ncoeff of var percent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.171856\n\n\nfalse\n0.138785\n\n\n\n\n\n\nAlternatively, we could write our own Python function to compute the coefficient of variation using Numpy. Say we had such a function lying around that take as input a Numpy array and returns the coefficient of variation as a Numpy array.\n\ndef coeff_of_var(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute coefficient of variation from an array of data.\"\"\"\n    return np.std(data) / np.mean(data)\n\nWe can use this function as an aggregating function using the map_elements() method of a Polars expression. The argument of map_elements() is a function that take a series as input and returns either a scalar or a series. We therefore need to pass in a function that converts the series to a Numpy array for use in the coeff_of_var() function, which we can accomplish with a lambda function.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .map_elements(lambda s: coeff_of_var(s.to_numpy()), return_dtype=float, returns_scalar=True)\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.171856\n\n\nfalse\n0.138785\n\n\n\n\n\n\nNote that it is important to supply the data type of the return of the function you are mapping to ensure that Polars correctly assigns data types in the resulting data frame.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#polars-structs-and-expressions-using-multiple-columns",
    "href": "lessons/exploratory/tidy_data.html#polars-structs-and-expressions-using-multiple-columns",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.6 Polars Structs and expressions using multiple columns",
    "text": "3.6 Polars Structs and expressions using multiple columns\nAs discussed earlier, Polars expressions strictly take a series as input and return a series as output. What if we want to perform a calculation that requires two columns? This is where the Polars Struct data type comes in. As a simple example, if we cast out whole data frame into a series, we get a series with a struct data type.\n\nstruct_series = pl.Series(df)\n\n# Take a look\nstruct_series\n\n\nshape: (102,)\n\n\n\n\n\n\nstruct[16]\n\n\n\n\n{8,\"f\",39,65,80,72.5,91.0,90.0,93.0,83.5,93.0,90.0,9,13,2,true}\n\n\n{16,\"m\",42,90,90,90.0,75.5,55.5,70.5,50.0,75.0,50.0,4,11,7,true}\n\n\n{18,\"f\",31,90,95,92.5,89.5,90.0,86.0,81.0,89.0,88.0,10,9,3,true}\n\n\n{22,\"f\",35,100,75,87.5,89.5,null,71.0,80.0,88.0,80.0,13,8,20,true}\n\n\n{27,\"f\",74,60,65,62.5,68.5,49.0,61.0,49.0,65.0,49.0,13,9,12,true}\n\n\n…\n\n\n{97,\"f\",23,70,85,77.5,77.0,66.5,77.0,77.5,77.0,74.0,20,8,10,false}\n\n\n{98,\"f\",70,90,85,87.5,65.5,85.5,87.0,80.0,74.0,80.0,19,8,7,false}\n\n\n{99,\"f\",24,70,80,75.0,61.5,81.0,70.0,61.0,65.0,81.0,31,2,15,false}\n\n\n{102,\"f\",40,75,65,70.0,53.0,37.0,84.0,52.0,81.0,51.0,22,4,7,false}\n\n\n{103,\"f\",33,85,40,62.5,80.0,27.0,31.0,82.5,81.0,73.0,24,5,7,false}\n\n\n\n\n\n\nEach entry is an entire row of a data frame. The labels of each column is still present, as can be seen by considering one entry in the series of structs.\n\nstruct_series[0]\n\n{'participant number': 8,\n 'gender': 'f',\n 'age': 39,\n 'correct hit percentage': 65,\n 'correct reject percentage': 80,\n 'percent correct': 72.5,\n 'confidence when correct hit': 91.0,\n 'confidence incorrect hit': 90.0,\n 'confidence correct reject': 93.0,\n 'confidence incorrect reject': 83.5,\n 'confidence when correct': 93.0,\n 'confidence when incorrect': 90.0,\n 'sci': 9,\n 'psqi': 13,\n 'ess': 2,\n 'insomnia': True}\n\n\nThe labels for each entry in a struct (the keys in the dictionary displated above), are called fields, and we can get a list of the fields for the structs in a series.\n\nstruct_series.struct.fields\n\n['participant number',\n 'gender',\n 'age',\n 'correct hit percentage',\n 'correct reject percentage',\n 'percent correct',\n 'confidence when correct hit',\n 'confidence incorrect hit',\n 'confidence correct reject',\n 'confidence incorrect reject',\n 'confidence when correct',\n 'confidence when incorrect',\n 'sci',\n 'psqi',\n 'ess',\n 'insomnia']\n\n\nThe struct_series.struct.field() method allows us to take entries from all entries corresponding to a given field.\n\nstruct_series.struct.field('percent correct')\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5\n\n\n\n\n\n\nThe following two operations give the same result.\npl.Series(df).struct.field('percent correct')\ndf.get_column('percent correct')\nSo, if we need to compute with multiple columns with an expression, we can convert whatever columns we need into a series of data type struct. We can then unpack what we need using the struct_series.struct.field() method.\nAs an example, say we have a function that compute the bivariate (a.k.a. Pearson) correlation coeff between two data sets given as Numpy arrays.\n\ndef bivariate_corr(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute bivariate correlation coefficient for `x` and `y`.\n    Ignores NaNs.\"\"\"\n    # Masked arrays to handle NaNs\n    x = np.ma.masked_invalid(x)\n    y = np.ma.masked_invalid(y)\n    mask = ~x.mask & ~y.mask\n    \n    return np.corrcoef(x[mask], y[mask])[0, 1]\n\nNow we want to compute the bivariate correlation coefficient for confidence when correct and confidence when incorrect for insomniacs and for normal sleepers. In our call to agg(), we use pl.struct to generate a series of data type struct containing the columns we need for the calculation of the correlation coefficient. We then use map_elements() to use the above function to do the calculation.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.struct(['confidence when correct', 'confidence when incorrect'])\n        .map_elements(\n            lambda s:\n            bivariate_corr(\n                s.struct.field('confidence when correct').to_numpy(), \n                s.struct.field('confidence when incorrect').to_numpy()\n            ), \n           return_dtype=float, returns_scalar=True\n        )\n        .alias('bivariate correlation')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nbivariate correlation\n\n\nbool\nf64\n\n\n\n\ntrue\n0.590435\n\n\nfalse\n0.552045\n\n\n\n\n\n\nWhile this example is instructive to demonstrate how to write your own functions to operate on data frames, as is often the case, Polars has a built-in function that computes the bivariate correlation coefficient.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(pl.corr('confidence when correct', 'confidence when incorrect'))\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nconfidence when correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.590435\n\n\nfalse\n0.552045",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#looping-over-a-groupby-object",
    "href": "lessons/exploratory/tidy_data.html#looping-over-a-groupby-object",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.7 Looping over a GroupBy object",
    "text": "3.7 Looping over a GroupBy object\nWhile the GroupBy methods we have learned so far (like transform() and agg()) are useful and lead to concise code, we sometimes want to loop over the groups of a GroupBy object. This often comes up in plotting applications, as we will see in future lessons. As an example, I will compute the median percent correct for female and males, insomniacs and not (which we already computed using describe()).\n\nfor group_name, sub_df in df.group_by(['gender', 'insomnia']):\n    print(group_name, \": \", sub_df[\"percent correct\"].median())\n\n('m', False) :  82.5\n('f', False) :  85.0\n('m', True) :  83.75\n('f', True) :  72.5\n\n\nBy using the GroupBy object as an iterator, it yields the name of the group (which I assigned as group_name) and the corresponding sub-data frame (which I assigned sub_df). Note that the group name is always given as a tuple, even when only grouping by a single column.\n\nfor (group_name,), sub_df in df.group_by('insomnia'):\n    print(group_name, \": \", sub_df[\"percent correct\"].median())\n\nFalse :  85.0\nTrue :  75.0",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/tidy_data.html#computing-environment",
    "href": "lessons/exploratory/tidy_data.html#computing-environment",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.8 Computing environment",
    "text": "3.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.3\nIPython version      : 8.25.0\n\nnumpy     : 1.26.4\npolars    : 1.2.0\njupyterlab: 4.0.13",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/polars_for_pandas.html",
    "href": "lessons/exploratory/polars_for_pandas.html",
    "title": "4  Polars for Pandas users",
    "section": "",
    "text": "4.1 A sample data frame\n| Download notebook\nAs of June 2025, Pandas is by far and away the most widely used data frame package for Python. We are using Polars primarily because, in my opinion, the API is more intuitive and therefore easier for beginners and experts alike to use. It is also faster, sometimes much faster. It is, however, important to know about Pandas and how to use it because many of your colleagues use it and many packages you may use do, too.\nTherefore, in this part of the lesson, I discuss how to convert a Polars data frame to Pandas, and vice versa. I also provide syntax for doing common tasks in Polars and Pandas. It is also worth reading the section of the Polars user guide comparing Pandas and Polars.\nFor ease of discussion and comparison, we will use a simple data frame that has two categorical columns, 'c1', and 'c2', two quantitative columns as floats, 'q1', and 'q2', and a column, 'i1' of integer values. It also has an identity column, a unique identifier for each row that is useful when converting the data frame to tall format. Note that 'q1' has a null value and 'q2' has a NaN value.\ndata = dict(\n    id=list(range(1, 9)),\n    c1=['a']*4 + ['b']*4,\n    c2=['c', 'd'] * 4,\n    q1=[1.1, 2.2, 3.1, None, 2.9, 1.7, 3.0, 7.3],\n    q2=[4.5, 2.3, np.nan, 1.1, 7.8, 2.3, 1.1, 0.8],\n    i1=[5, 3, 0, 2, 4, 3, 4, 1],\n)\n\ndf = pl.DataFrame(data)\n\n# Take a look\ndf\n\n\nshape: (8, 6)\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\ni64\nstr\nstr\nf64\nf64\ni64\n\n\n\n\n1\n\"a\"\n\"c\"\n1.1\n4.5\n5\n\n\n2\n\"a\"\n\"d\"\n2.2\n2.3\n3\n\n\n3\n\"a\"\n\"c\"\n3.1\nNaN\n0\n\n\n4\n\"a\"\n\"d\"\nnull\n1.1\n2\n\n\n5\n\"b\"\n\"c\"\n2.9\n7.8\n4\n\n\n6\n\"b\"\n\"d\"\n1.7\n2.3\n3\n\n\n7\n\"b\"\n\"c\"\n3.0\n1.1\n4\n\n\n8\n\"b\"\n\"d\"\n7.3\n0.8\n1",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/polars_for_pandas.html#from-polars-to-pandas-and-from-pandas-to-polars",
    "href": "lessons/exploratory/polars_for_pandas.html#from-polars-to-pandas-and-from-pandas-to-polars",
    "title": "4  Polars for Pandas users",
    "section": "4.2 From Polars to Pandas and from Pandas to Polars",
    "text": "4.2 From Polars to Pandas and from Pandas to Polars\nIf you have a Polars data frame, you can directly convert it to a Pandas data frame using the to_pandas(), method. Let’s do that for our data frame.\n\ndf.to_pandas()\n\n\n\n\n\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\n\n\n0\n1\na\nc\n1.1\n4.5\n5\n\n\n1\n2\na\nd\n2.2\n2.3\n3\n\n\n2\n3\na\nc\n3.1\nNaN\n0\n\n\n3\n4\na\nd\nNaN\n1.1\n2\n\n\n4\n5\nb\nc\n2.9\n7.8\n4\n\n\n5\n6\nb\nd\n1.7\n2.3\n3\n\n\n6\n7\nb\nc\n3.0\n1.1\n4\n\n\n7\n8\nb\nd\n7.3\n0.8\n1\n\n\n\n\n\n\n\nNote that the null value because a NaN. All missing data in Pandas are NaN. (Well, not really. You can have an object data data type for a column that permits None values. However, when Pandas reads in data, when there are missing data, it assigns it to be NaN by default.)\nNote also that Pandas has an index displayed on the left side of the data frame. In general, we will not use Pandas indexes.\nSimilarly, if you have a data frame in Pandas, you can convert it to a Polars data frame using the pl.from_pandas() function.\n\npl.from_pandas(pd.DataFrame(data))\n\n\nshape: (8, 6)\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\ni64\nstr\nstr\nf64\nf64\ni64\n\n\n\n\n1\n\"a\"\n\"c\"\n1.1\n4.5\n5\n\n\n2\n\"a\"\n\"d\"\n2.2\n2.3\n3\n\n\n3\n\"a\"\n\"c\"\n3.1\nnull\n0\n\n\n4\n\"a\"\n\"d\"\nnull\n1.1\n2\n\n\n5\n\"b\"\n\"c\"\n2.9\n7.8\n4\n\n\n6\n\"b\"\n\"d\"\n1.7\n2.3\n3\n\n\n7\n\"b\"\n\"c\"\n3.0\n1.1\n4\n\n\n8\n\"b\"\n\"d\"\n7.3\n0.8\n1",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/polars_for_pandas.html#pandas-and-polars-for-common-tasks",
    "href": "lessons/exploratory/polars_for_pandas.html#pandas-and-polars-for-common-tasks",
    "title": "4  Polars for Pandas users",
    "section": "4.3 Pandas and Polars for common tasks",
    "text": "4.3 Pandas and Polars for common tasks\nBelow is a table listing common tasks with data frame done using Polars and Pandas.\n\n\n\n\n\n\n\n\nDescription\nPandas\nPolars\n\n\n\n\nConvert dictionary to df\npd.DataFrame(data)\npl.DataFrame(data)\n\n\nMake 2D Numpy array into df\npd.DataFrame(my_ar, columns=['col1', 'col2', 'col3'])\npl.DataFrame(my_ar, schema=['col1', 'col2', 'col3']', orient='row')\n\n\nRead from CSV\npd.read_csv(file_name)\npl.read_csv(file_name)\n\n\nLazily read CSV\n—\npl.scan_csv(file_name)\n\n\nRead from Excel\npd.read_excel(file_name)\npl.read_excel(file_name)\n\n\nRead from JSON\npd.read_json(file_name)\npl.read_json(file_name)\n\n\nRead from HDF5\npd.read_hdf(file_name)\n—\n\n\nWrite CSV\ndf.to_csv(fname, index=False)\ndf.write_csv(fname)\n\n\nRename columns\ndf.rename(columns={'c1': 'cat1', 'c2': 'cat2'})\ndf.rename({'c1': 'cat1', 'c2': 'cat2'})\n\n\nGet column 'q1' as series\ndf['q1'] or df.get_column('q1') or df.loc[:,'q1']\ndf['q1'] or df.get_column('q1')\n\n\nGet column 'q1' as data frame\ndf[['q1']] or df.loc[:,['q1']]\ndf.select('q1')\n\n\nGet columns 'c1' and 'q2'\ndf[['c1', 'q2']] or df.loc[:, ['c1', 'q2']]\ndf.select('c1', 'q2')\n\n\nGet columns containing floats\ndf.select_dtypes(float)\ndf.select(cs.float())\n\n\nGet row 4\ndf.loc[4, :]\ndf.row(4) or df.row(4, named=True)\n\n\nGet row 4 as data frame\ndf.loc[[4], :]\ndf[4]\n\n\nGet row where i1 is 2\ndf.loc[df['i1']==2, :]\ndf.row(by_predicate=pl.col('i1')==2) or df.filter(pl.col('i1')==2)\n\n\nSub df with rows where c1 is 'a'\ndf.loc[df['c1']=='a', :]\ndf.filter(pl.col('c1')=='a')\n\n\nSub df where c1 is 'a' and c2 is 'd'\ndf.loc[(df['c1']=='a') & (df['c2']=='d'), :]\ndf.filter((pl.col('c1')=='a') & (pl.col('c2')=='d'))\n\n\nIterate over columns of df\nfor col, s in df.items()\nfor s in df\n\n\nIterate over rows of df\nfor row_ind, row in df.iterrows()\nfor r in df.iter_rows(named=True)\n\n\nGroup by single column\ndf.groupby('c1')\ndf.group_by('c1')\n\n\nGroup by maintaining order\ndf.groupby('c1', sort=False)\ndf.group_by('c1', maintain_order=True)\n\n\nGroup by multiple columns\ndf.groupby(['c1', 'c2'])\ndf.group_by(['c1', 'c2'])\n\n\nIterate over groups\nfor group, subdf in df.groupby('c1')\nfor (group,), subdf in df.group_by('c1')\n\n\nIterate over nested groups\nfor (g1, g2), subdf in df.groupby(['c1', 'c2'])\nfor (g1, g2), subdf in df.groupby(['c1', 'c2'])\n\n\nGroup by and apply mean¹\ndf.groupby('c1').mean(numeric_only=True)\ndf.group_by('c1').mean()\n\n\nGroup by and apply median to one column\ndf.groupby('c1')['q1'].median()\ndf.group_by('c1').agg(pl.col('q1').median())\n\n\nGroup by and apply mean to two columns\ndf.groupby('c1')[['q1', 'q2']].mean()\ndf.group_by('c1').agg(pl.col('q1', 'q2').mean())\n\n\nGroup by and apply custom func to col²\ndf.groupby('c1')['q1'].apply(my_fun)\ndf.group_by('c1').agg(pl.col('q1').map_elements(my_fun, return_dtype=float))\n\n\nGroup by and apply custom func to 2 cols³\ndf.groupby('c1')[['q1', 'q2']].apply(my_fun)\ndf.group_by('c1').agg(pl.struct(['q1', 'q2']).map_elements(my_fun, return_dtype=float))\n\n\nGroup by and rank within each group\ndf.groupby('c1')['q1'].rank()\ndf.select(pl.col('q1').rank().over('c1'))\n\n\nConvert to tall format\ndf.melt(value_name='value', var_name='var', id_vars='id')\ndf.unpivot(value_name='value', variable_name='var', index='id')\n\n\nPivot tall result above\ndf_tall.pivot(columns='var', index='id').reset_index()\ndf_tall.pivot(on='var', index='id')\n\n\nSelect columns with string in name\ndf.filter(regex='q') or df[df.columns[df.columns.str.contains('q')]]\ndf.select(cs.contains('q'))\n\n\nAdd column of zeros to data frame\ndf['new_col'] = 0 or df.assign(new_col=0)\ndf.with_columns(pl.lit(0).alias('new_col'))\n\n\nAdd a Numpy array as column\ndf['new_col'] = my_array or df.assign(new_col=my_array)\ndf.with_columns(pl.Series(my_array).alias('new_col'))\n\n\nMultiply two columns; make new column\ndf['q1q2'] = df['q1'] * df['q2'] or df.assign(q1q2=df['q1'] * df['q2']\ndf.with_columns((pl.col('q1') * pl.col('q2')).alias('q1q2'))\n\n\nApply a function to each row making new col⁴\ndf.assign(new_col=my_fun)\ndf.with_columns(pl.struct(pl.all()).map_elements(my_fun, return_dtype=float).alias('new_col'))\n\n\nDrop rows with missing data\ndf.dropna()\ndf.drop_nulls()\n\n\nSort according to a column\ndf.sort_values(by='i1')\ndf.sort(by='i1')\n\n\nInner join two data frames⁵\npd.merge(df, df2, on=shared_columns)\ndf.join(df2, on=shared_columns)\n\n\nConcatenate data frames vertically\npd.concat((df, df2))\npl.concat((df, df2), how='diagonal')\n\n\nConcatenate data frames horizontally\npd.concat((df, df2), axis=1)\npl.concat((df, df2), how='horizontal')\n\n\n\nFootnotes\n\nNote that in Pandas, NaNs are omitted from calculations like means. In Polars, NaNs are included, and the result will be NaN. However, nulls are not included.\nFor Pandas, the function my_fun must take an array_like data type (list, Numpy array, Pandas Series, etc.) as input. For Polars, the function my_fun must take a Polars Series as input. It is wise to specify the data type of the output of the function (shown as float in the above example, but can be whatever type my_fun returns). A Pandas example: my_fun = lambda x: np.sum(np.sin(x)). A Polars example: my_fun = lambda s: s.exp().sum().\nFor Pandas, the function must take a Pandas DataFrame as an argument. For Polars, it must take a Polars Series with a struct data type. A Pandas example: my_fun = lambda df: (np.sin(s['q1']) * s['q2']).sum(). A Polars example: my_fun = lambda s: (s.struct.field('q1').sin() * s.struct.field('q2')).sum()\nFor Pandas, my_fun must take as its argument a Pandas Series with an index containing the names of the columns of the original data frame. For Polars, my_fun must take as its argument a dictionary with keys given by the names of the columns of the original data frame. The functions may then have the same syntax (though possibly with different type hints). An example: my_fun = lambda r: r['i1'] * np.sin(r['q2']). However, note that in Polars, a null value is treated as None, which means you cannot apply a function to it, multiply by it, etc.\nFor Polars, the on kwarg for df.join() is required. With Pandas, which columns to join on are inferred based on like-names of columns.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/polars_for_pandas.html#hierarchical-indexes",
    "href": "lessons/exploratory/polars_for_pandas.html#hierarchical-indexes",
    "title": "4  Polars for Pandas users",
    "section": "4.4 Hierarchical indexes",
    "text": "4.4 Hierarchical indexes\nPandas supports hierarchical indexes, called MultiIndexes. This is not supports by Polars. Polars will not read a CSV file with hierarchical indexes. If you have a data set in a CSV file with hierarchical indexes, you can convert it to a CSV file in tall format where the MultiIndex has been converted to columns using the bebi103.utils.unpivot_csv() function. This operation is akin to a df.melt() operation on a data frame with a hierarchical index. You can then read the converted CSV file into Polars and begin working with it.",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/polars_for_pandas.html#computing-environment",
    "href": "lessons/exploratory/polars_for_pandas.html#computing-environment",
    "title": "4  Polars for Pandas users",
    "section": "4.5 Computing environment",
    "text": "4.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,pandas,polars,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npandas    : 2.2.3\npolars    : 1.27.1\njupyterlab: 4.3.6",
    "crumbs": [
      "Polars and split-apply-combine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting.html",
    "href": "lessons/exploratory/plotting.html",
    "title": "Data display",
    "section": "",
    "text": "The Python visualization landscape\nIn 1977, John Tukey, one of the prominent statisticians and mathematicians in history, published a book entitled Exploratory Data Analysis. In it, he laid out general principles on how researchers should handle their first encounters with their data, before formal statistical inference. Most of us spend a lot of time doing exploratory data analysis, or EDA, without really knowing it. Mostly, EDA involves a graphical exploration of a data set.\nWe start off with a few wise words from John Tukey himself, chosen from that brilliant book.\nClearly data display, or plotting, is central to exploratory data analysis.\nLet us start by looking at some of the many plotting packages available in Python. In a talk at PyCon in 2017, Jake VanderPlas, who is one of the authors of one of them (Altair), gave an overview of the Python visualization landscape. That landscape is depicted below, taken from this visualization of it by Nicolas Rougier. (It is from 2017, so it is dated, and definitely not complete, notably missing Panel and domain-specific plotting like napari and Folium, for example.)\nThe landscape is divided into three main pods based on the low-level renderer of the graphics, JavaScript, Matplotlib, and OpenGL (though Matplotlib is higher-level than JavaScript and OpenGL). We will not discuss packages based on OpenGL. Packages that use JavaScript for rendering are particularly well suited for interactivity in browsers. Interactivity and portability (accomplished by rendering in browsers) are key features of modern plotting libraries, so we will use JavaScript-based plotting in the workshop (as I do in my own work).\nThough we will be using Bokeh (and a little bit of HoloViews/Datashader), for a neuroscientist working in Python, it is important to take note of the following.",
    "crumbs": [
      "Data display"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting.html#the-python-visualization-landscape",
    "href": "lessons/exploratory/plotting.html#the-python-visualization-landscape",
    "title": "Data display",
    "section": "",
    "text": "Figure 1: A somewhat dated, but reasonably complete, picture of the landscape of data visualization packages in Python.\n\n\n\n\n\n\nMatplotlib is by far the most widely used plotting package in Python. It was even developed a neuroscientist! Seaborn is also widely used as a higher level statistical plotting package that has Matplotlib as its backend (and also developed by a neuroscientist!). We choose to use Bokeh because it is effective\nThere are many neuroscience-specific packages that have plotting modules, like MNE, Nilearn, and SpikeInterface. Here, we are focusing on general tools. If you have master of lower-level plotting software, you can get much more out of domain-specific packages. You are also unshackled to do the visualization you want to do, and not just those that are available.",
    "crumbs": [
      "Data display"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html",
    "href": "lessons/exploratory/plotting_with_bokeh.html",
    "title": "5  Making plots with Bokeh",
    "section": "",
    "text": "5.1 High-level and low-level plotting packages\n| Download notebook\nAs a demonstration of what I mean by high-level and low-level plotting packages, let us first think about one of our tasks we did with Polars with the facial matching data set. We computed the median percent correct for those with and without insomnia. Here’s the code to do it.\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias(\"insomnia\"))\n\ndf.group_by(\"insomnia\").agg(pl.col('percent correct').median())\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\nLiterally just a few lines of code. Now what if we tried to do it without Polars? I won’t even go through the many lines of code necessary to read in the data. Consider just this one line.\nThere are elementary tasks that go into it if we were to code it up without using Polars’s delicious functionality. We can loop over the rows in the data frame with a for loop, check to see what the value of the insomnia column is with an if statement, put the value in the percent correct field into an appropriate array based on whether or not the subject suffers from insomnia, and then, given those arrays, sort them and pull out the middle value. Under the hood, all of those steps take place, but because we use Polars’s high-level functionality, those details are invisible to us, and glad we are of that.\nNow, say we want to make a plot of some data. You can imagine that there are many many steps to building that. One way you could build a plot is to hand-generate an SVG file that is a set of specifications for lines and circles and text and whatnot that comprises a plot. (I have actually done this before, writing a C program that hand-generated SVG, and it was paaaaainful.) That would be a very low-level way of generating a plot. Plotting libraries in Python usually take care of the rendering part for you, either rendering the plot as SVG, PDF, PNG, or other formats, including interactive ones that use JavaScript and HTML Canvas that can be viewed in a browser. The plotting libraries then vary in their level of abstraction from the data set.\nLower-level plotting libraries typically are more customizable, but require more boilerplate code to get your data plotted and are therefore more cumbersome. Higher-level plotting libraries aim to make it easier to move directly from a data frame to a plot. Because of this streamlining, though, they are often more difficult to customize.\nThe developers of HoloViz made a nice graphic for this concept (Copyright PyViz authors, downloaded here).\nUsing a low-level plotting library, you can get to any graphic you like, but it takes many steps to do so. Using a high level library, you can rapidly get to many, if not most, of the graphics you like in very few steps. However, you cannot get to all graphics. In a layered approach, in which the higher level libraries give you access to the lower level customizations, you can get to any graphic, and can do so quickly. The layered approach requires proficiency in using the low-level and high-level libraries.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#high-level-and-low-level-plotting-packages",
    "href": "lessons/exploratory/plotting_with_bokeh.html#high-level-and-low-level-plotting-packages",
    "title": "5  Making plots with Bokeh",
    "section": "",
    "text": "df.group_by(\"insomnia\").agg(pl.col('percent correct').median())\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Routes to making a plot, copyright PyViz authors.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#bokeh-and-the-summer-school",
    "href": "lessons/exploratory/plotting_with_bokeh.html#bokeh-and-the-summer-school",
    "title": "5  Making plots with Bokeh",
    "section": "5.2 Bokeh and the summer school",
    "text": "5.2 Bokeh and the summer school\nOne debate I have every time I teach data visualization is what plotting packages, high-level or low-level, to use. I decided to almost exclusively Bokeh, a low-level plotting library first for a few reasons.\n\nThough low-level, generating plot you might like to construct is fairly straightforward. (Read: it’s not that bad for quickly making plots.)\nBy being familiar with a lower-level plotting package, you can then take a layered approach as you learn a higher-level package.\nWe will discuss at least one higher-level package, iqplot, in short order, discussed in the next part of this lesson.\n\nImportantly, note that Bokeh’s submodules often have to be explicitly imported, as we did in the code cell at the top of this notebook. Not also that if you want your plots to be viewable (and interactive) in the notebook, you need to execute\nbokeh.io.output_notebook()\nat the top of the notebook (as we have done). Finally, note that we also have to have installed the Bokeh JupyterLab extension.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "href": "lessons/exploratory/plotting_with_bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "title": "5  Making plots with Bokeh",
    "section": "5.3 Bokeh’s grammar and our first plot with Bokeh",
    "text": "5.3 Bokeh’s grammar and our first plot with Bokeh\nConstructing a plot with Bokeh consists of four main steps.\n\nCreating a figure on which to populate glyphs (symbols that represent data, e.g., dots for a scatter plot). Think of this figure as a “canvas” which sets the space on which you will “paint” your glyphs.\nDefining a data source that is the reference used to place the glyphs.\nChoose the kind of glyph you would like.\nAnnotate the columns of data source to determine how they are used to place (and possibly color, scale, etc.) the glyph.\n\nAfter completing these steps, you need to render the graphic.\nLet’s go through these steps in generating a scatter plot of confidence when incorrect versus confidence when correct for the face matching under sleep deprivation study. So you have the concrete example in mind, the final graphic will look like this\n\n\n\n  \n\n\n\n\n\n\nOur first step is creating a figure, our “canvas.” In creating the figure, we are implicitly thinking about what kind of representation for our data we want. That is, we have to specify axes and their labels. We might also want to specify the title of the figure, whether or not to have grid lines, and all sorts of other customizations. Naturally, we also want to specify the shape of the figure.\n\n(Almost) all of this is accomplished in Bokeh by making a call to bokeh.plotting.figure() with the appropriate keyword arguments.\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=400,\n    frame_height=300,\n    x_axis_label='confidence when correct',\n    y_axis_label='condifence when incorrect'\n)\n\nThere are many more keyword attributes you can assign, including all of those listed in the Bokeh Plot class and the additional ones listed in the Bokeh Figure class.\n\nNow that we have set up our canvas, we can decide on the data source. We will use the data frame, df as our data source.\nWe will choose dots (or circles) as our glyph. This kind of glyph requires that we specify which column of the data source will serve to place the glyphs along the \\(x\\)-axis and which will serve to place the glyphs along the \\(y\\)-axis.\nWe choose the 'confidence when correct' column to specify the \\(x\\)-coordinate of the glyph and the 'confidence when incorrect' column to specify the \\(y\\)-coordinate. We already made this decision when we set up our axis labels, but we did not necessarily have to make that decision at that point.\n\nSteps 3 and 4 are accomplished by calling one of the glyph methods of the Bokeh Figure instance, p. Since we are choosing dots, the appropriate method is p.scatter(), and we use the source, x, and y kwargs to specify the positions of the glyphs.\n\np.scatter(\n    source=df.to_dict(),\n    x='confidence when correct',\n    y='confidence when incorrect'\n);\n\nNote that the source must be a ColumnDataSource, a special Bokeh class. If we pass in a dictionary (or Polars data frame) for the source keyword argument, Bokeh converts it to a ColumnDataSource. Therefore, we use the to_dict() method to convert the Polars data frame to a dictionary.\nNow that we have built the plot, we can render it in the notebook using bokeh.io.show().\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn looking at the plot, notice a toolbar to right of the plot that enables you to zoom and pan within the plot.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#the-importance-of-tidy-data-frames",
    "href": "lessons/exploratory/plotting_with_bokeh.html#the-importance-of-tidy-data-frames",
    "title": "5  Making plots with Bokeh",
    "section": "5.4 The importance of tidy data frames",
    "text": "5.4 The importance of tidy data frames\nIt might be clear for you now that building a plot in this way requires that the data frame you use be tidy. The organization of tidy data is really what enables this and high level plotting functionality. There is a well-specified organization of the data.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#code-style-in-plot-specifications",
    "href": "lessons/exploratory/plotting_with_bokeh.html#code-style-in-plot-specifications",
    "title": "5  Making plots with Bokeh",
    "section": "5.5 Code style in plot specifications",
    "text": "5.5 Code style in plot specifications\nSpecifications of plots often involves calls to functions with lots of keyword arguments to specify the plot, and this can get unwieldy without a clear style. You can develop your own style, maybe reading Trey Hunner’s blog post again. I like to do the following.\n\nPut the function call, like p.scatter( or p = bokeh.plotting.figure( on the first line.\nThe closed parenthesis for the function call is on its own line, unindented.\nAny arguments are given as kwargs (even if they can also be specified as positional arguments) at one level of indentation.\n\nNote that you cannot use method chaining when instantiating figures or populating glyphs.\nIf you adhere to a style (which is roughly the style imposed by Black), it makes your code cleaner and easier to read.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#coloring-with-other-dimensions",
    "href": "lessons/exploratory/plotting_with_bokeh.html#coloring-with-other-dimensions",
    "title": "5  Making plots with Bokeh",
    "section": "5.6 Coloring with other dimensions",
    "text": "5.6 Coloring with other dimensions\nLet’s say we wanted to make the same plot, but with orange circles for insomniacs and blue circles for normal sleepers. To do this, we take advantage of two features of Bokeh.\n\nWe can make multiple calls to p.scatter() to populate more and more glyphs.\np.scatter(), like all of the glyph methods, has many keyword arguments, including color and legend_label, which will enable us to color the glyphs and include a legend.\n\nWe can loop through the data frame grouped by 'insomnia' and populate the glyphs as we go along.\n\n# For convenience\nx = \"confidence when correct\"\ny = \"confidence when incorrect\"\n\n# Make figure\np = bokeh.plotting.figure(\n    width=400,\n    height=300,\n    x_axis_label=x,\n    y_axis_label=y,\n)\n\n# Add glyphs\ncolors = {'normal sleepers': '#1f77b4', 'insomniacs': 'orange'}\nfor (insom,), sub_df in df.group_by('insomnia'):\n    category = \"insomniacs\" if insom else \"normal sleepers\"\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=x,\n        y=y,\n        color=colors[category],\n        legend_label=category,\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe got the plot we wanted, but the legend is clashing with the data. Fortunately, Bokeh allows us to set attributes of the figure whenever we like. (We will further discuss styling Bokeh plots in a future lesson.) We can therefore set the legend position to be in the upper left corner. We will also set the click_policy for the legend to be 'hide', which will hide glyphs if you click the legend, which can be convenient for viewing cluttered plots (though this one is not cluttered, really).\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#adding-tooltips",
    "href": "lessons/exploratory/plotting_with_bokeh.html#adding-tooltips",
    "title": "5  Making plots with Bokeh",
    "section": "5.7 Adding tooltips",
    "text": "5.7 Adding tooltips\nBokeh’s interactivity is one of its greatest strengths. While we are plotting confidences when correct and incorrect, we have colored with insomniac status. We might also like to have access to other information in our (tidy) data source if we hover over a glyph. Let’s say we want to know the participant number, gender, and age of each participant. We can tell Bokeh to give us this information by adding tooltips when we instantiate the figure.\nThe syntax for a tooltip is a list of 2-tuples, where each tuple represents the tooltip you want. The first entry in the tuple is the label and the second is the column from the data source that has the values. The second entry must be preceded with an @ symbol signifying that it is a field in the data source and not field that is intrinsic to the plot, which is preceded with a $ sign. If there are spaces in the column heading, enclose the column name in braces. (See the documentation for tooltip specification for more information.)\n\n# For convenience\nx = \"confidence when correct\"\ny = \"confidence when incorrect\"\n\n# Make figure\np = bokeh.plotting.figure(\n    width=400,\n    height=300,\n    x_axis_label=x,\n    y_axis_label=y,\n    tooltips=[\n        (\"p-number\", \"@{participant number}\"),\n        (\"gender\", \"@gender\"),\n        (\"age\", \"@age\"),\n    ],\n)\n\n# Add glyphs\ncolors = {'normal sleepers': '#1f77b4', 'insomniacs': 'orange'}\nfor (insom,), sub_df in df.group_by('insomnia'):\n    category = \"insomniacs\" if insom else \"normal sleepers\"\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=x,\n        y=y,\n        color=colors[category],\n        legend_label=category,\n    )\n\np.legend.location = \"top_left\"\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#saving-bokeh-plots",
    "href": "lessons/exploratory/plotting_with_bokeh.html#saving-bokeh-plots",
    "title": "5  Making plots with Bokeh",
    "section": "5.8 Saving Bokeh plots",
    "text": "5.8 Saving Bokeh plots\nAfter you create your plot, you can save it to a variety of formats. Most commonly you would save them as PNG (for presentations), SVG (for publications in the paper of the past), and HTML (for the paper of the future or sharing with colleagues).\nTo save as a PNG for quick use, you can click the disk icon in the tool bar.\nTo save to SVG, you first change the output backend to 'svg' and then you can click the disk icon again, and you will get an SVG rendering of the plot. After saving the SVG, you should change the output backend back to 'canvas' because it has much better in-browser performance.\n\np.output_backend = 'svg'\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNow, click the disk icon in the plot above to save it.\nAfter saving, we should switch back to canvas.\n\np.output_backend = 'canvas'\n\nYou can also save the figure programmatically using the bokeh.io.export_svgs() function. This requires additional installations, so we will not do it here, but show the code to do it. Again, this will only work if the output backed is 'svg'.\np.output_backend = 'svg'\nbokeh.io.export_svgs(p, filename='insomniac_confidence_correct.svg')\np.output_backend = 'canvas'\nFinally, to save as HTML, you can use the bokeh.io.save() function. This saves your plot as a standalone HTML page. Note that the title kwarg is not the title of the plot, but the title of the web page that will appear on your Browser tab.\n\nbokeh.io.save(\n    p, \n    filename='insomniac_confidence_correct.html', \n    title='Bokeh plot',\n    resources=bokeh.resources.CDN,\n);\n\nThe resulting HTML page has all of the interactivity of the plot and you can, for example, email it to your collaborators for them to explore.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/plotting_with_bokeh.html#computing-environment",
    "href": "lessons/exploratory/plotting_with_bokeh.html#computing-environment",
    "title": "5  Making plots with Bokeh",
    "section": "5.9 Computing environment",
    "text": "5.9 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p polars,bokeh,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\npolars    : 1.27.1\nbokeh     : 3.6.2\njupyterlab: 4.3.6",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html",
    "href": "lessons/exploratory/iqplot.html",
    "title": "6  High level plotting with iqplot",
    "section": "",
    "text": "6.1 Plots with categorical variables\n| Download notebook\nIn this lesson, we do some plotting with a high-level package iqplot. For fun, we will use a data set from Kleinteich and Gorb that features data about strikes of the tongues of frogs on a target. Let’s get the data frame loaded in so we can be on our way.\nLet us first consider the different kinds of data we may encounter as we think about constructing a plot.\nIn practice, ordinal data can be cast as quantitative or treated as categorical with an ordering enforced on the categories (e.g., categorical data [1, 2, 3] becomes ['1', '2', '3'].). Temporal data can also be cast as quantitative, (e.g., second from the start time). We will therefore focus out attention on quantitative and categorical data.\nWhen we made scatter plots in the previous lesson, both types of data were quantitative. We did actually incorporate categorical information in the form of colors of the glyph (insomniacs and normal sleepers being colored differently) and in tooltips.\nBut what if we wanted a single type of measurement, say impact force, for each frog? Here, we have the quantitative impact force data and the categorical frog ID data. One of our axes is now categorical.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#plots-with-categorical-variables",
    "href": "lessons/exploratory/iqplot.html#plots-with-categorical-variables",
    "title": "6  High level plotting with iqplot",
    "section": "",
    "text": "Quantitative data may have continuously varying (and therefore ordered) values.\nCategorical data has discrete, unordered values that a variable can take.\nOrdinal data has discrete, ordered values. Integers are a classic example.\nTemporal data refers to time, which can be represented as dates.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#bar-graph",
    "href": "lessons/exploratory/iqplot.html#bar-graph",
    "title": "6  High level plotting with iqplot",
    "section": "6.2 Bar graph",
    "text": "6.2 Bar graph\nTo demonstrate how to set up a categorical axis with Bokeh, I will make a bar graph of the mean impact force for each of the four frogs. But before I even begin this, I will give you the following piece of advice: Don’t make bar graphs. More on that in a moment.\nBefore we do that, we need to compute the means from the inputted data frame.\n\ndf_mean = df[['ID', 'impact force (mN)']].group_by('ID').mean()\n\n# Take a look\ndf_mean\n\n\nshape: (4, 2)\n\n\n\nID\nimpact force (mN)\n\n\nstr\nf64\n\n\n\n\n\"III\"\n550.1\n\n\n\"I\"\n1530.2\n\n\n\"II\"\n707.35\n\n\n\"IV\"\n419.1\n\n\n\n\n\n\nTo set up a categorical axis, you need to specify the x_range (or y_range if you want the y-axis to be categorical) as a list with the categories you want on the axis when you instantiate the figure. I will make a horizontal bar graph, so I will specify y_range. Also, when I instantiate this figure, because it is not very tall and I do not want the reset tool cut off, I will also explicitly set the tools I want in the toolbar.\n\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=400,\n    x_axis_label='impact force (mN)',\n    y_range=list(df_mean['ID'].sort(descending=True)),\n    tools='pan,wheel_zoom,save,reset'\n)\n\nNow that we have the figure, we can put the bars on. The p.hbar() method populates the figure with horizontal bar glyphs. The right kwarg says what column of the data source dictates how far to the right to show the bar, while the height kwarg says how think the bars are.\nI will also ensure the quantitative axis starts at zero and turn off the grid lines on the categorical axis, which is commonly done.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y='ID',\n    right='impact force (mN)',\n    height=0.6\n)\n\n# Turn off gridlines on categorical axis\np.ygrid.grid_line_color = None\n\n# Start axes at origin on quantitative axis\np.x_range.start = 0\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe similarly make vertical bar graphs specifying x_range and using p.vbar().\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=250,\n    y_axis_label='impact force (mN)',\n    x_range=list(df_mean['ID'].sort()),\n)\n\np.vbar(\n    source=df_mean.to_dict(),\n    x='ID',\n    top='impact force (mN)',\n    width=0.6\n)\n\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#iqplot",
    "href": "lessons/exploratory/iqplot.html#iqplot",
    "title": "6  High level plotting with iqplot",
    "section": "6.3 iqplot",
    "text": "6.3 iqplot\nGenerating the bar graphs was not too painful, even tough we used Bokeh, a low-level plotting library. Nonetheless, we would like to make plots more declaratively. We do not want to have to explicitly pre-process the data, set up the categorical axis, etc. We would like to just provide a data set, say which column(s) is/are categorical and which is quantitative, and then just get our plot.\niqplot generates plots from tidy data frames where one or more columns contain categorical data and the column of interest in the plot is quantitative.\nThere are seven types of plots that iqplot can generate. As you will see, all four of these modes of plotting are meant to give a picture about how the quantitative measurements are distributed for each category.\n\nBox plots: iqplot.box()\nStrip plots: iqplot.strip()\nSpike plots: iqplot.spike()\nStrip-box plots (strip and box plots overlaid): iqplot.stripbox()\nHistograms: iqplot.histogram()\nStrip-histogram plots (strip and histograms overlaid): iqplot.striphistogram()\nECDFs: iqplot.ecdf()\n\nThis first seven arguments are the same for all plots. They are:\n\ndata: A tidy data frame or Numpy array.\nq: The column of the data frame to be treated as the quantitative variable.\ncats: A list of columns in the data frame that are to be considered as categorical variables in the plot. If None, a single box, strip, histogram, or ECDF is plotted.\nq_axis: Along which axis, x or y that the quantitative variable varies. The default is 'x'.\npalette: A list of hex colors to use for coloring the markers for each category. By default, it uses the Glasbey Category 10 color palette from colorcet.\norder: If specified, the ordering of the categories to use on the categorical axis and legend (if applicable). Otherwise, the order of the inputted data frame is used.\np: If specified, the bokeh.plotting.Figure object to use for the plot. If not specified, a new figure is created.\n\nIf data is given as a Numpy array, it is the only required argument. If data is given as a Pandas DataFrame, q must also be supplied. All other arguments are optional and have reasonably set defaults. Any extra kwargs not in the function call signature are passed to bokeh.plotting.figure() when the figure is instantiated.\nWith this in mind, we will put iqplot to use on facial identification data set to demonstrate how we can make each of the seven kinds of plots using the frog data set.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#box-plots-with-iqplot",
    "href": "lessons/exploratory/iqplot.html#box-plots-with-iqplot",
    "title": "6  High level plotting with iqplot",
    "section": "6.4 Box plots with iqplot",
    "text": "6.4 Box plots with iqplot\nAs I discuss below, bar graphs are almost never a good choice for visualization. You distill all of the information in the data set down to one or two summary statistics, and then use giant glyphs to show them. As a start for improvement, you could distill the data set down to five or so summary statistics and show those graphically, as opposed to just one or two.\nBox plots provide such a summary. I will first make one using iqplot.box() and then describe how a box plot is interpreted.\n\np = iqplot.box(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe line in the middle of each box is the median and the top and bottom of the box at the 75th and 25th percentiles, respectively. The distance between the 25th and 75th percentiles is called the interquartile range, or IQR. The whiskers of the box plot extend to the most extreme data point within 1.5 times the interquartile range. If any data points are more extreme than the end of the whisker, they are shown individually, and are often referred to as outliers.\nA box plot can use a useful visualization if you have many data points and it is difficult to plot them all. I rarely find that there are situations where all data cannot be plotted, either with strip plots of ECDFs, which we will cover in a moment, so I generally do not use box plots. Nonetheless, I do not find them too objectionable, as they effectively display important nonparametric summary statistics of your data set.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#plot-all-your-data",
    "href": "lessons/exploratory/iqplot.html#plot-all-your-data",
    "title": "6  High level plotting with iqplot",
    "section": "6.5 Plot all your data",
    "text": "6.5 Plot all your data\nBox plots summarize a data set with summary statistics, but what not plot all your data? You work hard to acquire them. You should show them all. This is a mantra to live by.\n\nPlot all of your data.\n\nLet’s do that now.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#strip-plots",
    "href": "lessons/exploratory/iqplot.html#strip-plots",
    "title": "6  High level plotting with iqplot",
    "section": "6.6 Strip plots",
    "text": "6.6 Strip plots\nA strip plot is like a scatter plot; it puts a glyph for every measured data point. The only difference is that one of the axes is categorical. In this case, you are plotting all of your data.\n\np = iqplot.strip(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is a good plot to make since you are plotting all of your data, but it does have the problem that you cannot tell if multiple data points overlap. We will deal with this in the next lesson when we discuss styling plots.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#spike-plots",
    "href": "lessons/exploratory/iqplot.html#spike-plots",
    "title": "6  High level plotting with iqplot",
    "section": "6.7 Spike plots",
    "text": "6.7 Spike plots\nA spike plot is useful when you want to see how many times a specific value was encountered in your data set. Let’s make a spike plot for the number of times specific impact times were observed.\n\np = iqplot.spike(\n    df,\n    q='impact time (ms)'\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNotice that an impact time of 31 ms was observed eight times, though most impact times were observed once or twice. In the above plot, we have not had a categorical variable to demonstrate how a spike plot looks. If we do have a categorical variable, it is more difficult to display counts on the y-axis, so the proportion of the measurements with a given value are displayed.\n\np = iqplot.spike(\n    df,\n    q='impact time (ms)',\n    cats='ID',\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#strip-box-plots",
    "href": "lessons/exploratory/iqplot.html#strip-box-plots",
    "title": "6  High level plotting with iqplot",
    "section": "6.8 Strip-box plots",
    "text": "6.8 Strip-box plots\nIt is sometimes useful to overlay strip plots with box plots, as the box plots show useful quantile information. This is accomplished using the iqplot.stripbox() function.\n\np = iqplot.stripbox(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#histograms",
    "href": "lessons/exploratory/iqplot.html#histograms",
    "title": "6  High level plotting with iqplot",
    "section": "6.9 Histograms",
    "text": "6.9 Histograms\nIn plotting all of our data in a strip plot, we can roughly see how the data are distributed. There are more measurements where there are more glyphs. We ofter seek to visualize the distribution of the data. Histograms are commonly used for this. They are typically interpreted to as an empirical representation of the probability density function.\n\np = iqplot.histogram(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNote that by default, iqplot includes a rug plot at the bottom of the histogram, showing each measurement. The number of bins are automatically chosen using the Freedman-Diaconis rule.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#strip-histogram",
    "href": "lessons/exploratory/iqplot.html#strip-histogram",
    "title": "6  High level plotting with iqplot",
    "section": "6.10 Strip-histogram",
    "text": "6.10 Strip-histogram\nStrip plots may also be combined with histograms. By default, the histograms are normalized and mirrored, similar to a violin plot.\n\np = iqplot.striphistogram(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#ecdfs",
    "href": "lessons/exploratory/iqplot.html#ecdfs",
    "title": "6  High level plotting with iqplot",
    "section": "6.11 ECDFs",
    "text": "6.11 ECDFs\nI just mentioned that histograms are typically used to display how data are distributed, but it was hard to make out the distributions in the above plot, partly because we do not have very many measurements. As another example I will generate Normally distributed data and plot the histogram. (We will learn how to generate data like this when we study random number generation with NumPy in a future lesson. For now, this is for purposes of discussing plotting options.)\nNote that iqplot can take a Numpy array as the data argument, and makes a plot assuming it contains a single data set.\n\n# Generate normally distributed data\nrng = np.random.default_rng(3252)\nx = rng.normal(size=500)\n\n# Plot the histogram\np = iqplot.histogram(x, rug=False)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks similar to the standard Normal curve we are used to seeing and is a useful comparison to a probability density function (PDF). However, Histograms suffer from binning bias. By binning the data, you are not plotting all of them. In general, if you can plot all of your data, you should. For that reason, I prefer not to use histograms for studying how data are distributed, but rather prefer to use ECDFs, which enable plotting of all data.\nThe ECDF evaluated at x for a set of measurements is defined as\n\\[\\begin{align}\n\\text{ECDF}(x) = \\text{fraction of measurements } \\le x.\n\\end{align}\\]\nWhile the histogram is an attempt to visualize a probability density function (PDF) of a distribution, the ECDF visualizes the cumulative density function (CDF). The CDF, \\(F(x)\\), and PDF, \\(f(x)\\), both completely define a univariate distribution and are related by\n\\[\\begin{align}\nf(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{align}\\]\nThe definition of the ECDF is all that you need for interpretation. Once you get used to looking at CDFs, they will become as familiar to you as PDFs. A peak in a PDF corresponds to an inflection point in a CDF.\nTo make this more clear, let us look at plot of a PDF and ECDF for familiar distributions, the Gaussian and Binomial.\n\n\n\n\n\n\nFigure 6.1: Top: The PDF (left) and CDF (right) of the Normal (a.k.a. Gaussian) distribution. Bottom: The PMF (left) and CDF (right) of the Binomial distribution.\n\n\n\nNow that we know how to interpret ECDFs, lets plot the ECDF for our dummy Normally-distributed data.\n\np = iqplot.ecdf(x)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nLet’s make a set of ECDFs for our frog data.\n\np = iqplot.ecdf(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThough we do not see points, this is still plotting all of your data. The concave corners of the staircase correspond to the measured data. This can be seen by overlaying the “dot” version of the ECDFs.\n\np = iqplot.ecdf(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n    p=p,\n    style='dots',\n    show_legend=False,\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#sec-no-bar-graphs",
    "href": "lessons/exploratory/iqplot.html#sec-no-bar-graphs",
    "title": "6  High level plotting with iqplot",
    "section": "6.12 Don’t make bar graphs",
    "text": "6.12 Don’t make bar graphs\nBar graphs, especially with error bars (in which case they are called dynamite plots), are typically awful. They are pervasive in biology papers. I have yet to find a single example where a bar graph is the best choice. Strip plots (with jitter) or even box plots, are more informative and almost always preferred. In fact, ECDFs are often better even than these. Here is a simple message:\n\nDon’t make bar graphs.\n\nWhat should I do instead you ask? The answer is simple: plot all of your data when you can. If you can’t, box plots are always better than bar graphs.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/iqplot.html#computing-environment",
    "href": "lessons/exploratory/iqplot.html#computing-environment",
    "title": "6  High level plotting with iqplot",
    "section": "6.13 Computing environment",
    "text": "6.13 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/styling_bokeh.html",
    "href": "lessons/exploratory/styling_bokeh.html",
    "title": "7  Styling Bokeh plots",
    "section": "",
    "text": "7.1 Styling Bokeh plots as they are built\n| Download notebook\nWe have seen how to use Bokeh (and the higher-level plotting package iqplot) to make interactive plots. We have seen how to adjust plot size, axis labels, glyph color, etc. We have also seen how to style plots generated with iqplot. But we have just started to touch the surface of how we might customize plots. In this lesson, we investigate ways to stylize Bokeh plots to our visual preferences.\nWe will again make use of the face-matching data set. We’ll naturally start by loading the data set.\nBokeh figures and renderers (which are essentially the glyphs) have a plethora of attributes pertaining to visual appearance that may be adjusted at instantiation and after making a plot.\nA color palette an ordering of colors that are used for glyphs, usually corresponding to categorical data. Colorcet’s Glasbey Category 10 provides a good palette for categorical data, and we store this as our categorical colors for plotting.\ncat_colors = colorcet.b_glasbey_category10\nNow we can build the plot. Since the data are percentages, we will set the axes to go from zero to 100 and enforce that the figure is square. We will also include a title as well so we can style that.\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"confidence when correct\",\n    y_axis_label=\"condifence when incorrect\",\n    title=\"GMFT with sleep conditions\",\n    x_range=[0, 100],\n    y_range=[0, 100],\n)\nIn styling this plot, we will also put the legend outside of the plot area. This is a bit trickier than what we have been doing using the legend_label kwarg in p.scatter(). To get a legend outside of the plot area, we need to:\nNow, we add the glyphs, storing them as variables normal_glyph and insom_glyph.\nnormal_glyph = p.scatter(\n    source=df.filter(~pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[0],\n)\n\ninsom_glyph = p.scatter(\n    source=df.filter(pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[1],\n)\nNow we can construct and add the legend.\n# Construct legend items\nlegend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n# Instantiate legend\nlegend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n# Add the legend to the right of the plot\np.add_layout(legend, 'right')\nNow, let’s take a look at this beauty!\nbokeh.io.show(p)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/styling_bokeh.html#styling-bokeh-plots-as-they-are-built",
    "href": "lessons/exploratory/styling_bokeh.html#styling-bokeh-plots-as-they-are-built",
    "title": "7  Styling Bokeh plots",
    "section": "",
    "text": "Assign each glyph to a variable.\nInstantiate a bokeh.models.Legend object using the stored variables containing the glyphs. This is instantiated as bokeh.models.Legend(items=legend_items), where legend_items is a list of 2-tuples. In each 2-tuple, the first entry is a string with the text used to label the glyph. The second entry is a list of glyphs that have the label.\nAdd the legend to the figure using the add_layout() method.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/styling_bokeh.html#styling-bokeh-plots-after-they-are-built",
    "href": "lessons/exploratory/styling_bokeh.html#styling-bokeh-plots-after-they-are-built",
    "title": "7  Styling Bokeh plots",
    "section": "7.2 Styling Bokeh plots after they are built",
    "text": "7.2 Styling Bokeh plots after they are built\nAfter building a plot, we sometimes want to adjust styling. To do so, we need to change attributes of the object p. For example, let’s look at the font of the x-axis label.\n\np.xaxis.axis_label_text_font\n\n'helvetica'\n\n\nWe can also look at the style and size of the font.\n\np.xaxis.axis_label_text_font_style, p.xaxis.axis_label_text_font_size\n\n('italic', '13px')\n\n\nSo, the default axis labels for Bokeh are italicized 13 pt Helvetica. I personally think this choice if fine, but we may have other preferences.\nTo find out all of the available options to tweak, I usually type something like p. and hit tab to see what the options are. Finding p.xaxis is an option, then type p.xaxis. and hit tab again to see the styling option there.\nUsing this technique, we can set some obnoxious styling for this plot. I will make all of the fonts non-italicized, large papyrus. I can also set the background and grid colors. Note that in Bokeh, any named CSS color or any valid HEX code, entered as a string, is a valid color.\nBefore we do the obnoxious styling, we will do one adjustment that is useful. Note in the above plot that the glyphs at the end of the plot are cropped. We would like the whole glyph to show. To do that, we set the level of the glyphs to be 'overlay'. To do that, we extract the first two elements of the list of renderers, which contains the glyphs, and set the level attribute.\n\np.renderers[0].level = 'overlay'\np.renderers[1].level = 'overlay'\n\nNow we can proceed to make our obnoxious styling.\n\n# Obnoxious fonts\np.xaxis.major_label_text_font = 'papyrus'\np.xaxis.major_label_text_font_size = '14pt'\np.xaxis.axis_label_text_font = 'papyrus'\np.xaxis.axis_label_text_font_style = 'normal'\np.xaxis.axis_label_text_font_size = '20pt'\np.yaxis.major_label_text_font = 'papyrus'\np.yaxis.major_label_text_font_size = '14pt'\np.yaxis.axis_label_text_font = 'papyrus'\np.yaxis.axis_label_text_font_style = 'normal'\np.yaxis.axis_label_text_font_size = '20pt'\np.title.text_font = 'papyrus'\np.title.text_font_size = '18pt'\np.legend.label_text_font = 'papyrus'\n\n# Align the title center\np.title.align = 'center'\n\n# Set background and grid color\np.background_fill_color = 'blanchedalmond'\np.legend.background_fill_color = 'chartreuse'\np.xgrid.grid_line_color = 'azure'\np.ygrid.grid_line_color = 'azure'\n\n# Make the ticks point inward (I *hate* this!)\n# Units are pixels that the ticks extend in and out of plot\np.xaxis.major_tick_out = 0\np.xaxis.major_tick_in = 10\np.xaxis.minor_tick_out = 0\np.xaxis.minor_tick_in = 5\np.yaxis.major_tick_out = 0\np.yaxis.major_tick_in = 10\np.yaxis.minor_tick_out = 0\np.yaxis.minor_tick_in = 5\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is truly hideous, but it demonstrates how we can go about styling plots after they are made.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/styling_bokeh.html#bokeh-themes",
    "href": "lessons/exploratory/styling_bokeh.html#bokeh-themes",
    "title": "7  Styling Bokeh plots",
    "section": "7.3 Bokeh themes",
    "text": "7.3 Bokeh themes\nBokeh has several built-in themes which you can apply to all plots in a given document (e.g., in a notebook). Please see the documentation for details about the built-in themes. I personally prefer the default styling to all of their themes, but your opinion may differ.\nYou may also specify custom themes using JSON or YAML. As an example, we can specify a theme such that plots are styled like the default style of the excellent plotting packages Vega-Altair/Vega-Lite/Vega. If we use JSON formatting, we can specify a theme as a dictionary of dictionaries, as below.\n\naltair_theme_dict = {\n    \"attrs\": {\n        \"Axis\": {\n            \"axis_line_color\": \"dimgray\",\n            \"minor_tick_out\": 0,\n            \"major_tick_in\": 0,\n            \"major_tick_line_color\": \"dimgray\",\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"8pt\",\n            \"axis_label_text_font_style\": \"bold\",\n        },\n        \"Scatter\": {\n            \"fill_alpha\": 0, \n            \"line_width\": 2, \n            \"size\": 5, \n            \"line_alpha\": 0.7,\n        },\n        \"ContinuousTicker\": {\n            \"desired_num_ticks\": 10\n        },\n        \"figure\": {\n            \"frame_width\": 350, \n            \"frame_height\": 300,\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"lightgray\",\n            \"level\": \"underlay\",\n        },\n        \"Legend\": {\n            \"border_line_color\": None,\n            \"background_fill_color\": None,\n            \"label_text_font_size\": \"7.5pt\",\n            \"title_text_font_size\": \"8pt\",\n            \"title_text_font_style\": \"bold\",\n        },\n        \"Renderer\": {\n            \"level\": \"overlay\"\n        },\n        \"Title\": {\n            \"align\": \"center\",\n        },\n    }\n}\n\nTo activate the theme, we convert it to a Bokeh theme and then add it to the curdoc(), or the current document.\n\naltair_theme = bokeh.themes.Theme(json=altair_theme_dict)\n\nbokeh.io.curdoc().theme = altair_theme\n\nNow the theme is activated, and future plots will have this theme by default. Let’s remake our plot using this theme. For convenience later on, I will write a function to generate this scatter plot that we will use to test various styles.\n\ndef gfmt_plot():\n    \"\"\"Make a plot for testing out styles in this notebook.\"\"\"\n    p = bokeh.plotting.figure(\n        frame_width=300,\n        frame_height=300,\n        x_axis_label=\"confidence when correct\",\n        y_axis_label=\"condifence when incorrect\",\n        title=\"GMFT with sleep conditions\",\n        x_range=[0, 100],\n        y_range=[0, 100],\n    )\n\n    normal_glyph = p.scatter(\n        source=df.filter(~pl.col('insomnia')).to_dict(),\n        x=\"confidence when correct\",\n        y=\"confidence when incorrect\",\n        color=cat_colors[0],\n    )\n\n    insom_glyph = p.scatter(\n        source=df.filter(pl.col('insomnia')).to_dict(),\n        x=\"confidence when correct\",\n        y=\"confidence when incorrect\",\n        color=cat_colors[1],\n    )\n\n    # Construct legend items\n    legend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n    # Instantiate legend\n    legend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n    # Add the legend to the right of the plot\n    p.add_layout(legend, 'right')\n    \n    return p\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nWe could also style our plots to resemble the default “dark” styling of Seaborn.\n\nseaborn_theme_dict = {\n    \"attrs\": {\n        \"figure\": {\n            \"background_fill_color\": \"#eaeaf2\",\n            \"frame_height\": 300,\n            \"frame_width\": 350,\n        },\n        \"Axis\": {\n            \"axis_line_color\": None,\n            \"minor_tick_out\": 0,\n            \"major_tick_in\": 0,\n            \"major_tick_out\": 0,\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_style\": \"normal\",\n        },\n        \"Legend\": {\n            \"border_line_color\": \"darkgray\",\n            \"background_fill_color\": \"#eaeaf2\",\n            \"border_line_width\": 0.75,\n            \"label_text_font_size\": \"7.5pt\",\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"#FFFFFF\", \n            \"grid_line_width\": 0.75,\n        },\n        \"Title\": {\n            \"align\": \"center\",\n            'text_font_style': 'normal',\n            'text_font_size': \"8pt\",\n        },\n    }\n}\n\nseaborn_theme = bokeh.themes.Theme(json=seaborn_theme_dict)\nbokeh.io.curdoc().theme = seaborn_theme\n\nLet’s make the plot, yet again, with this new styling.\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nFinally, we can specify a style I like. Note that I do not specify that the glyphs are at an overlay level, since by default Bokeh will scale the axes such that the glyphs are fully contained in the plot area. I also put the toolbar above the plot, which is usually not a problem because I generally prefer not to title my plots, opting instead for good textual description in captions or in surrounding text.\n\njb_theme_dict = {\n    \"attrs\": {\n        \"Axis\": {\n            \"axis_line_color\": \"dimgray\",\n            \"major_tick_line_color\": \"dimgray\",\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"9pt\",\n            \"axis_label_text_font_style\": \"bold\",\n        },\n        \"Scatter\": {\n            \"size\": 5, \n            \"fill_alpha\": 0.8,\n            \"line_width\": 0,\n        },\n        \"figure\": {\n            \"frame_height\": 300,\n            \"frame_width\": 350,\n            \"toolbar_location\": \"above\",\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"lightgray\",\n            \"level\": \"underlay\",\n        },\n        \"Legend\": {\n            \"border_line_color\": \"darkgray\",\n            \"border_line_width\": 0.75,\n            \"background_fill_color\": \"#ffffff\",\n            \"background_fill_alpha\": 0.7,\n            \"label_text_font\": \"helvetica\", \n            \"label_text_font_size\": \"7.5pt\",\n            \"title_text_font\": \"helvetica\", \n            \"title_text_font_size\": \"8pt\", \n            \"title_text_font_style\": \"bold\", \n        },\n        \"Renderer\": {\n            \"level\": \"overlay\"\n        },\n        \"Title\": {\n            \"text_font\": \"helvetica\",\n            \"text_font_size\": \"10pt\",\n            'text_font_style': 'bold',\n        },\n    }\n}\n\njb_theme = bokeh.themes.Theme(json=jb_theme_dict)\nbokeh.io.curdoc().theme = jb_theme\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nFinally, if I were to make this particular plot, I would do it without a title and with axes leaving a little buffer.\n\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"confidence when correct\",\n    y_axis_label=\"condifence when incorrect\",\n    x_range=[-2.5, 102.5],\n    y_range=[-2.5, 102.5],\n)\n\nnormal_glyph = p.scatter(\n    source=df.filter(~pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[0],\n)\n\ninsom_glyph = p.scatter(\n    source=df.filter(pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[1],\n)\n\n# Construct legend items\nlegend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n# Instantiate legend\nlegend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n# Add the legend to the right of the plot\np.add_layout(legend, 'right')\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYou can play with these themes and develop your own style as you see fit. As you can see, Bokeh is highly configurable, and you can really make the plots your own!",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/styling_bokeh.html#computing-environment",
    "href": "lessons/exploratory/styling_bokeh.html#computing-environment",
    "title": "7  Styling Bokeh plots",
    "section": "7.4 Computing environment",
    "text": "7.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\nbokeh     : 3.6.2\njupyterlab: 4.3.6",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/overplotting.html",
    "href": "lessons/exploratory/overplotting.html",
    "title": "8  Dealing with overplotting",
    "section": "",
    "text": "8.1 The data set\n| Download notebook\nData set download\nWe are often faced with data sets that have too many points to plot and when overlayed, the glyphs obscure each other. This is called overplotting. In this lesson, we will explore strategies for dealing with this issue.\nAs an example, we will consider a flow cytometry data set consisting of 100,000 data points. The data appeared in Razo-Mejia, et al., Cell Systems, 2018, and the authors provided the data here. We will consider one CSV file from one flow cytometry experiment extracted from this much larger data set, available here: https://s3.amazonaws.com/bebi103.caltech.edu/data/20160804_wt_O2_HG104_0uMIPTG.csv.\nLet’s read in the data and take a look.\ndf = pl.read_csv(\n    os.path.join(data_path, '20160804_wt_O2_HG104_0uMIPTG.csv'), \n    comment_prefix='#'\n).select(cs.exclude(''))\n\ndf.head()\n\n\nshape: (5, 13)\n\n\n\nHDR-T\nFSC-A\nFSC-H\nFSC-W\nSSC-A\nSSC-H\nSSC-W\nFITC-A\nFITC-H\nFITC-W\nAPC-Cy7-A\nAPC-Cy7-H\nAPC-Cy7-W\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n0.418674\n6537.148438\n6417.625\n133513.125\n24118.714844\n22670.142578\n139447.21875\n11319.865234\n6816.254883\n217673.40625\n55.798954\n255.540833\n28620.398438\n\n\n2.563462\n6402.21582\n5969.625\n140570.171875\n23689.554688\n22014.142578\n141047.390625\n1464.151367\n5320.254883\n36071.4375\n74.539612\n247.540833\n39468.460938\n\n\n4.92126\n5871.125\n5518.852539\n139438.421875\n16957.433594\n17344.511719\n128146.859375\n5013.330078\n7328.779785\n89661.203125\n-31.788519\n229.903214\n-18123.212891\n\n\n5.450112\n6928.865723\n8729.474609\n104036.078125\n13665.240234\n11657.869141\n153641.3125\n879.165771\n6997.65332\n16467.523438\n118.226028\n362.191162\n42784.375\n\n\n9.57075\n11081.580078\n6218.314453\n233581.765625\n43528.683594\n22722.318359\n251091.96875\n2271.960693\n9731.527344\n30600.585938\n20.693352\n210.486893\n12885.928711\nEach row is a single object (putatively a single cell) that passed through the flow cytometer. Each column corresponds to a variable in the measurement. As a first step in analyzing flow cytometry data, we perform gating in which properly separated cells (as opposed to doublets or whatever other debris goes through the cytometer) are chosen based on their front and side scattering. Typically, we make a plot of front scattering (FSC-A) versus side scattering (SSC-A) on a log-log scale. This is the plot we want to make in this part of the lesson.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with overplotting</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/overplotting.html#lets-plot",
    "href": "lessons/exploratory/overplotting.html#lets-plot",
    "title": "8  Dealing with overplotting",
    "section": "8.2 Let’s plot",
    "text": "8.2 Let’s plot\nLet’s explore this data using the tools we seen so far. This is just a simple call to HoloViews. We will only use 5,000 data points so as not to choke the browser with all 100,000 (though we of course want to plot all 100,000). The difficulty with large data sets will become clear.\n\np = bokeh.plotting.figure(\n    frame_height=300,\n    frame_width=300,\n    x_axis_label=\"SSC-A\",\n    y_axis_label=\"FSC-A\",\n    x_axis_type=\"log\",\n    y_axis_type=\"log\",\n)\n\nsource = bokeh.models.ColumnDataSource(df[::20].to_dict())\n\np.scatter(source=source, x=\"SSC-A\", y=\"FSC-A\", marker=\"circle\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYikes. We see that many of the glyphs are obscuring each other, preventing us from truly visualizing how the data are distributed. We need to explore options for dealing with this overplotting.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with overplotting</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/overplotting.html#applying-transparency",
    "href": "lessons/exploratory/overplotting.html#applying-transparency",
    "title": "8  Dealing with overplotting",
    "section": "8.3 Applying transparency",
    "text": "8.3 Applying transparency\nOne strategy to deal with overplotting is to specify transparency of the glyphs so we can visualize where they are dense and where they are sparse. We specify transparency with the fill_alpha and line_alpha options for the glyphs. An alpha value of zero is completely transparent, and one is completely opaque.\n\np = bokeh.plotting.figure(\n    frame_height=300,\n    frame_width=300,\n    x_axis_label=\"SSC-A\",\n    y_axis_label=\"FSC-A\",\n    x_axis_type=\"log\",\n    y_axis_type=\"log\",\n)\n\nsource = bokeh.models.ColumnDataSource(df[::20].to_dict())\n\np.scatter(source=source, x=\"SSC-A\", y=\"FSC-A\", fill_alpha=0.05, line_alpha=0)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe transparency helps us see where the density is, but it washes out all of the detail for points away from dense regions. There is the added problem that we cannot populate the plot with too many glyphs, so we can’t plot all of our data. We should see alternatives.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with overplotting</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/overplotting.html#plotting-the-density-of-points",
    "href": "lessons/exploratory/overplotting.html#plotting-the-density-of-points",
    "title": "8  Dealing with overplotting",
    "section": "8.4 Plotting the density of points",
    "text": "8.4 Plotting the density of points\nWe could instead make a hex-tile plot, which is like a two-dimensional histogram. The space in the x-y plane is divided up into hexagons, which are then colored by the number of points that we in each bin.\nBokeh’s implementation of hex tiles does not cleanly allow for logarithmic axes, so we need to compute the logarithms by hand.\n\ndf = df.with_columns(\n    pl.col('SSC-A').log10().alias('log SSC-A'),\n    pl.col('FSC-A').log10().alias('log FSC-A'),\n)\n\nBokeh provides a useful function, bokeh.util.hex.hexbin(), that takes a set of (x, y) data points, divides the space of the plot up into hexagonal segments, and counts the number of points that lie within each hexagon. The result is a data frame with columns 'q' and 'r' that give the centers of the hex tiles as coordinates on an integer-based hexagonal lattice. Column 'counts' gives the count of points lying within each hexagon. The size of the hexagons are set by the size argument.\nWhen computing the hexbins, it is important to remove any NaNs.\n\n# Side of hexagons, center to vertex\nhexsize = 0.05\n\n# Data with NaN's dropped\ndf_nonan = df[['log SSC-A', 'log FSC-A']].drop_nans()\n\nhexbins = bokeh.util.hex.hexbin(df_nonan['log SSC-A'], df_nonan['log FSC-A'], size=hexsize)\n\n# Convert to column data source\nsource = bokeh.models.ColumnDataSource(hexbins)\n\nNow, we can make a plot and populate it with the hextile glyphs.\n\np = bokeh.plotting.figure(\n    frame_height=300,\n    frame_width=300,\n    x_axis_label=\"log₁₀ SSC-A\",\n    y_axis_label=\"log₁₀ FSC-A\",\n    match_aspect=True,\n)\n\ncoloring = bokeh.transform.linear_cmap(\n        \"counts\", \"Viridis256\", 0, max(hexbins.counts)\n    )\n\np.hex_tile(\n    source=source,\n    q=\"q\",\n    r=\"r\",\n    size=hexsize,\n    line_color=coloring,\n    fill_color=coloring,\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis plot is useful for seeing the distribution of points, but is not really a plot of all the data. We should strive to do better. Enter DataShader.",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with overplotting</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/overplotting.html#datashader",
    "href": "lessons/exploratory/overplotting.html#datashader",
    "title": "8  Dealing with overplotting",
    "section": "8.5 Datashader",
    "text": "8.5 Datashader\nDatashader allows us to plot all points for millions to billions of points (so 100,000 is a piece of cake!). It works like Google Maps: it displays raster images on the plot that show the level of detail of the data points appropriate for the level of zoom. It adjusts the image as you interact with the plot, so the browser never gets hit with a large number of individual glyphs to render. Furthermore, it shades the color of the data points according to the local density.\nTo make a datashaded version of this plot directly using Bokeh, we have to write a fair amount of boilerplate code to get interactivity when we zoom in and out of the plot. We will skip this, and instead use the built-in Datashader support of HoloViews. HoloViews is a quite powerful high-level plotting package (we discuss it in an auxiliary lesson). For now, we will treat it more or less as a black box for generating plots of points and lines that can then be datashaded.\nNote, though, that as was the case with hex tiles using Bokeh, HoloViews currently cannot display datashaded plots with a log axis, so we have to manually compute the logarithms for the data set.\nWe start by making an hv.Points element, which is a scatter plot. It requires two keyword arguments, data, which provides the data frame with the data to be plotted, and kdims, which is a list containing two entries corresponding to the columns of the data frame that respectively give the x- and y-coordinates of the points.\n\n# Generate HoloViews Points Element\npoints = hv.Points(\n    data=df.to_dict(),\n    kdims=['log SSC-A', 'log FSC-A'],\n)\n\nAfter we make the points element, we can datashade it using holoviews.operation.datashader.datashade(). Note that we need to explicitly import holoviews.operation.datashader (which we did in the first code cell of this notebook) before using it.\n\n# Datashade\nhv.operation.datashader.datashade(\n    points,\n).opts(\n    frame_width=250, \n    frame_height=250, \n    padding=0.05,\n    show_grid=True,\n)\n\n\n\n\n\n  \n\n\n\n\nNote that the opts() method specifies some of the properties of the plot.\nIn the HTML-rendered view of this notebook, the data set is rendered as a rastered image. In a running notebook, zooming in and out of the image results in re-display of the data as an image that is appropriate for the level of zoom. It is important to remember that actively using Datashader requires a running Python engine.\nNote that, unlike with transparency, we can see each data point. This is the power of Datashader. Before we delve into the details, you may have a few questions:\n\n8.5.1 First, what exactly is Datashader?\nDatashader is not a plotting package, but a Python library that aggregates data (more on that in a bit) in a way that can then be rendered by your favorite plotting package: Holoviews, Bokeh, and others. This graphic from the Datashader documentation illustrates how all these packages interact with each other:\n\n\n\n\n\n\nFigure 8.1: A schematic of how Datashader fits into a data visualization workflow. Copyright Datashader authors.\n\n\n\nWe see that the data analyst (i.e. you!) can harness the power of Datashader by using HoloViews as a convenient high-level wrapper. While you have the option to interact with Datashader (and Bokeh) directly, this is very cumbersome (HoloViews takes care of this for you) and is generally not necessary for our purposes.\n\n\n8.5.2 Second, how does Datashader work?\nDatashader is extremely powerful with a lot happening under the hood. It’s important to understand how Datashader works if you want to work beyond the default settings. The Datashader pipeline is as follows:\n\n\n\n\n\n\nFigure 8.2: The pipeline of processed that Datashader does to get a zoomable raster representation of data.\n\n\n\nThe first two steps are no different from what we’ve seen so far.\n\nWe start with a (tidy!) dataset that is well annotated for easy plotting.\nWe select the scene which we want to ultimately render. We are not yet plotting anything: we are just specifying what we would like to plot. That’s what we were doing when we called\n\npoints = hv.Points(\n    data=df,\n    kdims=['log SSC-A', 'log FSC-A'],\n)\nThe next step is where the Datashader magic comes in.\n\nAggregation is the method by which the data are binned. In the example above, we binned by count, although there are other options. The beauty of Datashader is the this aggregation is re-computed quickly whenever you zoom in on your plot. This allows for rapid exploration of how your data are distributed at any scale.\nColor mapping is the process by which the quantitative information of the aggregation is visualized. The default color map used above was varying shades of blue, but we could easily use a different color map by using the cmap keyword argument of the datashade operation.\n\n\n# Datashade\nhv.operation.datashader.datashade(\n    points,\n    cmap=list(bokeh.palettes.Magma10),\n).opts(\n    frame_width=250, \n    frame_height=250,  \n    padding=0.05,\n    show_grid=True,\n)\n\n\n\n\n\n  \n\n\n\n\nAnd lastly comes the plotting.\n\nAs mentioned previously, Datashader is not a plotting package, but it computes rasterized images that can then be rendered nearly effortlessly with HoloViews or other plotting packages.\n\nNow that we have a better handle on how Datashader work, let’s play around with more some more options.\n\n\n8.5.3 Dynamic spreading\nOn some retina displays, a pixel is very small and seeing an individual point is difficult. To account for this, we can also apply dynamic spreading which makes the size of each point bigger than a single pixel.\n\n# Datashade with spreading of points\nhv.operation.datashader.dynspread(\n    hv.operation.datashader.datashade(\n        points,\n    )\n).opts(\n    frame_width=250, \n    frame_height=250, \n    padding=0.05,\n    show_grid=True,\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n8.5.4 Datashading paths random walk\nDatashader also works on lines and paths. This can be particularly useful in EEG/MEG and electrophysiology data traces, and we will soon use it in those cases. For now, I will use Datashader to visualize the scale invariance of random walks, something that has always fascinated me. First, I will generate a random walk of 10 million steps. For each step, the walker take a unit step in a random direction in a 2D plane.\n\nn_steps = 10000000\ntheta = np.random.uniform(low=0, high=2*np.pi, size=n_steps)\nx = np.cos(theta).cumsum()\ny = np.sin(theta).cumsum()\n\nTo plot lines with HoloViews, the syntax is the same as plotting points; we just use hv.Path() instead of hv.Points(). If we do not have the data in data frames, we can instead specify the data as a 2-tuple of Numpy arrays and the kdims keyword argument them becomes the axis labels.\n\npath = hv.Path(\n    data=(x, y), \n    kdims=['x', 'y']\n)\n\nNow, we can datashade it!\n\nhv.operation.datashader.datashade(\n    path\n).opts(\n    frame_height=300,\n    frame_width=300,\n    padding=0.05,\n    show_grid=True,\n)",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with overplotting</span>"
    ]
  },
  {
    "objectID": "lessons/exploratory/overplotting.html#computing-environment",
    "href": "lessons/exploratory/overplotting.html#computing-environment",
    "title": "8  Dealing with overplotting",
    "section": "8.6 Computing environment",
    "text": "8.6 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,pandas,bokeh,holoviews,datashader,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npandas    : 2.2.3\nbokeh     : 3.6.2\nholoviews : 1.20.2\ndatashader: 0.18.0\njupyterlab: 4.3.6",
    "crumbs": [
      "Data display",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with overplotting</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability.html",
    "href": "lessons/probability/probability.html",
    "title": "Probability: The foundation for generative modeling",
    "section": "",
    "text": "Any data set we encounter was generated by some process, usually a process that involved the ingenuity, blood, sweat, and tears of an experimenter. It we want to learn something more general about nature from acquired data, we need to have a model for the data generation process. Rob Phillips said it beautifully in his book Physical Biology of the Cell, “Quantitative data demand quantitative models.” We call models that describe the process of generating data generative models.\nWe will see in the following lessons that building generative models requires the mathematical machinery of probability. We will proceed with a lack of formality, but will nonetheless give useful working definitions of probability and aspects thereof with an eye for putting them to use for modeling and interpreting data.",
    "crumbs": [
      "Probability: The foundation for generative modeling"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html",
    "href": "lessons/probability/probability_logic_of_science.html",
    "title": "9  Probability as the logic of science",
    "section": "",
    "text": "9.1 What is statistical inference?\nIn the figure below, we have a sketch of the scientific processes. This cycle repeats itself as we explore nature and learn more. In the boxes are milestones, and along the arrows in orange text are the tasks that get us to these milestones.\nLet’s consider the tasks and their milestones. We start in the lower left.\nAs we designed our experiment under our hypothesis, we used deductive logic to say, “If A is true, then B is true,” where A is our hypothesis and B is an experimental observation. This was deductive inference.\nNow, let’s say we observe B. Does this make A true? Not necessarily. But it does make A more plausible. This is called a weak syllogism. As an example, consider the following hypothesis/observation pair.\nBecause B was observed, A is more plausible. A is not necessarily true, but definitely more plausible.\nStatistical inference is the business of quantifying how much more plausible A is after observing B. In order to do statistical inference, we need a way to quantify plausibility. Probability serves this role.\nSo, statistical inference requires a probability theory. Thus, probability theory is a generalization of logic. Due to this logical connection and its crucial role in science, E. T. Jaynes said that probability is the “logic of science.”",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#sec-what-is-statistical-inference",
    "href": "lessons/probability/probability_logic_of_science.html#sec-what-is-statistical-inference",
    "title": "9  Probability as the logic of science",
    "section": "",
    "text": "A = Wastewater injection after hydraulic fracturing, known as fracking, can lead to greater occurrence of earthquakes.\nB = The frequency of earthquakes in Oklahoma has increased 100 fold since 2010, when fracking became common practice there.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#the-problem-of-probability",
    "href": "lessons/probability/probability_logic_of_science.html#the-problem-of-probability",
    "title": "9  Probability as the logic of science",
    "section": "9.2 The problem of probability",
    "text": "9.2 The problem of probability\nWe know what we need, a theory called probability to quantify plausibility. We more formally1 defined probably last term. We will not formally define probability here, but use our common sense reasoning of it. Nonetheless, it is important to understand that there are two dominant interpretations of probability.\n\n9.2.1 Frequentist probability.\nIn the frequentist interpretation of probability, the probability \\(P(A)\\) represents a long-run frequency over a large number of identical repetitions of an experiment. These repetitions can be, and often are, hypothetical. The event \\(A\\) is restricted to propositions about random variables, a quantity that can very meaningfully from experiment to experiment.2\n\n\n9.2.2 Bayesian probability.\nHere, \\(P(A)\\) is interpreted to directly represent the degree of belief, or plausibility, about \\(A\\). So, \\(A\\) can be any logical proposition.\nYou may have heard about a split, or even a fight, between people who use Bayesian and frequentist interpretations of probability applied to statistical inference. There is no need for a fight. The two ways of approaching statistical inference differ in their interpretation of probability, the tool we use to quantify plausibility. Both are valid.\nIn my opinion, the Bayesian interpretation of probability is more intuitive to apply to scientific inference. It always starts with a simple probabilistic expression and proceeds to quantify plausibility. It is conceptually cleaner to me, since we can talk about plausibility of anything, including parameter values. In other words, Bayesian probability serves to quantify our own knowledge, or degree of certainty, about a hypothesis or parameter value. Conversely, in frequentist statistical inference, the parameter values are fixed, and we can only study how repeated experiments will convert the real parameter value to an observed real number.\nGoing forward, we will use the Bayesian interpretation of probability.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#desiderata-for-bayesian-probability",
    "href": "lessons/probability/probability_logic_of_science.html#desiderata-for-bayesian-probability",
    "title": "9  Probability as the logic of science",
    "section": "9.3 Desiderata for Bayesian probability",
    "text": "9.3 Desiderata for Bayesian probability\nIn 1946, R. Cox laid out a pair of rules based on some desired properties of probability as a quantifier of plausibility. These ideas were expanded on by E. T. Jaynes in the 1970s. The desiderata are\n\nProbability is represented by real numbers.\nProbability must agree with rationality. As more information is supplied, probability must rise in a continuous, monotonic manner. The deductive limit must be obtained where appropriate.\nProbability must be consistent.\n\nStructure consistency: If a result is reasoned in more than one way, we should get the same result.\nPropriety: All relevant information must be considered.\nJaynes consistency: Equivalent states of knowledge must be represented by equivalent probability.\n\n\nBased on these desiderata, we can work out important results that a probability function must satisfy. I pause to note that one can generally define probability without a specific interpretation in mind, and it is valid for both Bayesian and frequentist interpretations, and we did this last term.\nTwo results of these desiderata (which we will state, but not prove here) are the sum rule and the product rule.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#the-sum-rule-the-product-rule-and-conditional-probability",
    "href": "lessons/probability/probability_logic_of_science.html#the-sum-rule-the-product-rule-and-conditional-probability",
    "title": "9  Probability as the logic of science",
    "section": "9.4 The sum rule, the product rule, and conditional probability",
    "text": "9.4 The sum rule, the product rule, and conditional probability\nThe sum rule says that the probability of all events must add to unity. Let \\(A^c\\) be all events except \\(A\\), called the complement of \\(A\\). Then, the sum rule states that\n\\[\\begin{aligned}\nP(A) + P(A^c) = 1.\n\\end{aligned} \\tag{9.1}\\]\nNow, let’s say that we are interested in event \\(A\\) happening given that event \\(B\\) happened. So, \\(A\\) is conditional on \\(B\\). We denote this conditional probability as \\({P(A\\mid B)}\\). Given this notion of conditional probability, we can write the sum rule as\n\\[\\begin{aligned}\nP(A\\mid B) + P(A^c \\mid B) = 1,\n\\end{aligned} \\tag{9.2}\\]\nfor any \\(B\\).\nThe product rule states that\n\\[\\begin{aligned}\nP(A, B) = P(A\\mid B)\\, P(B),\n\\end{aligned} \\tag{9.3}\\]\nwhere \\(P(A,B)\\) is the probability of both \\(A\\) and \\(B\\) happening. The product rule is also referred to as the definition of conditional probability. It can similarly be expanded as we did with the sum rule.\n\\[\\begin{aligned}\nP(A, B\\mid C) = P(A\\mid B, C)\\, P(B \\mid C),\n\\end{aligned} \\tag{9.4}\\]\nfor any \\(C\\).",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#sec-bayes-applied-to-science",
    "href": "lessons/probability/probability_logic_of_science.html#sec-bayes-applied-to-science",
    "title": "9  Probability as the logic of science",
    "section": "9.5 Application to scientific measurement",
    "text": "9.5 Application to scientific measurement\nThis is all a bit abstract. Let's bring it into the realm of scientific experiment. We'll assign meanings to these things we have been calling \\(A\\), \\(B\\), and \\(C\\).\n\\[\\begin{aligned}\n\\begin{aligned}\nA &= \\text{hypothesis (or set of parameter values), } \\theta, \\\\\nB &= \\text{Measured data set, } y,\\\\\nC &= \\text{All other information we know, } I.\n\\end{aligned}\n\\end{aligned}\\]\nSo, we may be interested in the probability of obtaining a data set \\(y\\) given some set of parameters \\(\\theta\\) . In other words, we want to learn about \\(P(y|\\theta)\\).\nTo go a bit further, let’s rewrite the product rule.\n\\[\\begin{aligned}\nP(\\theta, y\\mid I) = P(\\theta \\mid y, I)\\, P(y \\mid I).\n\\end{aligned} \\tag{9.5}\\]\nWe have explicitly written all other information we know going into the experiment as \\(I\\). This is always present, so henceforth we will not write it, but we should keep in mind that we are not doing science in a vacuum; \\(I\\) is always there.\n\\[\\begin{aligned}\nP(\\theta, y) = P(\\theta \\mid y)\\, P(y).\n\\end{aligned} \\tag{9.6}\\]\nAhoy! The quantity \\(P(\\theta \\mid y)\\) is exactly what we want from our statistical inference. This is the probability for values of a parameter, given measured data.\nBut wait a minute. The parameter \\(\\theta\\) is not something that can vary meaningfully from experiment to experiment; it is not a random variable. So, in the frequentist picture, we cannot assign a probability to it. That is, \\(P(\\theta\\mid y)\\) and \\(P(y, \\theta)\\) do not make any sense. So, in the frequentist perspective, we can really only analyze \\(P(y\\mid \\theta)\\). But in a Bayesian perspective, we can analyze what we want, \\(P(\\theta\\mid y)\\)!\nNow, how do we compute it \\(P(\\theta\\mid y)\\)?",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#sec-bayes-theorem",
    "href": "lessons/probability/probability_logic_of_science.html#sec-bayes-theorem",
    "title": "9  Probability as the logic of science",
    "section": "9.6 Bayes’s Theorem",
    "text": "9.6 Bayes’s Theorem\nNote that because \"and\" is commutative, \\(P(\\theta, y) = P(y, \\theta)\\). So, we apply the product rule to both sides of the seemingly trivial equality.\n\\[\\begin{aligned}\n  P(\\theta \\mid y)\\, P(y) =  P(\\theta, y)\n  = P(y, \\theta) = P(y \\mid \\theta)\\, P(\\theta).\\end{aligned} \\tag{9.7}\\]\nIf we take the terms at the beginning and end of this equality and rearrange, we get\n\\[\\begin{aligned}\nP(\\theta \\mid y) = \\frac{P(y \\mid \\theta)\\, P(\\theta)}{P(y)}.\n\\end{aligned} \\tag{9.8}\\]\nThis result is called Bayes’s theorem. This is far more instructive in terms of how to compute our goal, which is the left hand side. The quantities on the right hand side all have meaning. We will talk about the meaning of each term in turn, and this is easier to do using their names; each item in Bayes’s theorem has a name.\n\\[\\begin{aligned}\n\\text{posterior} = \\,\\frac{\\text{likelihood} \\times \\text{prior}}{\\text{evidence}}.\n\\end{aligned} \\tag{9.9}\\]\n\n9.6.1 The prior probability.\nFirst, consider the prior, \\(P(\\theta)\\). As probability is a measure of plausibility, or how believable a hypothesis is. This represents the plausibility about hypothesis \\(\\theta\\) given everything we know before we did the experiment to get the data.\n\n\n9.6.2 The likelihood.\nThe likelihood, \\(P(y\\mid \\theta)\\), describes how likely it is to acquire the observed data, given the hypothesis or parameter value \\(\\theta\\). It also contains information about what we expect from the data, given our measurement method. Is there noise in the instruments we are using? How do we model that noise? These are contained in the likelihood.\n\n\n9.6.3 The evidence.\nI will not talk much about this here, except to say that the evidence, \\(P(y)\\) can be computed from the likelihood and prior, and is also called the marginal likelihood, a name whose meaning will become clear in the next section.3\n\n\n9.6.4 The posterior probability.\nThis is what we are after, \\(P(\\theta\\mid y)\\). How plausible is the hypothesis or parameter value, given that we have measured some new data? It is calculated directly from the likelihood and prior (since the evidence is also computed from them). Computing the posterior distribution constitutes the bulk of inference tasks.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#sec-bayes-notation",
    "href": "lessons/probability/probability_logic_of_science.html#sec-bayes-notation",
    "title": "9  Probability as the logic of science",
    "section": "9.7 Notation of parts of Bayes’s Theorem",
    "text": "9.7 Notation of parts of Bayes’s Theorem\nThe symbol \\(P\\) to denote probability is a bit overloaded. To help aid in notation, we will use the following conventions going forward in the class.\n\nProbabilities or probability densities describing measured data are denoted with \\(f\\).\nProbabilities or probability densities describing parameter values, hypotheses, or other non-measured quantities, are denoted with \\(g\\).\nA set of parameters for a given model are denoted \\(\\theta\\).\n\nSo, if we were to write down Bayes’s theorem for a parameter estimation problem, it would be\n\\[\\begin{aligned}\ng(\\theta \\mid y) = \\frac{f(y\\mid \\theta)\\,g(\\theta)}{f(y)}.\n\\end{aligned} \\tag{9.10}\\]\nProbabilities or probability densities written with a \\(g\\) denote the prior or posterior, and those with an \\(f\\) denote the likelihood or evidence.\nWe can also define a joint probability, \\(\\pi(y, \\theta) = f(y\\mid \\theta)\\,g(\\theta)\\), such that\n\\[\\begin{aligned}\ng(\\theta \\mid y) = \\frac{\\pi(y,\\theta)}{f(y)}.\n\\end{aligned} \\tag{9.11}\\]\nNote that we will use this notation in the context of Bayesian inference, and we may generally speak about joint probability density functions, for example, using \\(f(x, y)\\). The use of \\(f\\) for likelihoods and evidence, \\(g\\) for priors and posteriors, and \\(\\pi\\) for joint probabilities in the context of Bayesian modeling helps us keep track of what is what conceptually.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#sec-marginalization",
    "href": "lessons/probability/probability_logic_of_science.html#sec-marginalization",
    "title": "9  Probability as the logic of science",
    "section": "9.8 Marginalization",
    "text": "9.8 Marginalization\nI mentioned that the evidence can be computed from the likelihood and the prior. To see this, we apply the sum rule to the posterior probability. Let \\(\\theta_i\\) be a particular possible value of a parameter or hypothesis. Then,\n\\[\\begin{aligned}\n1 = g(\\theta_j\\mid y) + g(\\theta^c_j | y) \\nonumber = g(\\theta_j\\mid y) + \\sum_{i\\ne j}g(\\theta_i\\mid y) = \\sum_i g(\\theta_i\\mid y),\n\\end{aligned} \\tag{9.12}\\]\nNow, Bayes’s theorem gives us an expression for \\(g(\\theta_i\\mid y)\\), so we can compute the sum.\n\\[\\begin{aligned}\n\\sum_i g(\\theta_i\\mid y) = \\sum_i\\frac{f(y \\mid \\theta_i)\\, g(\\theta_i)}{f(y)} = \\frac{1}{f(y)}\\sum_i f(y \\mid \\theta_i)\\, g(\\theta_i) = 1.\n\\end{aligned} \\tag{9.13}\\]\nTherefore, we can compute the evidence by summing over the priors and likelihoods of all possible hypotheses or parameter values.\n\\[\\begin{aligned}\nf(y) = \\sum_i f(y \\mid \\theta_i)\\, g(\\theta_i).\n\\end{aligned} \\tag{9.14}\\]\nUsing the joint probability, we also have\n\\[\\begin{aligned}\nf(y) = \\sum_i \\pi(y, \\theta_i).\n\\end{aligned} \\tag{9.15}\\]\nThis process of eliminating a variable (in this case \\(\\theta_i\\)) from a probability by summing is called marginalization. This will prove useful in finding the probability distribution of a single parameter among many.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_logic_of_science.html#footnotes",
    "href": "lessons/probability/probability_logic_of_science.html#footnotes",
    "title": "9  Probability as the logic of science",
    "section": "",
    "text": "But we were not too formal. For example, we were not discussing \\(\\sigma\\) algebras, measurability, etc.↩︎\nMore formally, a random variable transforms the possible outcomes of an experiment to real numbers.↩︎\nI have heard this referred to as the \"fully marginalized likelihood\" because of the cute correspondence of the acronym and how some people feel trying to get their head around the meaning of the quantity.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability as the logic of science</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_distributions.html",
    "href": "lessons/probability/probability_distributions.html",
    "title": "10  Probability distributions",
    "section": "",
    "text": "10.1 Joint and conditional distributions and Bayes’s theorem for PDFs\nSo far we have talked about probability of events, and we have in mind measurements, parameter values and hypotheses as the events. We have a bit of a problem, though, if the sample space consists of real numbers, which we often encounter in our experiments and modeling. The probability of getting a single real value is identically zero. This is my motivation for introducing probability distributions, but the concept is more general and has much more utility than just dealing with sample spaces containing real numbers. Importantly, probability distributions provide the link between outcomes in the sample space to probability. Probability distributions describe both discrete quantities (like integers) and continuous quantities (like real numbers).\nThough we cannot assign a nonzero the probability for an outcome from a sample space containing all of the real numbers, we can assign a probability that the outcome is less than some real number. Notationally, we write this as\n\\[\\begin{aligned}\nP(\\text{having outcome that is}\\le y) = F(y).\n\\end{aligned} \\tag{10.1}\\]\nThe function \\(F(y)\\), which returns a probability, is called a cumulative distribution function (CDF), or just distribution function. It contains all of the information we need to know about how probability is assigned to \\(y\\). A CDF for a Normal distribution is shown in the left panel of the figure below.\nRelated to the CDF for a continuous quantity is the probability density function, or PDF. The PDF is given by the derivative of the CDF,\n\\[\\begin{aligned}\nf(y) = \\frac{\\mathrm{d}F(y)}{\\mathrm{d}y}.\n\\end{aligned} \\tag{10.2}\\]\nNote that \\(f(y)\\) is not the probability of outcome \\(y\\). Rather, the probability that of outcome \\(y\\) lying between \\(y_0\\) and \\(y_1\\) is\n\\[\\begin{aligned}\nP(y_0\\le y \\le y_1) = F(y_1) - F(y_0) = \\int_{y_0}^{y_1}\\mathrm{d}y\\,f(y).\n\\end{aligned} \\tag{10.3}\\]\nNote that with this definition of the probability density function, satisfaction of the axiom that all probabilities sum to zero (equivalently stated as \\(F(y\\to\\infty) = 1\\)) necessitates that the probability density function is normalized. That is,\n\\[\\begin{aligned}\n\\int_{-\\infty}^\\infty \\mathrm{d}t\\, f(y) = 1.\n\\end{aligned} \\tag{10.4}\\]\nConversely, for a discrete quantity, we have a probability mass function, or PMF,\n\\[\\begin{aligned}\nf(y) = P(y).\n\\end{aligned} \\tag{10.5}\\]\nThe PMF is a probability, unlike the PDF. An example of a CDF and a PMF for a discrete distribution are shown in the figure below. In this example, \\(n\\) is the outcome of the roll of a fair die (\\(n\\in\\{1,2,3,4,5,6\\}\\)).\nWe have defined a PDF as \\(f(x)\\), that is, describing a single variable \\(x\\). We can have joint distributions with a PDF \\(f(x, y)\\).\nWe may also have conditional distributions that have PDF \\(f(x\\mid y)\\). This is interpreted similarly to conditional probabilities we have already seen. \\(f(x\\mid y)\\) is the probability density function for \\(x\\), given \\(y\\). As similar relation between joint and conditional PDFs holds as in the case of joint and conditional probabilities.\n\\[\\begin{aligned}\nf(x\\mid y) = \\frac{f(x,y)}{f(y)}.\n\\end{aligned} \\tag{10.6}\\]\nThat this holds is not at all obvious. One immediate issue is that we are conditioning on an event \\(y\\) that has zero probability. We will not carefully derive why this holds, but state it without proof.\nAs a consequence, Bayes's theorem also holds for PDFs, as it does for probabilities.1\n\\[\\begin{aligned}\nf(\\theta\\mid y) = \\frac{f(y\\mid \\theta)\\,f(\\theta)}{f(y)}.\n\\end{aligned} \\tag{10.7}\\]\nNotationally in this course, we will use \\(f\\) to describe a PDF or PMF of a random variable and \\(g\\) to describe the PMF or PDF of a parameter or other logical conjecture that is not measured data or a random variable. For example, \\(f(y \\mid \\theta)\\) is the PDF for a continuous measured quantity and \\(g(\\theta)\\) is the PDF for a parameter value. In this notation, Bayes’s theorem is\n\\[\\begin{aligned}\ng(\\theta\\mid y) = \\frac{f(y\\mid \\theta)\\,g(\\theta)}{f(y)}.\n\\end{aligned} \\tag{10.8}\\]\nFinally, we can marginalize probability distribution functions to get marginalized PDFs.\n\\[\\begin{aligned}\nf(x) = \\int \\mathrm{d}y\\,f(x,y) = \\int\\mathrm{d}y\\,f(x\\mid y)\\,f(y).\n\\end{aligned} \\tag{10.9}\\]\nIn the case of a discrete distribution, we can compute marginal a marginal PMF.\n\\[\\begin{aligned}\nf(x) = \\sum_i\\,f(x,y_i) = \\sum_i f(x\\mid y_i)\\,f(y_i).\n\\end{aligned} \\tag{10.10}\\]",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_distributions.html#change-of-variables-formula-for-continuous-distributions",
    "href": "lessons/probability/probability_distributions.html#change-of-variables-formula-for-continuous-distributions",
    "title": "10  Probability distributions",
    "section": "10.2 Change of variables formula for continuous distributions",
    "text": "10.2 Change of variables formula for continuous distributions\nAs a last note about probability distributions, I discuss the change of variables formula. Say I have a continuous probability distribution with PDF \\(f_X(x)\\). I have included the subscript \\(X\\) to denote that this is a PDF describing the variable \\(X\\). If I wish to change variables to instead get a continuous distribution in \\(y=y(x)\\), or \\(f_Y(y) = f_Y(y(x))\\), how do I get \\(f_Y\\)? We must enforce that the distributions be normalized;\n\\[\\begin{align}\n\\int \\mathrm{d}x\\, f_X(x) =  \\int \\mathrm{d}y\\, f_Y(y) = 1.\n\\end{align} \\tag{10.11}\\]\nThus, we must have \\(\\left|\\mathrm{d}y\\,f_Y(y)\\right| = \\left|\\mathrm{d}x\\,f_x(x)\\right|\\). Equivalently, we have\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d} x}{\\mathrm{d}y}\\right|\\,f_X(x).\n\\end{align} \\tag{10.12}\\]\nThis is the change of variables formula.\n\n10.2.1 Generalization to multiple dimensions\nGenerically, if we have a set of variables \\(\\mathbf{x}\\) that are transformed into a new set of parameters \\(\\mathbf{y} = \\mathbf{y}(\\mathbf{x})\\), then\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\partial(x_1, x_2, \\ldots)}{\\partial(y_1, y_2, \\ldots)}\\right|f_X(x),\n\\end{align} \\tag{10.13}\\]\nwhere the first factor on the right hand side is the Jacobian, which is the absolute value of the determinant of the Jacobi matrix,\n\\[\\begin{aligned}\n\\begin{align}\n\\frac{\\partial(x_1, x_2, \\ldots)}{\\partial(y_1, y_2, \\ldots)} = \\begin{pmatrix}\n\\frac{\\partial x_1}{\\partial y_1} & \\frac{\\partial x_1}{\\partial y_2} & \\cdots \\\\\n\\frac{\\partial x_2}{\\partial y_1} & \\frac{\\partial x_2}{\\partial y_2} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}   .\n\\end{align}\n\\end{aligned} \\tag{10.14}\\]\n\n\n10.2.2 An example of change of variables\nImagine I have a random variable that is Exponentially distributed, such that\n\\[\\begin{align}\nf_X(x) = \\beta \\, \\mathrm{e}^{-\\beta x}.\n\\end{align} \\tag{10.15}\\]\nNow saw that I want to rescale \\(x\\) so that I instead get a distribution in \\(y = a x\\). Here, \\(g(x) = a x\\) and \\(g^{-1}(y) = y/a\\). So, we have\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d}}{\\mathrm{d}y}\\,\\frac{y}{a}\\right|\\,f_X(y/a)\n= \\frac{1}{a}\\,\\beta\\,\\mathrm{e}^{-\\beta y / a}.\n\\end{align} \\tag{10.16}\\]\nThe distribution is again Exponential, but the rate has been rescaled, \\(\\beta \\to \\beta/a\\). This makes sense; we have rescaled \\(x\\) by our change of variables, so the rate should be rescaled accordingly.\n\n\n10.2.3 Another example of change of variables: the Log-Normal distribution\nNow imagine I have a random variable that is Normally distributed and I wish to determine how \\(y = \\mathrm{e}^{x}\\) is distributed.\n\\[\\begin{align}\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(x-\\mu)^2/2\\sigma^2}.\n\\end{align} \\tag{10.17}\\]\nHere, \\(g(x) = \\mathrm{e}^x\\) and \\(g^{-1}(y) = \\ln y\\). Again applying the change of variables formula,\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d}\\,\\ln y}{\\mathrm{d}y}\\right|\\,f_X(\\ln y)\n= \\frac{1}{y\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(\\ln y-\\mu)^2/2\\sigma^2},\n\\end{align} \\tag{10.18}\\]\nwhich is indeed the PDF of the Log-Normal distribution.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_distributions.html#sec-probability-distribution-stories",
    "href": "lessons/probability/probability_distributions.html#sec-probability-distribution-stories",
    "title": "10  Probability distributions",
    "section": "10.3 Probability distributions as stories",
    "text": "10.3 Probability distributions as stories\nThe building blocks of statistical models are probability distributions. Specifying a model amounts to choosing probability distributions that describe the process of data generation. In some cases, you need to derive the distribution based on specific considerations or your experiment or model (or even numerically compute it when it cannot be written in closed form). In many practical cases, though, your model is composed of standard probability distributions. These distributions have stories associated with them. That is, the mathematical particulars of the distribution follow from a description of a data generation process. For example, the story behind the Bernoulli distribution is as follows. The outcome of a coin flip is Bernoulli distributed. So in building models, if your data generation process matches the story of a distribution, you know that this is the distribution to choose for your model.\nThe Distribution Explorer is a useful tool to connect distributions to stories and obtain their PDFs/PMFs and CDFs, as well as syntax for usage in popular software packages. I encourage you to explore the Explorer!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability/probability_distributions.html#footnotes",
    "href": "lessons/probability/probability_distributions.html#footnotes",
    "title": "10  Probability distributions",
    "section": "",
    "text": "This is very subtle. Jaynes’s book, Probability: The Logic of Science, Cambridge University Press, 2003, for more one these subtleties.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html",
    "href": "lessons/probability/entropy_and_kullback_leibler.html",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "",
    "text": "11.1 Information and entropy\nWe have defined probability distributions and their associated probability mass function and probability density functions. There are an infinitude of distributions we could use to model data generation processes, and we might wish to compare distributions. To do that, we need a definition of “closeness” of two probability distributions. To get this definition, we need to turn to notions about information.\nFormally, information is the reduction in ignorance derived from learning an outcome. Say event \\(i\\) happens with probability \\(p_i\\). If \\(i\\) is very probable and we observe it, we haven’t learned much. For example, if we observe that the current pope is Catholic, we haven’t learned much about popes. That is, we are still pretty ignorant about popes. But if \\(i\\) is very improbable and we observe it, we have learned a lot. If we however observe that the current pope is American, we have learned something new and interesting about the pope.\nTo codify this in mathematical terms, we might think that the information gained by observing event \\(i\\) should scale like \\(1/p_i\\), since more rare events give higher information.\nNow, say we observe two independent events, \\(i\\) and \\(j\\). Since they are totally independent, the information garnered from observing both should be the sum of the information garnered from observing each. We know that the probability of observing both is \\(1/p_ip_j\\). But\n\\[\\begin{aligned}\n\\frac{1}{p_i} + \\frac{1}{p_j} \\ne \\frac{1}{p_ip_j}.\n\\end{aligned} \\tag{11.1}\\]\nSo, our current metric of information as \\(1/p_i\\) does not satisfy this addability requirement. However,\n\\[\\begin{aligned}\n\\log\\frac{1}{p_i} + \\log\\frac{1}{p_j} = \\log \\frac{1}{p_ip_j}.\n\\end{aligned} \\tag{11.2}\\]\nSo, we choose \\(\\log (1/p_i) = -\\log p_i\\) as a measure of information. We are free to choose the base of the logarithm, and it is traditional to choose base 2. The units of information are then called bits. We, however, will use natural logarithms for convenience.\nNow, say we have an ensemble of events. Then the average information we get from observing events (i.e., the level of surprise) is\n\\[\\begin{aligned}\nH[p] = -\\sum_i p_i\\,\\ln p_i.\n\\end{aligned} \\tag{11.3}\\]\nThis is called the Shannon entropy or informational entropy. It has its name because of its relation to the same quantity in statistical thermodynamics. We will not delve into that, although it is a rich and beautiful subject.\nLet’s look at the Shannon entropy another way. Say we know all of the \\(p_i\\)’s. How much knowledge do we know about what events we might observe? If the probability distribution is flat, not much. Conversely, if it is sharply peaked, we know a lot about what event we will observe. In the latter case, observing an event does not give us more information beyond what we already knew from the probabilities. So, \\(H[p]\\) is a measure of ignorance. It tells us how uncertain or unbiased we are ahead of an observation.\nI pause to note that we shortcutted our way into this definition of entropy by using some logic and the desire that independent events add. A more careful derivation was done in 1948 by Claude Shannon. He showed that the function we wrote for the entropy is the only function that satisfies three desiderata about measurements of ignorance.\nThe derivation is beautiful, but we will not go into it here.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html#sec-information-and-entropy",
    "href": "lessons/probability/entropy_and_kullback_leibler.html#sec-information-and-entropy",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "",
    "text": "Entropy is continuous in \\(p_i\\).\nIf all \\(p_i\\) are equal, entropy is monotonic in \\(n\\), the number of events we could observe.\nEntropy satisfies a composition law; grouping of events does not change the value of entropy.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html#sec-cross-entropy",
    "href": "lessons/probability/entropy_and_kullback_leibler.html#sec-cross-entropy",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "11.2 Cross entropy",
    "text": "11.2 Cross entropy\nWe can extend this notion of entropy to define cross entropy, \\(H[p, q]\\). This is the amount of information (or loss of ignorance) needed to identify an event \\(i\\) described by probability \\(p_i\\) when we use some other probability \\(q_i\\). In other words, it tells us how much ignorance we have in using \\(q\\) to describe events governed by \\(p\\). The cross entropy is\n\\[\\begin{aligned}\n   H[p,q] = -\\sum_i p_i\\, \\ln q_i.\n\\end{aligned} \\tag{11.4}\\]",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html#sec-KL",
    "href": "lessons/probability/entropy_and_kullback_leibler.html#sec-KL",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "11.3 The Kullback-Leibler divergence",
    "text": "11.3 The Kullback-Leibler divergence\nWe may think about how close \\(p\\) and \\(q\\) are. The additional entropy induced by using \\(q\\) instead of \\(p\\) is \\(H[p, q] - H[p]\\). We can use this as a measure of closeness of \\(q\\) to \\(p\\). This is called the Kullback-Leibler divergence, also known as the KL divergence,\n\\[\\begin{aligned}\nD_\\mathrm{KL}(p \\| q) = H[p,q] - H[p] = -\\sum_i p_i\\, \\ln q_i + \\sum_i p_i\\,\\ln p_i = \\sum_i p_i\\,\\ln\\frac{p_i}{q_i}.\n\\end{aligned} \\tag{11.5}\\]\nNote that the KL divergence is not a distance, in that it is directional; \\(D_\\mathrm{KL}(p \\| q) \\ne D_\\mathrm{KL}(q \\| p)\\), except when \\(p = q\\). So, the KL divergence \\(D_\\mathrm{KL}(p \\| q)\\) is a measure of the closeness of \\(q\\) to \\(p\\), and not a measure of the closeness of \\(p\\) to \\(q\\).\nThe definition of the KL divergence generalizes to continuous distributions and their probability density functions as 1\n\\[\\begin{aligned}\nD_\\mathrm{KL}(f_0 \\| f) = \\int \\mathrm{d}y\\,\\,f_0(y)\\,\\ln\\frac{f_0(y)}{f(y)}.\n\\end{aligned} \\tag{11.6}\\]",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html#sec-jensen",
    "href": "lessons/probability/entropy_and_kullback_leibler.html#sec-jensen",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "11.4 Jensen’s inequality",
    "text": "11.4 Jensen’s inequality\nIn a moment, we will prove that the Kullback-Leibler divergence is always positive, except when \\(q = p\\), when it is zero. An important and useful result toward that end is Jensen’s inequality.\nA function \\(f(\\mathbf{x})\\) is convex if for any \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) on the domain of \\(f\\) and \\(\\theta\\) with \\(0 \\le \\theta \\le 1\\),\n\\[\\begin{align}\nf(\\theta\\mathbf{x} + (1 - \\theta)\\mathbf{y}) \\le \\theta f(\\mathbf{x}) + (1 - \\theta)f(\\mathbf{y}).\n\\end{align}\n\\tag{11.7}\\]\nThe function \\(f(\\mathbf{x})\\) is strictly convex if strict inequality holds in inequality 11.7 for any \\(\\mathbf{x} \\ne \\mathbf{y}\\). This is a mathematical statement that the curve lies below any cord between two points of the function.\nNow consider \\(\\{\\theta_1, \\theta_2, \\ldots, \\theta_n\\}\\), with all \\(\\theta_i\\)’s satisfying \\(0 \\le \\theta_i \\le 1\\) with \\(\\sum_{i=1}^n \\theta_i = 1\\). Then, for a convex function \\(f\\),\n\\[\\begin{align}\nf\\left(\\sum_{i=1}^n\\theta_i \\mathbf{x}_i\\right) \\le \\sum_{i=1}^n\\theta_i \\, f(\\mathbf{x}_i).\n\\end{align}\n\\tag{11.8}\\]\nThis is known as Jensen’s inequality. If \\(f\\) is strictly convex, then equality is achieved if and only if \\(\\mathbf{x}_i\\) is the same value for all \\(i\\). This has obvious implications when considering \\(\\theta_i\\) to be a probability mass. If \\(\\theta_i = P(\\mathbf{x}_i)\\), the probability of observing \\(\\mathbf{x}_i\\), then Jensen’s inequality relates expectations,\n\\[\\begin{align}\nf(\\langle \\mathbf{x} \\rangle) = f\\left(\\sum_{i=1}^n \\mathbf{x}_i\\, P(\\mathbf{x}_i)\\right) \\le \\langle f(\\mathbf{x}) \\rangle = \\sum_{i=1}^n f(\\mathbf{x})\\,P(\\mathbf{x}_i).\n\\end{align}\n\\tag{11.9}\\]\nThis extends to continuous distributions with \\(P(\\mathbf{x})\\) being a probability density such that\n\\[\\begin{align}\n\\int \\mathrm{d}\\mathbf{x}\\,P(\\mathbf{x}) = 1,\n\\end{align}\n\\tag{11.10}\\]\nso that\n\\[\\begin{align}\nf(\\langle \\mathbf{x} \\rangle) = f\\left(\\int\\mathrm{d}\\mathbf{x} \\,\\mathbf{x}\\, P(\\mathbf{x})\\right) \\le \\langle f(\\mathbf{x}) \\rangle = \\int\\mathrm{d}\\mathbf{x}\\,f(\\mathbf{x})\\,P(\\mathbf{x}).\n\\end{align}\n\\tag{11.11}\\]\nIt is evident, then, that inequality 11.7 is a special case of Jensen’s inequality.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html#sec-gibbs-inequality",
    "href": "lessons/probability/entropy_and_kullback_leibler.html#sec-gibbs-inequality",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "11.5 Gibbs’s inequality (positivity of the Kullback-Leibler divergence)",
    "text": "11.5 Gibbs’s inequality (positivity of the Kullback-Leibler divergence)\nJensen’s inequality has obvious implications when considering \\(\\theta_i\\) to be a probability mass. If \\(\\theta_i = P(\\mathbf{x}_i)\\), the probability of observing \\(\\mathbf{x}_i\\), then Jensen’s inequality relates expectations,\n\\[\\begin{align}\nf(\\langle \\mathbf{x} \\rangle) = f\\left(\\sum_{i=1}^n \\mathbf{x}_i\\, P(\\mathbf{x}_i)\\right) \\le \\langle f(\\mathbf{x}) \\rangle = \\sum_{i=1}^n f(\\mathbf{x})\\,P(\\mathbf{x}_i).\n\\end{align}\n\\tag{11.12}\\]\nThe direction of the inequality is flipped for concave \\(f\\).\nWe will now prove that \\(-D_\\mathrm{KL}(p\\| q) \\le 0\\), which is equivalent to proving nonnegativity of the Kullback-Leibler divergence, \\(D_\\mathrm{KL}(p\\| q) \\ge 0\\). The nonnegativity of the Kullback-Leibler divergence is known as Gibbs’s inequality.\nWe have\n\\[\n\\begin{align}\n-D_\\mathrm{KL}(p\\| q) = -\\sum_{i=1}^N p_i\\,\\ln\\left(\\frac{p_i}{q_i}\\right)\n= \\sum_{i=1}^N p_i\\,\\ln\\left(\\frac{q_i}{p_i}\\right) = \\left\\langle \\ln\\left(\\frac{q_i}{p_i}\\right)\\right\\rangle.\n\\end{align}\n\\tag{11.13}\\]\nNote that the logarithm function is strictly concave, since \\(\\mathrm{d}^2\\ln x/\\mathrm{d}x^2 = -1/x^2\\), the second derivative is always negative. Then, by Jensen’s inequality, we have\n\\[\\begin{align}\n\\left\\langle \\ln\\left(\\frac{q_i}{p_i}\\right)\\right\\rangle\n\\le \\ln \\left\\langle\\frac{q_i}{p_i}\\right\\rangle = \\ln\\left(\\sum_{i=1}^N p_i\\,\\frac{q_i}{p_i}\\right) = \\ln\\left(\\sum_{i=1}^N q_i\\right) = \\ln 1 = 0.\n\\end{align}\n\\tag{11.14}\\]\nWe have almost proved Gibbs’s inequality; we just need to prove that equality only holds when \\(p_i = q_i\\) for all \\(i\\). To do this, we first write a result from the above analysis. Let \\(x_i = q_i/p_i\\). Then,\n\\[\\begin{align}\n\\sum_{i=1}^N p_i\\,\\ln x_i \\le \\ln\\left(\\sum_{i=1}^N p_i x_i\\right).\n\\end{align}\n\\tag{11.15}\\]\nBecause the logarithm is strictly concave, by Jensen’s inequality, \\(x_i\\) must be a constant if the above holds with equality. Thus, \\(q_i = c p_i\\) for all \\(i\\), where \\(c\\) is a constant. But,\n\\[\\begin{align}\n1 = \\sum_{i=1}^N q_i = \\sum_{i=1}^N c p_i = c \\sum_{i=1}^N p_i = c.\n\\end{align}\n\\tag{11.16}\\]\nSince \\(c = 1\\), equality is achieved in Gibbs’s inequality only when \\(p_i = q_i\\) for all \\(i\\).",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/probability/entropy_and_kullback_leibler.html#footnotes",
    "href": "lessons/probability/entropy_and_kullback_leibler.html#footnotes",
    "title": "11  Entropy and the Kullback-Leibler divergence",
    "section": "",
    "text": "I am playing a little fast and loose here converting sums to integrals. There are some subtleties involved therein, but we will not delve into those here.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Entropy and the Kullback-Leibler divergence</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/sampling.html",
    "href": "lessons/sampling/sampling.html",
    "title": "Sampling out of probability distributions",
    "section": "",
    "text": "We have talked about generative probability distributions that model the process of data production. When we perform an experiment and obtain data, we are sampling out of the generative distribution. The true generative distribution is unknown, but by sampling out of it, we gain insights about the generative process. For example, if I measure the heights of a collection of humans, I learn something about the generative distribution just by investigating the samples out of it (the measured data).\nSimilarly, we can learn a lot about probability distributions, including model generative distributions, by sampling out of them directly using random number generation. In this section, we will learn about the techniques for doing so.",
    "crumbs": [
      "Sampling out of probability distributions"
    ]
  },
  {
    "objectID": "lessons/sampling/random_number_generation.html",
    "href": "lessons/sampling/random_number_generation.html",
    "title": "12  Random number generation",
    "section": "",
    "text": "| Download notebook\n\nRandom number generation (RNG) is the process by which a sequence of random numbers may be drawn. When using a computer to draw the random numbers, the numbers are not completely random. The notion of “completely random” is nonsensical because of the infinitude of numbers. Random numbers must be drawn from some probability distribution. Furthermore, in most computer applications, the random numbers are actually pseudorandom. They depend entirely on an input seed and are then generated by a deterministic algorithm from that seed.\nSo what do (pseudo)random number generators do? RNGs are capable of (approximately) drawing integers from a Discrete Uniform distribution. For example, NumPy’s default built-in generator, the PCG64 generator, generates 128 bit numbers, allowing for \\(2^{128}\\), or about \\(10^{38}\\), possible integers. Importantly, each draw of of a random integer is (approximately) independent of all others.\nIn practice, the drawn integers are converted to floating-point numbers (since a double floating-point number has far less than 128 bits) on the interval [0, 1) by dividing a generated random integer by \\(2^{138}\\). Effectively, then, the random number generators provide draws out of a Uniform distribution on the interval [0, 1).\nTo convert from random numbers on a Uniform distribution to random numbers from a nonuniform distribution, we need a transform. For many named distributions convenient transforms exist. For example, the Box-Muller transform is often used to get random draws from a Normal distribution. In the absence of a clever transform, we can use a distribution’s quantile function, also known as a percent-point function, which is the inverse CDF, \\(F^{-1}(y)\\). For example, the quantile function for the Exponential distribution is\n\\[\n\\begin{aligned}\nF^{-1}(p) = -\\beta^{-1}\\,\\ln(1-p),\n\\end{aligned} \\tag{12.1}\\]\nwhere \\(p\\) is the value of the CDF, ranging from zero to one. We first draw \\(p\\) out of a Uniform distribution on [0, 1), and then compute \\(F^{-1}(p)\\) to get a draw from an Exponential distribution. A graphical illustration of using a quantile function to draw 50 random numbers out of Gamma(5, 2) is shown below.\n\n\nCode\nimport numpy as np\nimport scipy.stats as st\n\nimport bokeh.io\nimport bokeh.plotting\nbokeh.io.output_notebook(hide_banner=True)\n\nrng = np.random.default_rng(seed=12341234)\nalpha = 5\nbeta = 2\ny = np.linspace(0, 8, 400)\ncdf = st.gamma.cdf(y, alpha, loc=0, scale=1 / beta)\n\nudraws = rng.uniform(size=50)\n\np = bokeh.plotting.figure(\n    width=300,\n    height=200,\n    x_axis_label=\"y\",\n    y_axis_label=\"F(y; 2, 5)\",\n    x_range=[0, 8],\n    y_range=[0, 1],\n)\np.xgrid.grid_line_color = None\np.ygrid.grid_line_color = None\n\np.line(y, cdf, line_width=2)\n\nfor u in udraws:\n    x_vals = [0] + [st.gamma.ppf(u, alpha, loc=0, scale=1 / beta)] * 2\n    y_vals = [u, u, 0]\n    p.line(x_vals, y_vals, color=\"gray\", line_width=0.5)\n\np.scatter(np.zeros_like(udraws), udraws, marker=\"x\", color=\"black\", line_width=0.5, size=7)\np.scatter(\n    st.gamma.ppf(udraws, alpha, loc=0, scale=1 / beta),\n    np.zeros_like(udraws),\n    line_width=0.5,\n    fill_color=None,\n    line_color=\"black\",\n)\n\np.renderers[0].level = \"overlay\"\np.renderers[-1].level = \"overlay\"\np.renderers[-2].level = \"overlay\"\n\nbokeh.io.show(p)\n\n\n\n  \n\n\n\nEach draw from a Uniform distribution, marked by an × on the vertical axis, is converted to a draw from a Gamma distribution, marked by ○ on the horizontal axis, by computing the inverse CDF. Because the draws of the random integers from which the draws from a Uniform distribution on \\([0, 1)\\) are independent, we get independent draws from the Gamma distribution.",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Random number generation</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html",
    "href": "lessons/sampling/rng_with_numpy.html",
    "title": "13  Random number generation using Numpy",
    "section": "",
    "text": "13.1 Uniform random numbers\n| Download notebook\nA good portion of the random number generation functionality you will need is in the np.random module. It allows for draws of independent random numbers for many convenient named distributions. The scipy.stats module offers even more distributions, but for most applications, Numpy’s generators suffice and are typically faster than using Scipy, which has more overhead.\nLet’s start by generating random numbers from a Uniform distribution.\nnp.random.uniform(low=0, high=1, size=10)\n\narray([0.40312242, 0.04051189, 0.03851483, 0.41999136, 0.99686689,\n       0.86482983, 0.57146661, 0.71342451, 0.20395884, 0.32728968])\nThe function uniform() in the np.random module generates random numbers on the interval [low, high) from a Uniform distribution. The size keyword argument is how many random numbers you wish to generate, and is a keyword argument in all Numpy’s functions to draw from specific distributions. The random numbers are returned as a Numpy array.\nWe can check to make sure it is appropriately drawing random numbers out of the Uniform distribution by plotting the cumulative distribution function. We’ll generate 1,000 random numbers and plot them along with the CDF of a Uniform distribution.\n# Generate random numbers\nx = np.random.uniform(low=0, high=1, size=1000)\n\n# Plot the ECDF of randomly generated numbers\np = iqplot.ecdf(x, marker_kwargs={\"fill_color\": None},)\n\np.line(\n    x=[0, 1], y=[0, 1], line_width=2, line_color=\"orange\",\n)\n\nbokeh.io.show(p)\nSo, it looks like our random number generator is doing a good job.\nGenerating random numbers on the uniform interval is one of the most commonly used RNG applications. For example, you can simulate flipping a biased (unfair) coin by drawing from a Uniform distribution and then asking if the random number if less than the bias.\n# Generate 20 random numbers on uniform interval\nx = np.random.uniform(low=0, high=1, size=20)\n\n# Make the coin flips (&lt; 0.7 means we have a 70% chance of heads)\nheads = x &lt; 0.7\n\n# Show which were heads, and count the number of heads\nprint(heads)\nprint(\"\\nThere were\", np.sum(heads), \"heads.\")\n\n[ True  True False  True  True  True  True False  True  True  True  True\n  True  True  True False False False  True  True]\n\nThere were 15 heads.\nOf course, you could also do this by drawing out of a Binomial distribution.\nprint(f\"There were {np.random.binomial(20, 0.7)} heads.\")\n\nThere were 12 heads.",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#sec-choice-of-generator",
    "href": "lessons/sampling/rng_with_numpy.html#sec-choice-of-generator",
    "title": "13  Random number generation using Numpy",
    "section": "13.2 Choice of generator",
    "text": "13.2 Choice of generator\nAs of version 1.23 of Numpy, the algorithm under the hood of calls to functions like np.random.uniform() is the Mersenne Twister Algorithm for generating random numbers. It is a very widely used and reliable method for generating random numbers. However, starting with version 1.17, the numpy.random module offers random number generators with better speed and statistical performance, including a 64-bit permuted congruential generator (PCG64). Going forward, the preferred approach to doing random number generation is to first instantiate a generator of your choice, and then use its methods to generate numbers out of probability distributions.\nLet’s set up a PCG64 generator, which is Numpy’s default (though this will soon be updated to the PCG64 DXSM, which works better for massively parallel generation, per Numpy’s documentation).\n\nrng = np.random.default_rng()\n\nNow that we have the generator, we can use it to draw numbers out of distributions. The syntax is the same as before, except rng replaces np.random.\n\nrng.uniform(low=0, high=1, size=20)\n\narray([0.83950893, 0.18995913, 0.7455994 , 0.65977687, 0.50251107,\n       0.3920528 , 0.98069072, 0.43620495, 0.42139065, 0.67577344,\n       0.44188521, 0.42143143, 0.64836288, 0.29192271, 0.17708979,\n       0.92594815, 0.04850722, 0.06206417, 0.09710099, 0.75790207])\n\n\nOr, for the Binomial,\n\nrng.binomial(20, 0.7)\n\n18",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#sec-seeding-rngs",
    "href": "lessons/sampling/rng_with_numpy.html#sec-seeding-rngs",
    "title": "13  Random number generation using Numpy",
    "section": "13.3 Seeding random number generators",
    "text": "13.3 Seeding random number generators\nNow, just to demonstrate that random number generation is deterministic, we will explicitly seed the random number generator (which is usually seeded with a number representing the date/time to avoid repeats) to show that we get the same random numbers.\n\n# Instantiate generator with a seed\nrng = np.random.default_rng(seed=3252)\n\n# Draw random numbers\nrng.uniform(size=10)\n\narray([0.18866535, 0.04418857, 0.02961285, 0.22083971, 0.43341773,\n       0.13166813, 0.42112164, 0.43507845, 0.61380912, 0.30627603])\n\n\nIf we reinstantiate with the same seed, we get the same sequence of random numbers.\n\n# Re-seed the RNG\nrng = np.random.default_rng(seed=3252)\n\n# Draw random numbers\nrng.uniform(size=10)\n\narray([0.18866535, 0.04418857, 0.02961285, 0.22083971, 0.43341773,\n       0.13166813, 0.42112164, 0.43507845, 0.61380912, 0.30627603])\n\n\nThe random number sequence is exactly the same. If we choose a different seed, we get totally different random numbers.\n\nrng = np.random.default_rng(seed=3253)\nrng.uniform(size=10)\n\narray([0.31390226, 0.73012457, 0.05800998, 0.01557021, 0.29825701,\n       0.10106784, 0.06329107, 0.58614237, 0.52023168, 0.52779988])\n\n\nIf you are writing tests, it is often useful to seed the random number generator to get reproducible results. Otherwise, it is best to use the default seed, based on the date and time, so that you get a new set of random numbers in your applications each time do computations.",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#sec-rng-dists",
    "href": "lessons/sampling/rng_with_numpy.html#sec-rng-dists",
    "title": "13  Random number generation using Numpy",
    "section": "13.4 Drawing random numbers out of other distributions",
    "text": "13.4 Drawing random numbers out of other distributions\nSay we wanted to draw random samples from a Normal distribution with mean μ and standard deviation σ.\n\n# Set parameters\nmu = 10\nsigma = 1\n\n# Draw 100000 random samples\nx = rng.normal(mu, sigma, size=100000)\n\n# Plot the histogram\np = iqplot.histogram(x, rug=False, density=True, y_axis_label=\"approximate PDF\",)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIt looks Normal, but, again, comparing the resulting ECDF is a better way to look at this. We’ll check out the ECDF with 1000 samples so as not to choke the browser with the display. I will also make use of the theoretical CDF for the Normal distribution available from the scipy.stats module.\n\n# Compute theoretical CDF\nx_theor = np.linspace(6, 14, 400)\ny_theor = st.norm.cdf(x_theor, mu, sigma)\n\n# Plot the ECDF of randomly generated numbers\np = iqplot.ecdf(x, marker_kwargs={\"fill_color\": None},)\n\np.line(\n    x=x_theor, y=y_theor, line_width=2, line_color=\"orange\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYup, right on!",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#sec-rng-choice",
    "href": "lessons/sampling/rng_with_numpy.html#sec-rng-choice",
    "title": "13  Random number generation using Numpy",
    "section": "13.5 Choosing elements from an array",
    "text": "13.5 Choosing elements from an array\nIt is often useful to randomly choose elements from an existing array. (Actually, this is probably the functionality we will use the most, since it is used in bootstrapping.) The rng.choice() function does this. You equivalently could do this using rng.integers(), where the integers represent indices in the array, except rng.choice() has a great keyword argument, replace, which allows random draws with or without replacement. For example, say you had 50 samples that you wanted to send to a facility for analysis, but you can only afford to send 20. If we used rng.integers(), we might have a problem.\n\nrng = np.random.default_rng(seed=126969234)\nrng.integers(0, 51, size=20)\n\narray([12, 31, 47, 26,  3,  5, 46, 49, 26, 38, 24, 17, 46, 26,  6, 17, 35,\n        4, 13, 29])\n\n\nSample 17 was selected twice and sample 26 was selected thrice. This is not unexpected. We can use rng.choice() instead.\n\nrng.choice(np.arange(51), size=20, replace=False)\n\narray([27, 34,  0, 46,  2, 48, 35, 50, 40, 12, 28, 19, 37, 38, 11, 23, 45,\n       15, 29, 32])\n\n\nNow, because we chose replace=False, we do not get any repeats.",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#sec-rng-shuffle",
    "href": "lessons/sampling/rng_with_numpy.html#sec-rng-shuffle",
    "title": "13  Random number generation using Numpy",
    "section": "13.6 Shuffling an array",
    "text": "13.6 Shuffling an array\nSimilarly, the rng.permutation() function is useful. It takes the entries in an array and shuffles them! Let’s shuffle a deck of cards.\n\nrng.permutation(np.arange(53))\n\narray([12, 18,  2, 34, 27, 10,  0, 30, 49,  7,  5, 35, 11, 23, 37, 17,  4,\n       44, 15, 28, 14,  8, 40, 21, 39, 36, 46, 24, 33, 20, 22,  1, 41, 45,\n       50, 26, 16, 42, 52,  3,  9, 48, 38, 25, 43, 51, 19, 47, 32,  6, 13,\n       29, 31])",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#sec-rng-when-do-we-need",
    "href": "lessons/sampling/rng_with_numpy.html#sec-rng-when-do-we-need",
    "title": "13  Random number generation using Numpy",
    "section": "13.7 When do we need RNG?",
    "text": "13.7 When do we need RNG?\nAnswer: VERY OFTEN! We will use random number generator extensively as we explore probability distributions.\nIn many ways, probability is the language of biology. Molecular processes have energetics that are comparable to the thermal energy, which means they are always influenced by random thermal forces. The processes of the central dogma, including DNA replication, are no exceptions. This gives rise to random mutations, which are central to understanding how evolution works. And of course neuronal firing is also probabilistic. If we want to understand how biology works, it is often useful to use random number generators to model the processes.",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/rng_with_numpy.html#computing-environment",
    "href": "lessons/sampling/rng_with_numpy.html#computing-environment",
    "title": "13  Random number generation using Numpy",
    "section": "13.8 Computing environment",
    "text": "13.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Sampling out of probability distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/simulation.html",
    "href": "lessons/sampling/simulation.html",
    "title": "Simulating generative distributions",
    "section": "",
    "text": "We have seen that we can draw random numbers out of distributions for which we have a convenient transform or access to a quantile function. We have also seen that we can derive a probability mass function or probability density function from the story of a distribution, and those PMFs/PDFs give us complete information about the distribution. Sometimes, though, it is difficult or impossible to derive a PMF or PDF from a story. In other situations, we may know the PMF or PDF buy cannot derive a transform nor an easily evaluated quantile function. In these cases, we can simulate the story of the distribution using random number generation.\nIn this section, we will learn how to simulate distributions through three examples, the Luria-Delbrück distribution from their famous experiment, the noisy integrate-and-fire model of neurons, and nonhomogenous Poisson processes as a model for neuronal spiking.",
    "crumbs": [
      "Simulating generative distributions"
    ]
  },
  {
    "objectID": "lessons/sampling/luria_delbruck.html",
    "href": "lessons/sampling/luria_delbruck.html",
    "title": "14  Simulating the Luria-Delbrück distribution",
    "section": "",
    "text": "14.1 Computing environment\n| Download notebook\nThe Luria-Delbrück distribution (also known as a jackpot distribution) is a classic example from the biological sciences of a distribution whose story is easy to state, but whose PMF is very difficult to write down. (It was written down by Lea and Colson, but the expression fills a couple pages and is not terribly usable.)\nWhen mutations occur in nature, they are often deleterious to the organism. However, mutations are a critical part of the genetic heritage of living organisms, arising in every type of organism and allowing life to evolve and adapt to new environments. In 1943, the question of how microorganisms acquire mutations was described in a famous paper by Salvador Luria and Max Delbrück (S. E. Luria and M. Delbrück, Genetics, 28, 491–511, 1943). At the time, there were two prominent theories of genetic inheritance, the “random mutation hypothesis,” in which mutations arose randomly in the absence of an environmental cue, and the “adaptive immunity hypothesis, in which mutations occur as an adaptive response to an environmental stimulus. See the figure below.\nTo test these two hypotheses, Luria and Delbrück grew many parallel cultures of bacteria and then plated each culture on agar containing phages (which infect and kill nearly all of the bacteria). Although most bacteria are unable to survive in the presence of phages, often mutations could enable a few survivors to give rise to resistant mutant colonies. If the adaptive immunity hypothesis is correct, mutations occur only after bacteria come in contact with phages, thus only after plating the bacteria on phage-agar plates. Under the random mutation hypothesis, some bacteria already have the immunity before being exposed.\nThe story of the Luria-Delbrück distribution arises from the random mutation hypothesis. Start with a single cell that cannot survive being exposed to phage. When it divides, there is a probability \\(\\theta\\) one of the daughter cells gets a mutation that will impart survivability. This is true for each division. Once a mutation that imparts survival is obtained, it is passed to all subsequent generations. The number \\(n\\) of survivors in \\(N\\) individual cells exposed to the stress is distributed according to the Luria-Delbrück distribution.\nWe will not attempt to write down the PMF, but will instead sample out of the Luria-Delbrück distribution by directly simulating its story. To do the simulation, we note that if we have a population of \\(M\\) unmutated cells, we will have \\(M\\) cell divisions. The number of cell divisions that result in a mutation is Binomially distributed, Binom(M, θ). So, to simulate the a Luria-Delbrück experiment, at each round of cell division, we draw a random number of a Binomial distribution parametrized by the number of cells that have not yet had mutations and \\(\\theta\\). For after the \\(g\\)th set of cell divisions, we have \\(2^g\\) total cells. If we have \\(n_\\mathrm{mut}\\) cells with favorable mutations when we had \\(2^{g-1}\\) cells, then we have \\(2 n_\\mathrm{mut} + n_\\mathrm{mut}^\\mathrm{new}\\), where \\(n_\\mathrm{mut}^\\mathrm{new}\\) is drawn from \\(\\text{Binom}(2^{g-1}-n_\\mathrm{mut}, \\theta)\\), after the \\(g\\)th set of cell divisions. Let’s code it up!\nThis function draws a single number of survivors. To get a picture of the distribution, we need to make many, many draws, so we write a function to call this function repeatedly to get the samples.\nLet’s put it to use! We will draw a million samples for 16 generations with a mutation rate of \\(10^{-5}\\).\nLuria and Delbrück knew that if the Fano factor, the ratio of the variance to the mean, was much bigger than one, the adaptive immunity hypothesis (which has a predicted Fano factor of 1—can you explain why?). We can compute the Fano factor of the Luria-Delbrück distribution from the samples.\nWow! Huge variance and bit Fano factor. We can get a feeling for the PMF by making a spike plot (We will avoid an ECDF in this case because we have LOTS of samples and we want to take it easy on the browser.)\nWe have a very heavy tail, which decays according to a power law. We also see artifacts due to the discrete nature of the cell divisions, where powers of two are more likely. Using just the powers of two, the apparent power law is approximately \\(P(n) \\sim n^{-1}\\), which is not even normalizable in the limit of an infinite number of plated cells. The fact that we have a finite number of cells keeps the PMF normalizable. A very heavy tail, indeed!\n%load_ext watermark\n%watermark -v -p numpy,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/lif.html",
    "href": "lessons/sampling/lif.html",
    "title": "15  The noisy leaky integrate-and-fire model",
    "section": "",
    "text": "15.1 Langevin dynamics of a leaky integrate-and-fire model\n| Download notebook\nWhile not a story, per se, we can sometimes define probability distributions in terms of differential equations. Master equations and Fokker-Planck equations describe the dynamics, respectively, of probability mass functions and probability density functions over time. Langevin equations and stochastic differential equations describe quantities that may undergo random fluctuations.\nAs an example of sampling out of probability distributions described by differential equations, we will consider here a leaky integrate-and-fire (LIF) model which describes the membrane potential of a neuron. The membrane of a neuron is modeled as a capacitor which can accumulate charge and a resistor in parallel. For an input current \\(I(t)\\), then, the membrane potential \\(V(t)\\) obeys, via Kirchhoff’s law,\n\\[\n\\begin{aligned}\nI(t) = C\\,\\frac{\\mathrm{d}V}{\\mathrm{d}t} + \\frac{V(t)}{R},\n\\end{aligned}\n\\tag{15.1}\\]\nor, upon rearrangement,\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d}V}{\\mathrm{d}t} = - \\frac{V(t)}{RC} + \\frac{I(t)}{C}.\n\\end{aligned}\n\\tag{15.2}\\]\nAs the membrane potential grows with time, when it hits a threshhold \\(V_\\mathrm{thresh}\\), an action potential is fired, after which the membrane potential rapidly relaxes back to a reset potential, \\(V_\\mathrm{reset}\\). We define\n\\[\n\\begin{aligned}\nv(t) = \\frac{V(t) - V_\\mathrm{reset}}{V_\\mathrm{thresh} - V_\\mathrm{reset}}\n\\end{aligned}\n\\tag{15.3}\\]\nas a dimensionless membrane potential. Note that the reset potential corresponds to \\(v = 0\\) and the threshold corresponds to \\(v = 1\\). We further define a dimensionless current, \\(i(t)\\), such that \\(I(t) = I_0\\,i(t)\\), where \\(I_0\\) is the amplitude of the input current. Using these expressions, we have\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} = - \\frac{v(t)}{RC} - \\frac{V_\\mathrm{reset}}{(V_\\mathrm{thresh} - V_\\mathrm{reset})RC} + \\frac{I_0\\,i(t)}{(V_\\mathrm{thresh} - V_\\mathrm{reset})C}.\n\\end{aligned}\n\\tag{15.4}\\]\nThere are two time scales we can identify. First, \\(\\tau_m = RC\\) is the time scale of the potential drop of the membrane due to leakage. Second, \\(\\tau = (V_\\mathrm{thresh} - V_\\mathrm{reset})C / I_0\\) is the amount of time it takes for the input current to bring the membrane up to the threshold potential from the reset potential. With these definitions, and also defining \\(\\alpha = V_\\mathrm{reset}/(V_\\mathrm{thresh} - V_\\mathrm{reset})\\), we have\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} = - \\frac{v(t) + \\alpha}{\\tau_m} + \\frac{i(t)}{\\tau}.\n\\end{aligned}\n\\tag{15.5}\\]\nThis is a deterministic differential equation. We may have noise in the input current, which we include in the equation as \\(\\sigma\\,\\xi(t)\\), giving us a Langevin equation\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} = - \\frac{v(t) + \\alpha}{\\tau_m} + \\frac{i(t)}{\\tau} + \\sigma\\,\\xi(t).\n\\end{aligned}\n\\tag{15.6}\\]\nThe function \\(i(t)\\) can be arbitrarily complicated. In many models, it is itself stochastic, modeling input currents from neighboring neurons. Similarly, the noise function \\(\\xi(t)\\) may also take a variety of forms. We will focus on Gaussian white noise, which means that the stochastic portion of the dynamics of the voltage is fluctuating according to a Normal distribution with zero mean. (More precisely, \\(\\xi(t)\\) is defined by a Wiener process, which means, among other things, that \\(\\langle \\xi(t)\\rangle = 0\\) and \\(\\langle \\xi(t)\\,\\xi(t')\\rangle = \\delta(t-t')\\).)",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The noisy leaky integrate-and-fire model</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/lif.html#sec-euler-maruyama",
    "href": "lessons/sampling/lif.html#sec-euler-maruyama",
    "title": "15  The noisy leaky integrate-and-fire model",
    "section": "15.2 Euler-Maruyama integration",
    "text": "15.2 Euler-Maruyama integration\nWe will solve the above stochastic LIF Langevin equation using Euler-Maruyama integration. The idea is that we integrate exactly like Euler’s method with time step size \\(dt\\), but we add a random number drawn from \\(\\text{Norm}(0, dt)\\) (which is then multiplied by \\(\\sigma\\), the scale of the noise) at every step. Note that this is equivalent to drawing out of a standard normal (with mean zero and variance of one) and multiplying the result by \\(\\sigma\\,dt\\). In this case, every time the dimensionless voltage reaches the spike threshold (\\(v = 1\\)), we record a spike and then immediately send the voltage back down to the reset voltage (\\(v = 0\\)). This is implemented in the function below.\n\n# Instantiate random number generator\nrng = np.random.default_rng()\n\n\ndef integrate_noisy_lif(\n    v0=0.0,\n    dt=0.001,\n    T=1000.0,\n    tau_m=1000.0,\n    tau=30.0,\n    alpha=1.0,\n    sigma=1.0,\n    ifun=1.0,\n    ifun_args=(),\n    return_traj=False,\n):\n    \"\"\"Integrate the stochastic LIF model and return spike times.\n\n    Parameters\n    ----------\n    v0 : float, default 0.0\n        Dimensionless voltage at time t = 0.\n    dt : float, default 0.001\n        Time step interval.\n    T : float, default 1000.0\n        Integration proceeds from t = 0 to t = T.\n    tau_m : float, default 1000.0\n        Time scale of membrane potential relaxation due to leakage.\n    tau : float, default 30.0\n        Time scale for membrane potential to rise to threshold.\n    alpha : float, default 1.0\n        alpha parameter from LIF model.\n    sigma : float, default 1.0\n        Magnitude of noise.\n    ifun : function or float, default 1.0\n        Function with call signature `ifun(t, *ifun_args)` that gives\n        the current as a function of time. If specified as a float,\n        it is a constant function.\n    ifun_args : tuple, default ()\n        Arguments to pass into `ifun`. Ignored if `ifun` is a float.\n    return_traj : bool, default False\n        If True, also return (t, v), the time points and corresponding\n        dimensionless voltage, as a tuple in addition to spike times.\n\n    Returns\n    -------\n    spikes : Numpy array\n        Times of spikes.\n    traj_tuple : tuple of Numpy arrays (optional)\n        If `return_traj` is True, a tuple (t, v) of the time and\n        corresponding dimensionless voltage.\n\n    Notes\n    -----\n    .. The parameters `dt`, `T`, `tau_m`, and `tau` all must be in the\n       same units. These are the same units of the time array returned\n       if `traj_tuple` is True.\n    .. The default parameter values of `tau` and `tau_m` are such that\n       the time units are in milliseconds and are in the limit of very\n       slow leakage and a \"typical\" firing rate of about 30 spikes/s.\n    \"\"\"\n    # Set up ifun, it not given as a function\n    if type(ifun) != types.FunctionType:\n        ret_val = float(ifun)\n        ifun = lambda t: ret_val\n\n    # Set up time points\n    t = np.arange(0, T, dt)\n\n    # Output array\n    v = np.empty_like(t)\n    v[0] = v0\n\n    # Initialize list to store spikes\n    spikes = []\n\n    for i in range(1, len(v)):\n        # Euler-Maruyama step\n        v[i] = v[i - 1] + dt * (\n            -(v[i - 1] + alpha) / tau_m\n            + ifun(t[i - 1], *ifun_args) / tau\n            + sigma * rng.standard_normal()\n        )\n\n        # Record spike and reset voltage if we passed threshold\n        if v[i] &gt;= 1.0:\n            spikes.append(t[i])\n            v[i] = 0.0\n\n    if return_traj:\n        return np.array(spikes), (t, v)\n    else:\n        return np.array(spikes)\n\nLet’s give this function a while to sample some spike times!\n\nspikes, (t, v) = integrate_noisy_lif(return_traj=True)\n\nWe can plot the trajectory.\n\np = bokeh.plotting.figure(\n    frame_width=500,\n    frame_height=200,\n    x_axis_label='t (ms)',\n    y_axis_label='v(t)'\n)\n\n# Thin the trajectory a bit when plotting\np.line(t[::10], v[::10])\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe can see what happens if we increase the leakage, making the two time scales \\(\\tau_m\\) and \\(\\tau\\) comparable. We will try \\(\\tau_m = 70\\) ms and \\(\\tau = 30\\) ms.\n\nspikes, (t, v) = integrate_noisy_lif(tau_m=70.0, return_traj=True)\n\np = bokeh.plotting.figure(\n    frame_width=500,\n    frame_height=200,\n    x_axis_label='t (ms)',\n    y_axis_label='v(t)'\n)\n\n# Thin the trajectory a bit when plotting\np.line(t[::10], v[::10])\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe get far less frequent spikes. This is because the leakage is causing the potential to drop as it gets large.\nUltimately our goal is to sample spike times (actually, interspike intervals, or ISIs), so we can run a longer trajectory to get plenty of samples.\n\nspikes = integrate_noisy_lif(T=10_000)\n\n# Look as a strip plot\nbokeh.io.show(\n    iqplot.strip(\n        spikes,\n        marker=\"dash\",\n        marker_kwargs=dict(line_width=0.5),\n        x_axis_label=\"time (ms)\",\n        frame_height=100,\n        frame_width=700,\n    )\n)\n\n\n  \n\n\n\n\n\nWe can compute interspike intervals by computing the difference between successive spike times and can plot the ECDF.\n\nbokeh.io.show(\n    iqplot.ecdf(np.diff(spikes), x_axis_label='ISI (ms)')\n)",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The noisy leaky integrate-and-fire model</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/lif.html#sec-lif-approx-poisson",
    "href": "lessons/sampling/lif.html#sec-lif-approx-poisson",
    "title": "15  The noisy leaky integrate-and-fire model",
    "section": "15.3 Approximate Poisson process",
    "text": "15.3 Approximate Poisson process\nAs we will see in the exercises, with the above choice of parameters, the ISI for the noisy leaky integrate-and-fire model is approximately Inverse Gaussian distributed. However, ISIs are often Exponentially distributed, which means that the spiking is a Poisson process. As described by Stevens and Zador, the noise leaky integrate-and-fire model can give approximately Exponentially distributed interspike intervals when the input current itself is also given by Gaussian white noise with a positive mean. They further showed that this works when spiking is slow, such that the characteristic interspike interval is much greater than \\(\\tau_m\\).\nAs an example, consider the case where \\(\\tau_m = 70\\) ms and \\(\\tau = 40\\) ms and the current function has a mean of one (in dimensionless units) and a standard deviation of 0.3. We can code this up below.\n\ndef ifun(t, sigma_I=0.3):\n    return rng.normal(1.0, sigma_I)\n\nspikes = integrate_noisy_lif(tau_m=70, tau=40, ifun=ifun, T=100_000)\n\nTo demonstrate that the ISIs are approximately Exponentially distributed, we plot the empirical complementary cumulative distribution function (ECCDF), which is \\(1 - \\hat{F}(t_\\mathrm{ISI})\\), where \\(\\hat{F}(t_\\mathrm{ISI})\\) is the ECDF. Importantly, a linear tail on a semilog plot of the ECCDF indicates an Exponential distribution, since the CDF for the Exponential distribution is \\(F(t_\\mathrm{ISI}) = 1 - \\mathrm{e}^{-\\beta t_\\mathrm{ISI}}\\) such that the ECCDF is \\(\\mathrm{e}^{-\\beta t_\\mathrm{ISI}}\\).\n\n# Plot ECCDF\np = iqplot.ecdf(np.diff(spikes), x_axis_label='ISI (ms)', complementary=True, y_axis_type='log')\n\n# Annotate with a slope 1 line\nt_shift = 110\nbeta = 1 / np.mean(np.diff(spikes) - t_shift)\nx = np.array([110, 1500])\ny = np.exp(-beta * (x - t_shift))\np.line(x, y, color='tomato', line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nSo, for ISIs large compared to \\(\\tau_m\\), the distribution of ISIs is approximately Exponential. In practice, this is like an Exponential distribution with a time lag. That is, spikes arrive a Poisson processes, but only after a refractory period after the previous spike. We will investigate a phenomenological model for Poissonian-with-refractory-period spikes in the exercises.",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The noisy leaky integrate-and-fire model</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/lif.html#sec-poissonian-stimuli",
    "href": "lessons/sampling/lif.html#sec-poissonian-stimuli",
    "title": "15  The noisy leaky integrate-and-fire model",
    "section": "15.4 Poissonian stimuli",
    "text": "15.4 Poissonian stimuli\nSpiking may also appear Poissonian if the stimuli (the input current) arrive in a Poissonian manner and the LIF circuit responds quickly (\\(\\tau\\) is small). This is kind of obvious; a circuit that responds close to instantaneously to a Poissionian stimulus will itself behave in a Poissonian manner.\nTo demonstrate this, we can put in current as Poissonian-distributed spikes with a Gaussian shape.\n\n# Arrival of stimulating current as a Poisson process\nstimulus_times = np.cumsum(rng.exponential(30, size=1000))\n\ndef ifun(t, stim_times, I_peak=1.0, stim_width=1.0):\n    \"\"\"Stimuli come as Gaussian-like peaks\"\"\"\n    return np.sum(I_peak * np.exp(-(t-stimulus_times)**2 / 2 / stim_width**2))\n\nspikes = integrate_noisy_lif(tau=3, ifun=ifun, ifun_args=(stimulus_times,), T=30_000)\n\nWe can compare the ISIs to what we would expect from a Poisson process (exponentially distributed ISI).\n\np = iqplot.ecdf(np.diff(spikes), x_axis_label='ISI (ms)')\nx = np.linspace(0, 275, 300)\ny = 1 - np.exp(-x / np.mean(np.diff(spikes)))\np.line(x, y, line_width=2, color='tomato')\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIndeed, we have Exponentially distributed ISIs.",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The noisy leaky integrate-and-fire model</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/lif.html#computing-environment",
    "href": "lessons/sampling/lif.html#computing-environment",
    "title": "15  The noisy leaky integrate-and-fire model",
    "section": "15.5 Computing environment",
    "text": "15.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,iqplot,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\niqplot    : 0.3.7\nbokeh     : 3.6.2\njupyterlab: 4.4.2",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The noisy leaky integrate-and-fire model</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/nonhomogeneous_poisson.html",
    "href": "lessons/sampling/nonhomogeneous_poisson.html",
    "title": "16  Modeling nonhomogeneous Poisson spiking",
    "section": "",
    "text": "16.1 The PDF for arrival times of a nonhomogenous Poisson process\n| Download notebook\nWe have previously modeled spiking using a noisy leaky integrate-and-fire model. We saw that in some limits, the noisy LIF model gives Poissonian spiking such that the interspike intervals are Exponentially distributed and the number of spikes in a given time interval are Poisson distributed. Poisson process-based models of spiking are widely used to good effect.\nHowever, though well-modelled as Poisson, the rate of spiking can vary over time in what we call a nonhomogeneous Poisson process. Here, we derive the PDF for ISIs in this context and show how to sample ISIs.\nConsider the waiting time for an arrival of a Poisson process that arrives at rate \\(\\beta\\). We know that the waiting time is Exponentially distributed.\n\\[\\begin{align}\nf(t\\mid \\beta) = \\beta \\mathrm{e}^{-\\beta t}.\n\\end{align}\n\\tag{16.1}\\]\nThe probability that a Poisson process has not arrived at or before time \\(t\\) is given by the complementary cumulative distribution function (CCDF) of the Exponential distribution.\n\\[\\begin{align}\nP(\\text{not arrived by time }t) = 1 - P(\\text{arrived by time }t) = 1 - \\int_0^t\\mathrm{d}t'\\,f(t\\mid\\beta) = 1 - (1 - \\mathrm{e}^{-\\beta t}) = \\mathrm{e}^{-\\beta t}.\n\\end{align}\n\\tag{16.2}\\]\nNow, consider the case where the arrival rate varies with time; \\(\\beta = \\beta(t)\\). Consider some small interval, \\([0, \\Delta t]\\). The probability that no Poisson process has arrived in time \\(\\Delta t\\) is\n\\[\\begin{align}\nP(\\text{not arrived by time } \\Delta t) \\approx \\mathrm{e}^{-\\beta(\\Delta t) \\Delta t},\n\\end{align}\n\\tag{16.3}\\]\nwhere \\(\\Delta t\\) is small so that \\(\\beta(t)\\) does not change appreciably over the time interval and \\(\\beta(t) \\approx \\beta(t+\\Delta t)\\). We will soon take the \\(\\Delta t\\to 0\\) limit, so this approximation is justified.\nNow consider another interval of length \\(\\Delta t\\) right after the one we just considered. The probability that no process arrives in that interval is\n\\[\\begin{align}\nP(\\text{not arrived in time window } [\\Delta t, 2\\Delta t]) \\approx \\mathrm{e}^{-\\beta(2\\Delta t) \\Delta t}.\n\\end{align}\n\\tag{16.4}\\]\nTherefore, owing to the memorylessness of Poisson processes, the probability that no processes arrived in the interval \\([0, 2\\Delta t]\\) is the product of the probabilities of not arriving in the respective subintervals,\n\\[\\begin{align}\nP(\\text{not arrived in time } 2\\Delta t) \\approx \\mathrm{e}^{-\\beta(\\Delta t) \\Delta t}\\,\\mathrm{e}^{-\\beta(2\\Delta t) \\Delta t}.\n\\end{align}\n\\tag{16.5}\\]\nWe can consider \\(m\\) such intervals.\n\\[\\begin{align}\nP(\\text{not arrived between in time } m\\Delta t) \\approx \\prod_{k=1}^m \\mathrm{e}^{-\\beta(k\\Delta t) \\Delta t} = \\exp\\left[-\\sum_{k=1}^m \\Delta t \\,\\beta(k\\Delta t)\\right].\n\\end{align}\n\\tag{16.6}\\]\nIn the limit of \\(\\Delta t\\to 0\\), the Riemann sum becomes an integral. Taking \\(t = m\\Delta t\\), we have\n\\[\\begin{align}\nP(\\text{not arrived by time } t) = \\exp\\left[- \\int_0^t\\mathrm{d}t'\\,\\beta(t')\\right].\n\\end{align}\n\\tag{16.7}\\]\nThis is the CCDF of the distribution desribing the arrival of a Poisson process with variable rate \\(\\beta(t)\\). The CDF is then \\(1 - \\text{CCDF}\\),\n\\[\\begin{align}\nF(t\\mid \\beta(t)) = 1 - \\exp\\left[- \\int_0^t\\mathrm{d}t'\\,\\beta(t')\\right].\n\\end{align}\n\\tag{16.8}\\]\nThe probability density function is then\n\\[\\begin{align}\nf(t\\mid \\beta(t)) &= \\frac{\\mathrm{d}F}{\\mathrm{d}t} = -\\frac{\\mathrm{d}}{\\mathrm{d}t}\\,\\exp\\left[- \\int_0^t\\mathrm{d}t'\\,\\beta(t')\\right]\n= -\\exp\\left[- \\int_0^t\\mathrm{d}t'\\,\\beta(t')\\right]\\,\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(- \\int_0^t\\mathrm{d}t'\\,\\beta(t')\\right) \\nonumber \\\\[1em]\n&= \\beta(t)\\,\\exp\\left[- \\int_0^t\\mathrm{d}t'\\,\\beta(t')\\right].\n\\end{align}\n\\tag{16.9}\\]",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling nonhomogeneous Poisson spiking</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/nonhomogeneous_poisson.html#sec-pdf-nonhomogeneous-poisson",
    "href": "lessons/sampling/nonhomogeneous_poisson.html#sec-pdf-nonhomogeneous-poisson",
    "title": "16  Modeling nonhomogeneous Poisson spiking",
    "section": "",
    "text": "16.1.1 Multiple arrivals\nSay we observe arrivals of a Poisson process one after another. If we have \\(n\\) arrivals of a Poisson process with a variable rate occuring at times \\(\\mathbf{t} = \\{t_1, t_2, \\ldots, t_n\\}\\), where the times are ordered, the PDF is given by the product of the PDFs for each waiting time.\n\\[\\begin{align}\nf(\\mathbf{t} \\mid \\beta(t)) = \\prod_{i=1}^n\\beta(t_i)\\exp\\left[- \\int_{t_{i-1}}^{t_i}\\mathrm{d}t'\\,\\beta(t')\\right]\n= \\left(\\prod_{i=1}^n\\beta(t_i)\\right)\\exp\\left[- \\sum_{i=1}^n\\int_{t_{i-1}}^{t_i}\\mathrm{d}t'\\,\\beta(t')\\right],\n\\end{align}\n\\tag{16.10}\\]\nwhere \\(t_0\\) is when we started observing (it is not a time of an arrival of a Poisson process), and we take \\(t_0 = 0\\). The integrals add together, giving\n\\[\\begin{align}\nf(\\mathbf{t} \\mid \\beta(t)) = \\left(\\prod_{i=1}^n\\beta(t_i)\\right)\\exp\\left[-\\int_0^{t_n}\\mathrm{d}t'\\,\\beta(t')\\right].\n\\end{align}\n\\tag{16.11}\\]\n\n\n16.1.2 A finite observation time\nIn practice, we start observing at time \\(t = 0\\) and end at time \\(t = T\\). While \\(t_n\\) is the time of the last Poisson process we saw arrive, we should also take into account that we continued watching for time \\(T - t_n\\) and saw no arrivals during that time. We therefore have\n\\[\\begin{align}\nf(\\mathbf{t}, T;\\beta(t)) &= f(\\mathbf{t} \\mid \\beta(t)) \\times P(\\text{no arrivals between }t_n\\text{ and }T) \\nonumber \\\\[1em]\n&= \\left(\\prod_{i=1}^n\\beta(t_i)\\right)\\exp\\left[-\\int_0^{t_n}\\mathrm{d}t'\\,\\beta(t')\\right]\\,\\exp\\left[\\int_{t_n}^T\\mathrm{d}t'\\,\\beta(t')\\right] \\nonumber  \\\\[1em]\n&= \\left(\\prod_{i=1}^n\\beta(t_i)\\right)\\exp\\left[-\\int_0^{T}\\mathrm{d}t'\\,\\beta(t')\\right].\n\\end{align}\n\\tag{16.12}\\]\nNote that in the special case of a constant rate, \\(\\beta(t) = \\beta\\), we get\n\\[\\begin{align}\nf(\\mathbf{t}, T;\\beta(t)) &= \\beta^n\\,\\mathrm{e}^{-\\beta T},\n\\end{align}\n\\tag{16.13}\\]\nIn this case, the PDF is not dependent on the times, but only on the number of arrivals in the time interval. We could have derived by noting that the interrarrival times are Exponentially distributed.\n\\[\\begin{align}\nf(\\mathbf{t}, T;\\beta(t)) &= \\beta\\,\\mathrm{e}^{-\\beta t_1}\\,\\left(\\prod_{i=2}^{n-1}\\beta\\,\\mathrm{e}^{-\\beta (t_{i+1}-t_i)}\\right)\\mathrm{e}^{-\\beta(T-t_n)} = \\beta^n\\,\\mathrm{e}^{-\\beta T}.\n\\end{align}\n\\tag{16.14}\\]",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling nonhomogeneous Poisson spiking</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/nonhomogeneous_poisson.html#sec-nonphomogeneous-poisson-pmf",
    "href": "lessons/sampling/nonhomogeneous_poisson.html#sec-nonphomogeneous-poisson-pmf",
    "title": "16  Modeling nonhomogeneous Poisson spiking",
    "section": "16.2 PMF for number of arrivals in time interval",
    "text": "16.2 PMF for number of arrivals in time interval\nConsider now a time interval from \\(0\\) to \\(T\\). We break this time interval into \\(N\\) small intervals, all of which are small enough such that, as before, \\(\\beta(t)\\) does not change appreciably across a small interval and furthermore that the probability of more than one spike landing in a given small interval is zero. Let \\(\\Delta t_i = t_{i+1}-t_i\\) be the width of small interval \\(i\\). For reasons that will become clear momentarily, we choose the widths of the small intervals such that the small interval width times the rate is the same for all intervals (which we will define as \\(\\xi\\) for convenience), or\n\\[\\begin{align}\n\\beta(t_{i+1}) \\,\\Delta t_i = \\xi \\text{ for all }i.\n\\end{align}\n\\tag{16.15}\\]\nSince every \\(\\beta(t_{i+1}) \\,\\Delta t_i\\) is the same, we can write\n\\[\\begin{align}\n\\xi = \\frac{1}{N}\\sum_{i=0}^N\\,\\beta(t_{i+1}) \\,\\Delta t_i = \\frac{1}{N}\\int_0^T\\mathrm{d}t'\\,\\beta(t') \\equiv \\frac{\\Lambda(T)}{N},\n\\end{align}\n\\tag{16.16}\\]\nwhere we have considered small \\(\\Delta t_i\\) and evaluated the Riemann sum, which is valid for unequal intervals provided the intervals are small enough. For convenience, we have defined\n\\[\\begin{align}\n\\Lambda(T) = \\int_0^T\\mathrm{d}t'\\,\\beta(t').\n\\end{align}\n\\tag{16.17}\\]\nThe probability of a spike not arriving in a small interval spanning from \\(t_i\\) to \\(t_{i+1}\\), as we have already worked out, is\n\\[\\begin{align}\nP(\\text{not arrived between } t_i \\text{ and } t_{i+1}) \\approx \\mathrm{e}^{-\\beta(t_{i+1})\\,\\Delta t_i} = \\mathrm{e}^{-\\xi},\n\\end{align}\n\\tag{16.18}\\]\nsuch that the probability a spike does arrive in that interval is\n\\[\\begin{align}\nP(\\text{arrived between } t_i \\text{ and } t_{i+1}) \\approx 1 - \\mathrm{e}^{-\\beta(t_{i+1}) \\Delta t_i} = 1-\\mathrm{e}^{-\\xi}.\n\\end{align}\n\\tag{16.19}\\]\nAgain owing the memorylessness of spiking, the probability of getting a spike in a given small interval constitutes a Bernoulli trial with probability \\(\\theta\\) of success. Then, the probability of getting \\(n\\) spikes in the interval \\([0, T]\\) is Binomially distributed,\n\\[\\begin{align}\nf(n;N,\\xi) \\approx \\frac{N!}{n!(N-n)!}\\,(1 - \\mathrm{e}^{-\\xi})^n\\,(\\mathrm{e}^{-\\xi})^{N-n}.\n\\end{align}\n\\tag{16.20}\\]\nWe now consider the limit of large \\(N\\). In this limit,\n\\[\\begin{align}\n\\mathrm{e}^{-\\xi} \\approx 1 - \\xi,\n\\end{align}\n\\tag{16.21}\\]\nsuch that\n\\[\\begin{align}\nf(n;N,\\xi) \\approx \\frac{N!}{n!(N-n)!}\\,\\xi^n\\,(1-\\xi)^{N-n}.\n\\end{align}\n\\tag{16.22}\\]\nNow, in the limit of large \\(N\\), \\(N \\gg n\\) such that \\(N - n \\approx N\\). Further, we have that \\(N = \\Lambda(T) / \\xi\\), such that\n\\[\\begin{align}\n(1-\\xi)^{N-n} \\approx (1-\\xi)^{N} = (1-\\xi)^{\\Lambda(T)/\\xi}.\n\\end{align}\n\\tag{16.23}\\]\nBecause\n\\[\\begin{align}\n\\lim_{\\xi\\to 0}(1-\\xi)^{1/\\xi} = \\mathrm{e}^{-1},\n\\end{align}\n\\tag{16.24}\\]\nwe have\n\\[\\begin{align}\n(1-\\xi)^{N-n} \\approx (1-\\xi)^{\\Lambda(T)/\\xi} \\approx \\mathrm{e}^{-\\Lambda(T)}.\n\\end{align}\n\\tag{16.25}\\]\nFinally, also have that in the limit of large \\(N\\),\n\\[\\begin{align}\nN!/(N-n)! \\approx N^n = \\left(\\frac{\\Lambda(T)}{\\xi}\\right)^n.\n\\end{align}\n\\tag{16.26}\\]\nPutting everything together, we have\n\\[\\begin{align}\nf(n;\\beta(t), T) = \\frac{(\\Lambda(T))^n}{n!}\\,\\mathrm{e}^{-\\Lambda(T)},\n\\end{align}\n\\tag{16.27}\\]\nwhere\n\\[\\begin{align}\n\\Lambda(T) = \\int_0^T\\mathrm{d}t'\\,\\beta(t'),\n\\end{align}\n\\tag{16.28}\\]\ngiving the PMF for the number of arrivals of a nonhomogenous Poisson process in an interval of length \\(T\\).",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling nonhomogeneous Poisson spiking</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/nonhomogeneous_poisson.html#sec-nonhomogeneous-poisson-sampling",
    "href": "lessons/sampling/nonhomogeneous_poisson.html#sec-nonhomogeneous-poisson-sampling",
    "title": "16  Modeling nonhomogeneous Poisson spiking",
    "section": "16.3 Sampling arrval times of a nonhomogeneous Poisson process",
    "text": "16.3 Sampling arrval times of a nonhomogeneous Poisson process\nWe can easily sample arrival times of a homogeneous Poisson process by noting that the interarrival times are Exponentially distributed. We keep drawing interarrival times from an Exponential distribution and cumulatively sum them to get the arrival times.\n\nrng = np.random.default_rng()\ndef sample_homogeneous_poisson_process(beta, T):\n    \"\"\"Draw samples of arrival times of a homogeneous Poisson process.\"\"\"\n    # Intialize arrival times\n    arrival_times = []\n\n    # Typical arrival time (Numpy likes this)\n    tau = 1 / beta\n\n    # Draw interarrival times and keep adding\n    t = rng.exponential(tau)\n    while t &lt; T:\n        arrival_times.append(t) \n        t += rng.exponential(tau)\n\n    return np.array(arrival_times)\n\n# Give it a whirl\nsample_homogeneous_poisson_process(1, 20)\n\narray([ 0.30127478,  2.12937407,  3.36913533,  3.42966043,  3.75863601,\n        4.47142917,  4.5084683 ,  4.55892601,  4.83689062,  5.61171643,\n        5.79705461,  5.94193015,  9.54332889,  9.80945717, 10.56075264,\n       12.96824828, 14.4488708 , 15.07726874, 16.50660407, 16.88488701,\n       18.12896431, 18.43671618])\n\n\nHowever, sampling arrival times of a nonhomogeneous Poisson is more challenging. The arrival time varies, so we cannot directly draw out of an Exponential distribution. To perform the sampling, we can use a thinning method. This works as follows. We set a fast rate \\(\\beta_u\\) that is greater than \\(\\beta(t)\\) everywhere on the interval \\([0, T]\\). We draw a time \\(t_1\\) out of an Exponential distribution with rate \\(\\beta_u\\). We accept \\(t_1\\) as a sample out of the nonhomogeneous distribution with probability \\(\\beta(t_1)/\\beta_u\\). We then draw an interarrival time \\(\\delta t_2\\) from an Exponential distribution parametrized with rate \\(\\beta_u\\). We accept \\(t_1 + \\delta t_2\\) as a draw from the nonhomogeneous distribution with probability \\(\\beta(t_1 + \\delta t_2) / \\beta_u\\). We continue in this manner until the total time of the draws meets or exceeds \\(T\\). This is implemented in the function below.\n\ndef sample_nhpp(beta, beta_u, T, beta_args=()):\n    \"\"\"Draw arrival times of a nonhomogeneous Poisson process.\n\n    Parameters\n    ----------\n    beta : function, call signature beta(t, *beta_args)\n        The function of the rate of arrivals as a function of time.\n    beta_u : float\n        A value of beta that is greater than beta(t) for all time.\n    T : float\n        The maximum time of observation.\n    beta_args : tuple, default ()\n        Any other arguments passed to the function `beta()`.\n\n    Returns\n    -------\n    output : Numpy array\n        Times for arrivals of the nonhomogeneous Poisson process.\n\n    Notes\n    -----\n    .. This is an implementation of the algorithm on page 85 of \n       Simulation, 5th Ed., by Sheldon Ross, Academic Press, 2013.\n    \"\"\"\n    samples = []\n    tau = 1 / beta_u\n    t = rng.exponential(tau)\n    while t &lt; T:\n        r = beta(t, *beta_args) * tau\n\n        if r &gt; 1:\n            raise RuntimeError('beta_u is less than beta; sampling invalid.')\n\n        if np.random.uniform() &lt;= r:\n            samples.append(t)\n\n        t += rng.exponential(tau)\n\n    return np.array(samples)\n\nLet’s take this for a spin. We will have \\(\\beta(t)\\) given by a sinusoid.\n\nbeta = lambda t: 1.15 + np.sin(t/10)\narrival_times = sample_nhpp(beta, 2.15, 500)\n\n# Plot the arrivals\np = iqplot.strip(arrival_times, marker='dash', marker_kwargs=dict(alpha=0.3))\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNice!\nIf we want to speed things up, we can break the interval up into smaller segments and for each interval we make sure that we choose a \\(\\beta_u\\) specific to that interval that is close to the maximum \\(\\beta(t)\\) on that interval. The code (which also includes other speed enhancements) to achieve this is below.\n\ndef sample_nhpp(beta, beta_j, t_j, beta_args=()):\n    \"\"\"Draw arrival times of a nonhomogeneous Poisson process.\n\n    Parameters\n    ----------\n    beta : function, call signature beta(t, *beta_args)\n        The function of the rate of arrivals as a function of time.\n    beta_j : scalar or array_like\n        If scalar, a value of beta that is greater than beta(t)\n        for all time. If array_like, then beta_j[j] &gt; beta(t) for\n        all times between t_j[j-1] and t_j[j].\n    t_j : scalar or array_like\n        If scalar, the maximum time of observation. If array_like, must\n        be the same length of `beta_j`. beta_j[j] is the value \n        of the the upper bound of the rate for the interval between\n        t[j-1] and t[j].\n\n    beta_args : tuple, default ()\n        Any other arguments passed to beta.\n\n    Returns\n    -------\n    output : Numpy array\n        Times for arrivals of the nonhomogeneous Poisson process.\n\n    Notes\n    -----\n    .. This is an implementation of the algorithm on page 86 of \n       Simulation, 5th Ed., by Sheldon Ross, Academic Press, 2013.\n    \"\"\"\n    # Convert scalar inputs to arrays\n    if np.isscalar(beta_j):\n        beta_j = np.array([beta_j])\n    if np.isscalar(t_j):\n        t_j = np.array([t_j])\n\n    # Make sure dimensions match\n    if len(beta_j) != len(t_j):\n        raise RuntimeError(\n            f'`beta_j` is length {len(beta_j)} '\n            f'and `t_j` is length {len(t_j)}. '\n            'They must have the same length.'\n        )\n\n    return _sample_nhpp(beta, beta_j, t_j, beta_args)\n\n                                                  \ndef _sample_nhpp(beta, beta_j, t_j, beta_args=()):\n    \"\"\"\n\n    Parameters\n    ----------\n    beta : function, call signature beta(t, *beta_args)\n        The function of the rate of arrivals as a function of time.\n    beta_j : Numpy array\n        If scalar, a value of beta that is greater than beta(t)\n        for all time. If array_like, then beta_j[j] &gt; beta(t) for\n        all times between t_j[j-1] and t_j[j].\n    t_j : Numpy array\n        Must be the same length of `beta_j`. beta_j[j] is the value \n        of the the upper bound of the rate for the interval between\n        t[j-1] and t[j].\n\n    beta_args : tuple, default ()\n        Any other arguments passed to beta.\n\n    Returns\n    -------\n    output : Numpy array\n        Times for arrivals of the nonhomogeneous Poisson process.\n\n    Notes\n    -----\n    .. This is an implementation of the algorithm on page 86 of \n       Simulation, 5th Ed., by Sheldon Ross, Academic Press, 2013.\n    \"\"\"\n    # Number of samples to take before concatenating arrays\n    n_samples = 1000\n\n    # Initializations\n    t = 0.0  # time\n    j = 0    # Index in beta_j and t_j arrays\n    i = 0    # index in sample array\n    n = 0    # total number of samples drawn\n    x_from_boundary = False  # If we've hit a boundary of\n\n    # Array for storing subtrajectory\n    samples = np.empty(n_samples, dtype=float)\n\n    # Output array for all samples\n    samples_output = np.array([], dtype=float)\n\n    # Loop until done (we exceed final time point)\n    not_done = True\n    while not_done:\n        # Take samples until we fill array\n        # We do it this way for speed to avoid list append operations\n        while not_done and i &lt; n_samples:\n            if x_from_boundary:\n                x = (x - t_j[j] + t) * beta_j[j] / beta_j[j+1]\n                t = t_j[j]\n                j += 1\n            else:\n                x = np.random.exponential(1 / beta_j[j])\n\n            if t + x &gt; t_j[j]:\n                # If we got here, we went past the edge of this interval\n                if j == len(t_j) - 1:\n                    not_done = False\n                else:\n                    x_from_boundary = True\n            else:\n                t += x\n                x_from_bounday = False\n                if np.random.uniform() &lt;= beta(t, *beta_args) / beta_j[j]:\n                    samples[i] = t\n                    i += 1\n                    n += 1\n        samples_output = np.concatenate((samples_output, samples))\n        i = 0\n\n    return np.array(samples_output[:n])",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling nonhomogeneous Poisson spiking</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/nonhomogeneous_poisson.html#computing-environment",
    "href": "lessons/sampling/nonhomogeneous_poisson.html#computing-environment",
    "title": "16  Modeling nonhomogeneous Poisson spiking",
    "section": "16.4 Computing environment",
    "text": "16.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,iqplot,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\niqplot    : 0.3.7\nbokeh     : 3.6.2\njupyterlab: 4.4.2",
    "crumbs": [
      "Simulating generative distributions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling nonhomogeneous Poisson spiking</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/mcmc.html",
    "href": "lessons/sampling/mcmc.html",
    "title": "Markov chain Monte Carlo",
    "section": "",
    "text": "We have seen how to sample out of distributions that have a convenient transform or inverse CDF. We have also seen how to simulate stories of distributions and to use clever tricks like thinning. We now turn to the question, how can we sample out of an arbitrary distribution for which I an write the probability density function? This has immediate applications in Bayesian inference, since it will enable us to sample out of a posterior distribution \\(g(\\theta\\mid y)\\) in order to make sense of it.\nDominant among techniques to perform sampling out of an arbitrary distribution is Markov chain Monte Carlo.",
    "crumbs": [
      "Markov chain Monte Carlo"
    ]
  },
  {
    "objectID": "lessons/sampling/basics_of_mcmc.html",
    "href": "lessons/sampling/basics_of_mcmc.html",
    "title": "17  The basics of Markov chain Monte Carlo",
    "section": "",
    "text": "17.1 Why MCMC?\nBefore we jump in to using Stan, our go-to MCMC sampling software, we need to lay some groundwork, first understaning why we want to sample out of arbitrary distributions in the context of Bayesian inference. We then go over the basic ideas of MCMC.\nWhen doing Bayesian inference, our goal is very often to understand a posterior distribution, \\(g(\\theta \\mid y)\\), where \\(\\theta = \\{\\theta^{(1)}, \\theta^{(2)},\\ldots\\}\\) is a set of possibly many parameters and \\(y\\) is the data set.1 However, just having an analytical expression for the posterior is of little use if we cannot understand any properties about it. Importantly, we often want to marginalize the posterior; that is, we want to integrate over parameters we are not immediately interested in and get distributions only for those we are. This is often necessary to understand all but the simplest models. Doing these marginalizations requires computing (nasty) integrals, which is often impossible to do analytically.\nFurthermore, we almost always want to compute statistical functionals, and expectations in particular, of the posterior. For example, we might want the mean, or expectation value, of parameter \\(\\theta^{(1)}\\). If we know the posterior, this is\n\\[\\begin{aligned}\n\\langle \\theta^{(1)}\\rangle = \\int \\mathrm{d}\\theta \\,\\theta^{(1)}\\,g(\\theta\\mid y).\n\\end{aligned}\n\\tag{17.1}\\]\nGenerally, we can compute the expectation of any function of the parameters, \\(h(\\theta)\\), and we often want to. This is\n\\[\\begin{aligned}\n\\langle h(\\theta)\\rangle = \\int \\mathrm{d}\\theta \\,h(\\theta)\\,g(\\theta\\mid y).\n\\end{aligned}\n\\tag{17.2}\\]\nSo, most things we want to know about the posterior require computation of an integral.\nMarkov chain Monte Carlo (MCMC) allows us to sample out of an arbitrary probability distribution, which includes pretty much any posterior we could write down.2 By sampling, I mean that we can choose values of the parameters, \\(\\theta\\), where the probability that we choose a given value is proportional to the posterior probability or probability density. Note that each sample consists of a complete set of parameters \\(\\theta\\); that is a sample contains a value for \\(\\theta^{(1)}\\), a value for \\(\\theta^{(2)}\\), .…\nUsing MCMC, we can collect a large number of these samples. From these samples, we can trivially perform marginalizations. Say we are interested in the marginalized distribution\n\\[\\begin{aligned}\ng(\\theta^{(1)}\\mid y) = \\left[\\int\\mathrm{d}\\theta^{(2)} \\, \\int \\mathrm{d}\\theta^{(3)}\\cdots \\right] g(\\theta\\mid y).\n\\end{aligned}\n\\tag{17.3}\\]\nGiven a set of MCMC samples out of \\(g(\\theta\\mid y)\\), to get a set of samples out of \\(g(\\theta^{(1)}\\mid y)\\), we simply ignore the values of \\(\\theta^{(2)}\\), \\(\\theta^{(3)}\\), ...! Then, given the samples of the marginalized posterior, we can plot the CDF of the marginalized posterior as an ECDF of the samples, and the PDF of the marginalized posterior as a histogram of the samples.\nTo compute other expectations, the MCMC samples are again very convenient. Now, we just approximate the integral with an average over samples.\n\\[\\begin{aligned}\n\\langle h(\\theta)\\rangle = \\int \\mathrm{d}\\theta \\,h(\\theta)\\,g(\\theta\\mid y) \\approx \\frac{1}{N}\\sum_{i=1}^N h(\\theta_i),\n\\end{aligned}\n\\tag{17.4}\\]\nwhere \\(\\theta_i\\) is the \\(i\\)th of \\(N\\) MCMC samples taken from the posterior.\nFinally, we can compute other statistical functionals, such as quantiles, directly from the samples. For example, the median of \\(\\theta^{(1)}\\) is found by computing the median value of all of the samples of \\(\\theta^{(1)}\\).\nIt is now abundantly clear why the ability to generate samples from the posterior is so powerful. But generating samples that actually come from the probability distribution of interest is not a trivial matter. We will discuss how this is accomplished through MCMC.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The basics of Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/basics_of_mcmc.html#sec-mcmc-idea",
    "href": "lessons/sampling/basics_of_mcmc.html#sec-mcmc-idea",
    "title": "17  The basics of Markov chain Monte Carlo",
    "section": "17.2 The basic idea behind MCMC",
    "text": "17.2 The basic idea behind MCMC\nOur goal is to draw independent samples from a target distribution. As we have seen, in order get independent samples, we need to be able to define a transform the converts draws from a Uniform distribution to draws from the target distribution. Unfortunately, we almost never can derive such a transformation for an arbitrary distribution, including the posterior distributions we get from Bayesian models.\nBut the samples need not be independent! Instead, we only need that the samples be generated from a process that generates samples from the target distribution in the correct proportions. If we draw enough3 of them, we do not need to worry about the lack of independence. In the case of the parameter estimation problem, the distribution we wish to draw from is the posterior distribution \\(g(\\theta\\mid y)\\). For notational simplicity in what follows, since we know we are always talking about a posterior distribution, we will use \\(P(\\theta)\\) for shorthand notation for an arbitrary distribution of \\(\\theta\\). The techniques I discuss are not limited to sampling out of posteriors; we can sample out of any distribution.\nThe approach of MCMC is to take random walks in parameter space such that the probability that a walker arrives at point \\(\\theta\\) is proportional to \\(P(\\theta)\\). This is the main concept. You should re-read that bolded sentence.\nIf we can achieve such a walk, we can just take the walker positions as samples from the distributions. To implement this random walk, we define a transition kernel, \\(T(\\theta_{i+1}\\mid \\theta_i)\\), the probability of a walker stepping from position \\(\\theta_i\\) in parameter space to position \\(\\theta_{i+1}\\). The transition kernel defines a Markov chain, which you can think of as a random walker whose next step depends only on where the walker is right now; i.e., it has no memory. Where the walker goes for step \\(i+1\\) depends only on where it was at step \\(i\\).\nThe condition that the probability of arrival at point \\(\\theta_{i+1}\\) is proportional to \\(P(\\theta_{i+1})\\) may be stated as\n\\[\\begin{aligned}\nP(\\theta_{i+1}) = \\int \\mathrm{d}\\theta_i\\, T(\\theta_{i+1}\\mid \\theta_i)\\,\nP(\\theta_i).\n\\end{aligned}\n\\tag{17.5}\\]\nHere, we have taken \\(\\theta\\) to be continuous. Were it discrete, we just replace the integral with a sum. When this relation holds, it is said that the target distribution is an invariant distribution or stationary distribution of the transition kernel. When this invariant distribution is unique, it is called a limiting distribution. We want to choose our transition kernel \\(T(\\theta_{i+1}\\mid \\theta_i)\\) such that the target distribution \\(P(\\theta)\\) is limiting. This is the case if the above equation holds and the chain is ergodic. An ergodic Markov chain has the following properties:\n\nIt is aperiodic. A periodic Markov chain can only return to a given point in parameter space after \\(k\\), \\(2k\\), \\(3k,\\ldots\\) steps, where \\(k\\) is the period. An aperiodic chain is not periodic.\nIt is irreducible, which means that any point in parameter space is accessible to the walker from any other.\nIt is positive recurrent, which means that the walker will surely revisit any point in parameter space in a finite number of steps.\n\nSo, if our transition kernel satisfies this checklist and the invariance condition, it will eventually sample the posterior distribution. We will discuss how to come up with such a transition kernel in a moment; first we focus on the important concept of “eventually” in the preceding sentence.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The basics of Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/basics_of_mcmc.html#sec-mcmc-warmup",
    "href": "lessons/sampling/basics_of_mcmc.html#sec-mcmc-warmup",
    "title": "17  The basics of Markov chain Monte Carlo",
    "section": "17.3 Warming-up an MCMC sampler",
    "text": "17.3 Warming-up an MCMC sampler\nImagine for a moment that we devised a transition kernel that satisfies the desired properties we have laid out. Say we start a walker at position \\(\\theta_0\\) in parameter space and it starts walking according to the transition kernel. Most likely, for those first few steps, the walker is traversing a part of parameter space that has incredibly low probability. Once it gets to regions of high probability, the walker will almost never return to the region of parameter space in which it began. So, unless we sample for an incredibly long time, those first few samples visited are over-weighted. So, we need to let the walker walk for a while without keeping track of the samples so that it can arrive at the limiting distribution. This is called warm-up, otherwise known as burn-in or tuning4.\nThere is no general way to tell if a walker has reached the limiting distribution, so we do not know how many warm-up steps are necessary. Fortunately, there are several heuristics. For example, Gelman and coauthors proposed generating several warm-up chains and computing the Gelman-Rubin \\(\\hat{R}\\) statistic,\n\\[\\begin{aligned}\n\\hat{R} = \\frac{\\text{variance between the chains}}{\\text{mean variance within the chains}}.\\end{aligned}\n\\tag{17.6}\\]\nLimiting chains have \\(\\hat{R} \\approx 1\\), and this is commonly used as a metric for having achieved stationarity. Gelman and his coauthors in their famous book Bayesian Data Analysis suggest that \\(|1 - \\hat{R}| &lt; 0.1\\) as a good rule of thumb for stationary chains.\nThis simple metric, though widely used, has flaws. We will not go into them in great detail here, but will defer discussion of MCMC diagnostics to a later lecture. You can read more about ensuring chains reach stationarity in this paper by Vehtari and coworkers (2020). We will use the methods outlined in that paper, along with their more stringent suggestion that \\(|1 - \\hat{R}| &lt; 0.01\\).",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The basics of Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/basics_of_mcmc.html#sec-metropolis-hastings",
    "href": "lessons/sampling/basics_of_mcmc.html#sec-metropolis-hastings",
    "title": "17  The basics of Markov chain Monte Carlo",
    "section": "17.4 Generating a transition kernel: The Metropolis-Hastings algorithm",
    "text": "17.4 Generating a transition kernel: The Metropolis-Hastings algorithm\nThe Metropolis-Hastings algorithm covers a widely used class of algorithms for MCMC sampling. I will first state the algorithm here, and then will show that it satisfies the necessary conditions for the walkers to be sampling out of the target posterior distribution.\n\n17.4.1 The algorithm/kernel\nSay our walker is at position \\(\\theta_i\\) in parameter space.\n\nWe randomly choose a candidate position \\(\\theta'\\) to step to next from an arbitrary proposal distribution \\(K(\\theta'\\mid \\theta_i)\\).\nWe compute the Metropolis ratio,\n\\[\\begin{aligned}\n    r = \\frac{P(\\theta')\\,K(\\theta_i\\mid \\theta')}\n    {P(\\theta_i)\\,K(\\theta'\\mid \\theta_i)}.\n\\end{aligned}\n\\tag{17.7}\\]\nIf \\(r \\ge 1\\), accept the step and set \\(\\theta_{i+1} = \\theta'\\). Otherwise, accept the step with probability \\(r\\). If we do not accept the step, set \\(\\theta_{i+1} = \\theta_i\\).\n\nThe last two steps are used to define the transition kernel \\(T(\\theta_{i+1}\\mid \\theta_i)\\). We can define the acceptance probability of the proposal step as\n\\[\\begin{aligned}\n\\alpha(\\theta_{i+1}\\mid \\theta_i) = \\min(1, r) = \\min\\left(1, \\frac{P(\\theta_{i+1})\\,K(\\theta_i\\mid \\theta_{i+1})}\n    {P(\\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i)}\\right).\n\\end{aligned}\n  \\tag{17.8}\\]\nThen, the transition kernel is\n\\[\\begin{aligned}\nT(\\theta_{i+1}\\mid \\theta_i) = \\alpha(\\theta_{i+1}\\mid \\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i).\n\\end{aligned}\n\\tag{17.9}\\]\n\n\n17.4.2 Detailed balance\nThis algorithm seems kind of nuts! How on earth does this work? To investigate this, we consider the joint probability, \\(P(\\theta_{i+1}, \\theta_i)\\), that the walker is at \\(\\theta_i\\) and \\(\\theta_{i+1}\\) at sequential steps. We can write this in terms of the transition kernel,\n\\[\\begin{aligned}\nP(\\theta_{i+1}, \\theta_i) &= P(\\theta_i)\\,T(\\theta_{i+1}\\mid \\theta_i) \\nonumber \\\\[1em]\n&= P(\\theta_i)\\,\\alpha(\\theta_{i+1}\\mid \\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i) \\nonumber \\\\[1em]\n&= P(\\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i)\\,\\min\\left(1, \\frac{P(\\theta_{i+1})\\,K(\\theta_i\\mid \\theta_{i+1})}\n    {P(\\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i)}\\right) \\nonumber \\\\[1em]\n&= \\min\\left[P(\\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i),\nP(\\theta_{i+1})\\,K(\\theta_i\\mid \\theta_{i+1})\\right] \\nonumber \\\\[1em]\n&= P(\\theta_{i+1})\\,K(\\theta_i\\mid \\theta_{i+1})\\,\\min\\left(1,\\frac{P(\\theta_i)\\,K(\\theta_{i+1}\\mid \\theta_i)}{P(\\theta_{i+1})\\,K(\\theta_i\\mid \\theta_{i+1})}\\right) \\nonumber \\\\[1em]\n&= P(\\theta_{i+1})\\, \\alpha(\\theta_i\\mid \\theta_{i+1})\\,K(\\theta_i\\mid \\theta_{i+1})\\, \\nonumber \\\\[1em]\n&= P(\\theta_{i+1})\\, T(\\theta_i\\mid \\theta_{i+1}).\n\\end{aligned}\n\\tag{17.10}\\]\nThus, we have\n\\[\\begin{aligned}\nP(\\theta_i)\\,T(\\theta_{i+1}\\mid \\theta_i) = P(\\theta_{i+1})\\, T(\\theta_i\\mid \\theta_{i+1}).\n\\end{aligned}\n\\tag{17.11}\\]\nThis says that the rate of transition from \\(\\theta_i\\) to \\(\\theta_{i+1}\\) is equal to the rate of transition from \\(\\theta_{i+1}\\) to \\(\\theta_i\\). In this case, the transition kernel is said to satisfy detailed balance.\nAny transition kernel that satisfies detailed balance has \\(P(\\theta)\\) as an invariant distribution. This is easily shown.\n\\[\\begin{aligned}\n\\int \\mathrm{d}\\theta_i \\,P(\\theta_i)\\,T(\\theta_{i+1}\\mid \\theta_i)\n&= \\int \\mathrm{d}\\theta_i\\,P(\\theta_{i+1})\\, T(\\theta_i\\mid \\theta_{i+1}) \\nonumber \\\\[1em]\n&= P(\\theta_{i+1})\\left[\\int \\mathrm{d}\\theta_i\\, T(\\theta_i\\mid \\theta_{i+1})\\right] \\nonumber \\\\[1em]\n&= P(\\theta_{i+1}),\n\\end{aligned}\n\\tag{17.12}\\]\nsince the bracketed term is unity because the transition kernel is a probability density5.\nNote that all transition kernels that satisfy detailed balance have an invariant distribution. (If the chain is ergodic, this is a limiting distribution.) But not all kernels that have an invariant distribution satisfy detailed balance. So, detailed balance is a sufficient condition for a transition kernel having an invariant distribution.\n\n\n17.4.3 Choosing the transition kernel\nThere is an art to choosing the transition kernel. The original Metropolis algorithm (1953), took \\(K(\\theta_{i+1}\\mid \\theta_i) = 1\\). As a rule of thumb, you want to choose a proposal distribution such that you get an acceptance rate of about 0.4. If you accept every step, the walker just wanders around and it takes a while to get to the limiting distribution. If you reject too many steps, the walkers never move, and it again takes a long time to get to the limiting distribution. There are tricks to “tune” the walkers to achieve the target acceptance rate.\nGibbs sampling is a more modern approach to defining transition kernels. It was very popular for many years, though we will not go into the details. It is a special case of a Metropolis-Hastings sampler. These days, most samplers use Hamiltonian Monte Carlo (HMC). Stan uses a highly optimized HMC sampler which gives significant performance improvements for important subclasses of problems. To my knowledge and in my opinion, Stan is the current state-of-the-art in MCMC sampling software. (I cannot overemphasize how much of a performance improvement Stan’s sampler offers; it allows sampling of posteriors that are impossible with a more naive implementation of Metropolis-Hastings.)\nImportantly, the HMC sampler can only handle continuous variables; it cannot sample discrete variables. Depending on your problem, this could be a limitation, but in most applications we are interested in estimating continuous parameters. There are also ways around sampling discrete variables, such as explicitly marginalizing over them, such that we rarely need to sample them anyhow.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The basics of Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/basics_of_mcmc.html#footnotes",
    "href": "lessons/sampling/basics_of_mcmc.html#footnotes",
    "title": "17  The basics of Markov chain Monte Carlo",
    "section": "",
    "text": "It will become clear why I am using this strange superscripted parenthetical indexing of the parameters as we continue with the MCMC discussion. I need other subscripts later on.↩︎\nWell, not any. For some cases, we may not be able to make a transition kernel that satisfies the necessary properties, which I describe in the following pages.↩︎\nThe word \"enough\" is pretty loaded here. We need to draw enough samples such that we get a sufficient number effectively independent samples. One algorithm for drawing samples might require 1000 samples for each effectively independent one. Another, better one might only require 10. Margossian and Gelman have a nice discussion on this.↩︎\nWhen using Stan’s sampler, the warm-up is a bit more than just burn-in, which is just regular sampling where we simply neglect the first bunch of samples. Stan’s algorithm is actively choosing stepping strategies during the warm-up phase.↩︎\nIn the event that we are dealing with discrete parameters \\(\\theta\\), then the integral is replaced with a sum and the transition kernel is a probability, and not a probability density. The same results still hold and \\(P(\\theta)\\) is an invariant distribution.↩︎",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The basics of Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html",
    "href": "lessons/sampling/stan_hello_world.html",
    "title": "18  “Hello, world” —Stan",
    "section": "",
    "text": "18.1 Basics of Stan programs\n| Download notebook\nWhen getting familiar with a new programming language, we often write a “Hello, world” program. This is a simple, often minimal, to demonstrate some of the basic syntax of the language. Python’s “Hello, world” program is:\nHere, we introduce Stan, and write a “Hello, world” program for it.\nBefore we do, we note that you may run Stan on your own machine if you have managed to get Stan and CmdStanPy installed. Otherwise, you can use AWS using the bebi103 Amazon Machine Image, available in the Oregon region. If you wish, you may also use Google Colab, though you will be limited in how many cores you can use and how long you can use them.\nThis is our first introduction to Stan, a probabilistic programming language that we will use for much of our statistical modeling. Stan is a separate language. It is not Python. It has a command line interface and interfaces for R, Python, Julia, Matlab, and Stata.\nWe will be using one of the two Python interfaces, CmdStanPy. PyStan is another popular interface. Remember, though, that Stan is a separate language, and any Stan program you write works across all of these interfaces.\nBefore we dive in and write our first Stan program to draw samples out of the Normal distribution, I want to tell you a few things about Stan. Briefly, Stan works as follows when using the CmdStanPy interface.\nWe will learn the Stan language structure and syntax as we go along. To start with, a Stan program consists of seven sections, called blocks. They are, in order\nNot all blocks need to be in a Stan program, but they must be in this order. Some other important points to keep in mind as we venture into Stan:",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#basics-of-stan-programs",
    "href": "lessons/sampling/stan_hello_world.html#basics-of-stan-programs",
    "title": "18  “Hello, world” —Stan",
    "section": "",
    "text": "A user writes a model using the Stan language. This is usually stored in a .stan text file.\nThe model is compiled in two steps. First, Stan translates the model in the .stan file into C++ code. Then, that C++ code is compiled into machine code.\nOnce the machine code is built, the user can, via the CmdStanPy interface, sample out of the distribution defined by the model and perform other calculations (such as optimization and variational inference) with the model.\nThe results from the sampling are written to disk as CSV and txt files. As demonstrated below, we conveniently access these files using ArviZ, so we do not directly interact with them.\n\n\n\nfunctions: Any user-defined functions that can be used in other blocks.\ndata: Any inputs from the user. Most commonly, these are measured data themselves. You can also put user-adjustable parameters in this block as well, but nothing you intend to sample.\ntransformed data: Any transformations that need to be done on the data.\nparameters: The parameters of the model. Stan will give you samples of the variables described in this block. In the context of Bayesian inference, these are the \\(\\theta\\) in the posterior \\(g(\\theta\\mid y)\\).\ntransformed parameters: Any transformations that need to be done on the parameters.\nmodel: Specification of the generative model. The sampler will sample the parameters \\(\\theta\\) out of this model.\ngenerated quantities: Any other quantities you want to calculate with each sample.\n\n\n\nThe Stan documentation will be a very good friend of yours, both the user’s guide and reference manual.\nThe index origin of Stan is 1, not 0 as in Python.\nStan is strongly statically typed, which means that you need to declare the data type of a variable explicitly before using it.\nAll Stan commands must end with a semicolon.\nBlocks of code are separated using curly braces.\nStan programs are stored outside of your notebook in a .stan file. These are text files, which you can prepare with your favorite text editor, including the one included in JupyterLab.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#say-hi-stan",
    "href": "lessons/sampling/stan_hello_world.html#say-hi-stan",
    "title": "18  “Hello, world” —Stan",
    "section": "18.2 Say hi, Stan",
    "text": "18.2 Say hi, Stan\nWith this groundwork laid, let’s just go ahead and write our “Hello, world” Stan program to generate samples out of a standard Normal distribution (with zero mean and unit variance). (Note that this is not sampling out of a posterior.) Here is the code, which I have stored in the file hello_world.stan.\nparameters {\n  real x;\n}\n\n\nmodel {\n  x ~ std_normal();\n}\nNote that there are two blocks in this particular Stan code, the parameters block and the model block. These are two of the seven possible blocks in a Stan code, and we will explore others in the next part of the lesson when we learn more about Stan after we complete our Hello, world program.\nIn the parameters block, we have the names and types of parameters we want to obtain samples for. In this case, we want to obtain samples of a real number we will call x.\nIn the model block, we have our statistical model. The syntax is similar to how we would write the model on paper. We specify that x, the parameter we want to get samples of, is Normally distributed with location parameter zero and scale parameter one, the standard normal.\nNow that we have our code (which I have stored in a file named hello_world.stan), we can use CmdStanPy to compile it and get CmdStanModel, which is a Python object that provides access to the compiled Stan executable that we can conveniently access using Python syntax.\n\nsm = cmdstanpy.CmdStanModel(stan_file='hello_world.stan')\n\n00:52:11 - cmdstanpy - INFO - compiling stan file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/sampling/hello_world.stan to exe file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/sampling/hello_world\n00:52:14 - cmdstanpy - INFO - compiled model executable: /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/sampling/hello_world\n\n\nNow that we have the Stan model, stored as the variable sm, we can collect samples from it using the sm.sample() method. We pass in the number of chains; that is, the number of Markov chains to use in sampling. We can also pass in the number of sampling iterations to do. We’ll do four chains, which each taking 1000 samples. Let’s do it!\n\nsamples = sm.sample(\n    chains=4,\n    iter_sampling=1000,\n)\n\n00:52:14 - cmdstanpy - INFO - CmdStan start processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\n00:52:14 - cmdstanpy - INFO - CmdStan done processing.\n\n\n\n\n\nNotice that CmdStanPy conveniently gave us progress bars for the sampling. We can turn those off using the show_progress=False kwarg of sm.sample().",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#parsing-output-with-arviz",
    "href": "lessons/sampling/stan_hello_world.html#parsing-output-with-arviz",
    "title": "18  “Hello, world” —Stan",
    "section": "18.3 Parsing output with ArviZ",
    "text": "18.3 Parsing output with ArviZ\nAt this point, Stan did its job and acquired the samples. So, it said “hello, world.”\nLet’s take a look at the samples. They are stored as a CmdStanMCMC instance.\n\nsamples\n\nCmdStanMCMC: model=hello_world chains=4['method=sample', 'num_samples=1000', 'algorithm=hmc', 'adapt', 'engaged=1']\n csv_files:\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_1.csv\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_2.csv\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_3.csv\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_4.csv\n output_files:\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_0-stdout.txt\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_1-stdout.txt\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_2-stdout.txt\n    /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/tmpc164di3v/hello_world5eza7drh/hello_world-20250515005214_3-stdout.txt\n\n\nThis object that was returned by CmdStanPy points to CSV and text files Stan generated while running. We can load them into a more convenient format using ArviZ (pronounced like “RVs”, the abbreviation for “recreational vehicles” or “random variables”).\n\nsamples = az.from_cmdstanpy(samples)\n\n# Take a look\nsamples\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 40kB\nDimensions:  (chain: 4, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    x        (chain, draw) float64 32kB -0.01934 0.4713 0.5241 ... -2.013 -1.148\nAttributes:\n    created_at:                 2025-05-15T07:52:14.475961+00:00\n    arviz_version:              0.21.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.5xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (1)x(chain, draw)float64-0.01934 0.4713 ... -2.013 -1.148array([[-0.0193439,  0.471292 ,  0.524085 , ...,  0.199016 , -0.370709 ,\n         0.168701 ],\n       [-0.0384745,  0.269546 ,  0.0843281, ..., -0.809101 , -1.09216  ,\n        -1.50257  ],\n       [ 0.0232424, -1.1045   , -0.466936 , ...,  0.432153 , -0.884424 ,\n        -1.92024  ],\n       [-0.561542 , -0.881963 , -0.381456 , ..., -0.617422 , -2.01335  ,\n        -1.14838  ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2025-05-15T07:52:14.475961+00:00arviz_version :0.21.0inference_library :cmdstanpyinference_library_version :1.2.5\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 204kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    lp               (chain, draw) float64 32kB -0.0001871 -0.1111 ... -0.6594\n    acceptance_rate  (chain, draw) float64 32kB 0.9659 0.9873 ... 0.7065 1.0\n    step_size        (chain, draw) float64 32kB 0.8709 0.8709 ... 0.973 0.973\n    tree_depth       (chain, draw) int64 32kB 1 2 1 1 1 1 2 2 ... 2 1 1 1 1 1 1\n    n_steps          (chain, draw) int64 32kB 3 3 1 3 3 1 3 3 ... 3 1 3 3 3 3 1\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB 1.432 0.1148 ... 2.128 1.706\nAttributes:\n    created_at:                 2025-05-15T07:52:14.477720+00:00\n    arviz_version:              0.21.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.5xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)lp(chain, draw)float64-0.0001871 -0.1111 ... -0.6594array([[-1.87094e-04, -1.11058e-01, -1.37333e-01, ..., -1.98037e-02,\n        -6.87124e-02, -1.42300e-02],\n       [-7.40144e-04, -3.63276e-02, -3.55561e-03, ..., -3.27322e-01,\n        -5.96410e-01, -1.12886e+00],\n       [-2.70105e-04, -6.09961e-01, -1.09015e-01, ..., -9.33780e-02,\n        -3.91103e-01, -1.84367e+00],\n       [-1.57665e-01, -3.88929e-01, -7.27542e-02, ..., -1.90605e-01,\n        -2.02678e+00, -6.59387e-01]])acceptance_rate(chain, draw)float640.9659 0.9873 0.995 ... 0.7065 1.0array([[0.96586 , 0.987294, 0.994983, ..., 0.782646, 0.994486, 0.988668],\n       [1.      , 0.990683, 0.968129, ..., 0.93986 , 0.94965 , 0.967609],\n       [1.      , 0.849177, 0.829338, ..., 1.      , 0.907224, 0.630252],\n       [1.      , 0.946493, 0.974362, ..., 0.944828, 0.706453, 1.      ]])step_size(chain, draw)float640.8709 0.8709 ... 0.973 0.973array([[0.870947, 0.870947, 0.870947, ..., 0.870947, 0.870947, 0.870947],\n       [0.92854 , 0.92854 , 0.92854 , ..., 0.92854 , 0.92854 , 0.92854 ],\n       [1.15493 , 1.15493 , 1.15493 , ..., 1.15493 , 1.15493 , 1.15493 ],\n       [0.973029, 0.973029, 0.973029, ..., 0.973029, 0.973029, 0.973029]])tree_depth(chain, draw)int641 2 1 1 1 1 2 2 ... 1 2 1 1 1 1 1 1array([[1, 2, 1, ..., 2, 2, 1],\n       [2, 1, 2, ..., 1, 1, 2],\n       [1, 1, 2, ..., 1, 2, 1],\n       [2, 1, 2, ..., 1, 1, 1]])n_steps(chain, draw)int643 3 1 3 3 1 3 3 ... 1 3 1 3 3 3 3 1array([[3, 3, 1, ..., 3, 7, 3],\n       [3, 3, 3, ..., 3, 1, 3],\n       [3, 3, 3, ..., 1, 3, 1],\n       [3, 1, 3, ..., 3, 3, 1]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float641.432 0.1148 0.1516 ... 2.128 1.706array([[1.43181  , 0.114763 , 0.151609 , ..., 2.44874  , 0.0739619,\n        0.161438 ],\n       [0.218632 , 0.0635614, 0.295021 , ..., 0.423636 , 0.608507 ,\n        1.14692  ],\n       [0.182711 , 0.681241 , 1.66056  , ..., 0.363596 , 0.627483 ,\n        1.85709  ],\n       [0.399558 , 0.394084 , 0.603211 , ..., 0.46799  , 2.12783  ,\n        1.70613  ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2025-05-15T07:52:14.477720+00:00arviz_version :0.21.0inference_library :cmdstanpyinference_library_version :1.2.5\n                      \n                  \n            \n            \n              \n            \n            \n\n\nWe used ArviZ to convert the data type to an ArviZ InferenceData data type. This has two groups, posterior, which contains the samples, and sample_stats which gives information about the sampling. (Note that ArviZ named the group “posterior,” which it does by default, even though these samples are out of a standard Normal distribution and not out of a posterior distribution for some model we may have built.) We’ll start by looking at the samples themselves. Since the samples were taken using the model block, they are assumed to be samples out of a posterior distribution, and are therefore present in the samples.posterior group.\n\nsamples.posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 40kB\nDimensions:  (chain: 4, draw: 1000)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    x        (chain, draw) float64 32kB -0.01934 0.4713 0.5241 ... -2.013 -1.148\nAttributes:\n    created_at:                 2025-05-15T07:52:14.475961+00:00\n    arviz_version:              0.21.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.5xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (1)x(chain, draw)float64-0.01934 0.4713 ... -2.013 -1.148array([[-0.0193439,  0.471292 ,  0.524085 , ...,  0.199016 , -0.370709 ,\n         0.168701 ],\n       [-0.0384745,  0.269546 ,  0.0843281, ..., -0.809101 , -1.09216  ,\n        -1.50257  ],\n       [ 0.0232424, -1.1045   , -0.466936 , ...,  0.432153 , -0.884424 ,\n        -1.92024  ],\n       [-0.561542 , -0.881963 , -0.381456 , ..., -0.617422 , -2.01335  ,\n        -1.14838  ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2025-05-15T07:52:14.475961+00:00arviz_version :0.21.0inference_library :cmdstanpyinference_library_version :1.2.5\n\n\nThis is a new, interesting data type. This is an xarray Dataset. The xarray package is a very powerful package for data analysis. The two main data types we will use are xarray DataArrays and xarray Datasets. You can think of a DataArray like a Pandas data frame, except that the data need not be structured in a two-dimensional table like a data frame is. A Dataset is a collection of DataArrays and associated attributes. Interestingly, if multiple DataArrays in a Dataset have the same indexes, you can index multiple arrays at the same time.\nEssentially, you can think of xarray structures as data frames that can be arbitrarily multidimensional.\nIf we want to access the samples of x, we do so like this.\n\nsamples.posterior['x']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'x' (chain: 4, draw: 1000)&gt; Size: 32kB\narray([[-0.0193439,  0.471292 ,  0.524085 , ...,  0.199016 , -0.370709 ,\n         0.168701 ],\n       [-0.0384745,  0.269546 ,  0.0843281, ..., -0.809101 , -1.09216  ,\n        -1.50257  ],\n       [ 0.0232424, -1.1045   , -0.466936 , ...,  0.432153 , -0.884424 ,\n        -1.92024  ],\n       [-0.561542 , -0.881963 , -0.381456 , ..., -0.617422 , -2.01335  ,\n        -1.14838  ]])\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999xarray.DataArray'x'chain: 4draw: 1000-0.01934 0.4713 0.5241 -0.2863 -2.723 ... 0.4026 -0.6174 -2.013 -1.148array([[-0.0193439,  0.471292 ,  0.524085 , ...,  0.199016 , -0.370709 ,\n         0.168701 ],\n       [-0.0384745,  0.269546 ,  0.0843281, ..., -0.809101 , -1.09216  ,\n        -1.50257  ],\n       [ 0.0232424, -1.1045   , -0.466936 , ...,  0.432153 , -0.884424 ,\n        -1.92024  ],\n       [-0.561542 , -0.881963 , -0.381456 , ..., -0.617422 , -2.01335  ,\n        -1.14838  ]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (0)\n\n\nWe see that this is a two dimensional array, with the first index (the rows) being the chain and the second index (the columns) being the draw, of which there are 1000 for each chain. We can put all of our draws together by converting the DataArray to a Numpy array using the .values attribute and then raveling the Numpy array, and then plot an ECDF. The ECDF should look like a Normal distribution with location parameter zero and scale parameter one.\n\nbokeh.io.show(\n    iqplot.ecdf(\n        samples.posterior['x'].values.ravel()\n    )\n)\n\n\n  \n\n\n\n\n\nIndeed it does! We have just verified that Stan properly said, “Hello, world.”",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#direct-sampling",
    "href": "lessons/sampling/stan_hello_world.html#direct-sampling",
    "title": "18  “Hello, world” —Stan",
    "section": "18.4 Direct sampling",
    "text": "18.4 Direct sampling\nStan can also draw samples out of probability distributions without using MCMC, just as Numpy and Scipy can. For a generic distribution, we use MCMC, but for many named distributions we can directly sample.\nLet’s draw 300 random numbers from a Normal distribution with location parameter zero and scale parameter one using Numpy and Scipy.\n\nrng = np.random.default_rng()\nnp_samples = rng.normal(0, 1, size=300)\n\nsp_samples = st.norm.rvs(0, 1, size=300)\n\n# Plot samples\np = iqplot.ecdf(\n    np_samples,\n    style='staircase',\n    palette=[colorcet.b_glasbey_category10[0]],\n)\n\np = iqplot.ecdf(\n    sp_samples,\n    style='staircase',\n    palette=[colorcet.b_glasbey_category10[1]],\n    p=p,\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nTo generate random draws from a standard Normal distribution without using Markov chain Monte Carlo, we use the following Stan code.\ngenerated quantities {\n  real x;\n\n  x = std_normal_rng();\n}\nLet’s compile it, and then comment on the code.\n\nsm_rng = cmdstanpy.CmdStanModel(stan_file='norm_rng.stan')\n\n00:52:14 - cmdstanpy - INFO - compiling stan file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/sampling/norm_rng.stan to exe file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/sampling/norm_rng\n00:52:16 - cmdstanpy - INFO - compiled model executable: /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/sampling/norm_rng\n\n\nThere is just one block in this particular Stan code, the generated quantities block. In the generated quantities block, we have code for that tells Stan what to generate for each set of parameters it encountered while doing Markov chain Mote Carlo. Here, we are not performing Markov chain Monte Carlo, so we do the “sampling” in fixed parameter mode when we call sm_rng.sample() by setting the fixed_param kwarg to True.\n\n# Draw samples\nstan_samples = sm_rng.sample(\n    chains=1,\n    iter_sampling=300,\n    fixed_param=True,\n    show_progress=False,\n)\n\n00:52:16 - cmdstanpy - INFO - CmdStan start processing\n00:52:16 - cmdstanpy - INFO - Chain [1] start processing\n00:52:16 - cmdstanpy - INFO - Chain [1] done processing\n\n\nTo convert this sampling object to a Numpy array, we can first convert it to an ArviZ InferenceData instance and then extract the Numpy array. Note that we will define the samples as coming from a “posterior,” even though it is not a posterior, since that’s the default for ArviZ.\n\n# Convert to ArviZ InferenceData\nstan_samples = az.from_cmdstanpy(\n    posterior=stan_samples\n)\n\n# Extract Numpy array\nstan_samples = stan_samples.posterior['x'].values.flatten()\n\nNow, we can add the ECDF of these samples to the plot of Numpy and Scipy samples.\n\np = iqplot.ecdf(\n    stan_samples,\n    style='staircase',\n    palette=[colorcet.b_glasbey_category10[2]],\n    p=p,\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAs we expect, sampling with Stan gives the same results.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#displaying-your-stan-code",
    "href": "lessons/sampling/stan_hello_world.html#displaying-your-stan-code",
    "title": "18  “Hello, world” —Stan",
    "section": "18.5 Displaying your Stan code",
    "text": "18.5 Displaying your Stan code\nWhen you are working on assignments, your Stan models are written as separate files. It is instructive to display the Stan code in the Jupyter notebook. This is easily accomplished for any CmdStanPy model using the code() method.\n\nprint(sm.code())\n\nparameters {\n  real x;\n}\n\n\nmodel {\n  x ~ std_normal();\n}\n\n\nYou should do this in your notebooks so the code is visible.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#saving-samples",
    "href": "lessons/sampling/stan_hello_world.html#saving-samples",
    "title": "18  “Hello, world” —Stan",
    "section": "18.6 Saving samples",
    "text": "18.6 Saving samples\nWhile your samples are saved in CSV and text files by Stan, is is convenient to save the sampling information in a format the can immediately be read into an ArviZ InferenceData object. The NetCDF format is useful for this. ArviZ enables saving as NetCDF as follows.\n\nsamples.to_netcdf('stan_hello_world.nc')\n\n'stan_hello_world.nc'\n\n\nWhen calling the function, it returns the string of the filename to which the NetCDF file is written. The samples can be read from the NetCDF file using az.from_netcdf().\n\nsamples = az.from_netcdf('stan_hello_world.nc')",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#cleaning-up-the-shrapnel",
    "href": "lessons/sampling/stan_hello_world.html#cleaning-up-the-shrapnel",
    "title": "18  “Hello, world” —Stan",
    "section": "18.7 Cleaning up the shrapnel",
    "text": "18.7 Cleaning up the shrapnel\nWhen using Stan, CmdStanPy leaves a lot of files on your file system.\n\nYour stan model is translated into C++, and the result is stored in a .hpp file.\nThe .hpp file is compiled into an object file (.o file).\nThe .o file is used to build an executable.\n\nAll of these files are deposited in your present working directory, and can get annoying for version control purposes and can add clutter. To clean them up after you are finished running your models, you can run the function below.\n\nbebi103.stan.clean_cmdstan()\n\nWhen doing sampling the results are stored in a /var/ directory in various CSV and text files. We never work with these directly, but rather read them into RAM in a convenience az.InferenceData object using ArviZ. When exiting your session, CmdStanPy deletes all of these CSV files, etc., unless you specifically say which directory to store the results in your call to sm.sample() using the outpur_dir kwarg.",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/stan_hello_world.html#computing-environment",
    "href": "lessons/sampling/stan_hello_world.html#computing-environment",
    "title": "18  “Hello, world” —Stan",
    "section": "18.8 Computing environment",
    "text": "18.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,cmdstanpy,arviz,iqplot,bebi103,bokeh,colorcet,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\ncmdstanpy : 1.2.5\narviz     : 0.21.0\niqplot    : 0.3.7\nbebi103   : 0.1.27\nbokeh     : 3.6.2\ncolorcet  : 3.1.0\njupyterlab: 4.4.2\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>\"Hello, world\"  —Stan</span>"
    ]
  },
  {
    "objectID": "lessons/sampling/nonhomogeneous_poisson_stan.html",
    "href": "lessons/sampling/nonhomogeneous_poisson_stan.html",
    "title": "19  Nonhomogeneous Poisson process arrival times with Stan",
    "section": "",
    "text": "19.1 Computing environment\n| Download notebook\nWe have posited that we can use MCMC to sample out of an arbitrary distribution so long as we can write down the PDF. We will put this to use in Bayesian inference contexts soon, but for now, let us generate samples out of a nontrivial distribution, that for the arrival times of a nonhomogeneous Poisson process. In doing so, we will expose some neat features of Stan.\nYou will recall from ?sec-variable-spiking that we can use thinning to sample out of a nonhomogeneous Poisson process. We repeat the sample_nhpp() function below in a (folded) code cell.\nWe can again sample out of this distribution using the thinning technique so we have some samples to compare. We will again have a sinusoidal variation in arrival rate.\nWe can write down the probability density function for the arrival times, given by {Equation 16.11}. Barring any chronic issues with the sampler, we can use Markov chain Monte Carlo to do the sampling.\nHere, I present the Stan code for doing so, and then discuss its contents.\nFirst, we see a demonstration of how functions in the functions block work. Each function is declared with its return type. The function beta_, for example, returns a real, so it is declared as real beta_(...). (We name the function beta_ instead of beta because beta is part of the Stan language, standing for the Beta distribution.) The type of each argument must also be specified, separated by commas. Each function must also have a return statement, with the value being returned following return.\nNext, not how we used the data block to enable passing in parameters. The data block is used to pass anything that does not change or get sampled in a Stan program.\nIn the parameters block, we define a positive_ordered data type that we use for the arrival times. This is a convenient data type in Stan. It comprises a vector where all entries are positive, and they are ordered from smallest to largest in magnitude. Since the arrival times are indeed positive and ordered, this data type is perfect for the present application.\nIn the model block, we add values to target. target is a special Stan variable that is the log probability density of the distribution being sampled out of, the so-called target distribution. Under the hood, sampling statements, such as x ~ std_normal(), add the appropriate quantities to target. When a sampling statement is not available available for a given distribution, as is the case here, we directly add the appropriate log probability density to target.\nNow that we have our code, let’s compile it and give it a whirl! Since we aim to draw a single set of spike times, we will only use one chain and take one sample (since a single sample is 500 spikes). We will still take 1000 warm up samples (though this is excessive; we do not need that many).\nLet’s plot to see if we get the same kind of set of spikes as in the sampling by thinning.\nIndeed, we do!\nTo demonstrate that we are actually sampling out of the correct distribution, we can approximate the arrival rate by counting how many spikes arrive within each one-time-unit bin. To do that, we will get many more samples and then perform the binning and average the number of spikes that arrived in each bin for each sample.\nFirst, the samples….\nNow we can reshape the data, stacking all of the draws together in a single Numpy array.\nNow we can compute the mean number of spikes per unit time in each one-time-unit bin.\nTo make the plot, it is useful to have a function that can convert bins-histogram values to \\(x\\) and \\(y\\) coordinates to plot a histogram.\nThis this function in hand, we can make out plot!\nBeautiful! Indeed we are sampling arrival times out of the correct nonhomogeneous Poisson process.\n%load_ext watermark\n%watermark -v -p numpy,cmdstanpy,arviz,iqplot,bebi103,bokeh,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\ncmdstanpy : 1.2.5\narviz     : 0.21.0\niqplot    : 0.3.7\nbebi103   : 0.1.27\nbokeh     : 3.6.2\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Markov chain Monte Carlo",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonhomogeneous Poisson process arrival times with Stan</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/bayesian_modeling.html",
    "href": "lessons/bayesian_modeling/bayesian_modeling.html",
    "title": "Bayesian modeling and inference",
    "section": "",
    "text": "We now turn the instruments of probability to modeling data generative processes. One could say that the rest of the workshop is entirely doing this, since we will build models for everything we do. But we must crawl before we walk, so in this section, we go over some of the basics of building generative models.",
    "crumbs": [
      "Bayesian modeling and inference"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/basics_of_bayes.html",
    "href": "lessons/bayesian_modeling/basics_of_bayes.html",
    "title": "20  Basics of Bayesian modeling",
    "section": "",
    "text": "20.1 Tasks of Bayesian modeling\nWe have learned about Bayes’s theorem as a way to update a hypothesis in light of new data. We use the word “hypothesis” very loosely here. Remember, in the Bayesian view, probability can describe the plausibility of any proposition. The value of a parameter is such a proposition, and we will apply the term “hypothesis” to parameter values as well. Again for concreteness, we will consider the problem of parameter estimation here, where \\(\\theta\\) will denote a set of parameter values, our “hypothesis” in this context.\nAs we have seen, our goal is to obtain updated knowledge about \\(\\theta\\) as codified in the posterior distribution, \\(g(\\theta \\mid y)\\), where \\(y\\) is the data we measured. Bayes's theorem gives us access to this distribution.\n\\[\\begin{align}\ng(\\theta \\mid y) = \\frac{f(y\\mid\\theta)\\,g(\\theta)}{f(y)}.\n\\end{align}\\]\nNote, though, that the split between the likelihood and prior need not be explicit. In fact, when we study hierarchical models, we will see that it is not obvious how to unambiguously define the likelihood and prior. In order to do Bayesian inference, we really only need to specify the joint distribution, \\(\\pi(y,\\theta) = f(y\\mid\\theta)\\,g(\\theta)\\), which is the product of the likelihood and prior. The posterior can also be written as\n\\[\\begin{align}\ng(\\theta \\mid y) = \\frac{\\pi(y,\\theta)}{f(y)}.\n\\end{align}\\]\nBecause the evidence, \\(f(y)\\), is computed from the likelihood and prior (or more generally from the joint distribution) as\n\\[\\begin{align}\nf(y) = \\int\\mathrm{d}\\theta\\,\\pi(y,\\theta) = \\int\\mathrm{d}\\theta\\,f(y\\mid\\theta)\\,g(\\theta),\n\\end{align}\\]\nthe posterior is fully specified by the joint distribution. In light of this, there are two broad tasks in Bayesian modeling.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Basics of Bayesian modeling</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/basics_of_bayes.html#tasks-of-bayesian-modeling",
    "href": "lessons/bayesian_modeling/basics_of_bayes.html#tasks-of-bayesian-modeling",
    "title": "20  Basics of Bayesian modeling",
    "section": "",
    "text": "Define the joint distribution \\(\\pi(y,\\theta)\\), most commonly specified as a likelihood and prior.\nMake sense of the resulting posterior.\n\n\n20.1.1 Model building\nStep (1) is the task of model building, or modeling. This is typically done by first specifying a likelihood, \\(f(y\\mid \\theta)\\). This is a generative distribution in that it gives the full description of how data \\(y\\) are generated given parameters \\(\\theta\\). Importantly, included in the likelihood is a theoretical model.\nAfter the likelihood is constructed we need to specify prior distributions for the parameters of the likelihood. That is, the likelihood, as a model of the generative process, prescribes what the parameters \\(\\theta\\) of the model are (but not their values). Knowing what parameters we need to write priors for, we go about codifying what we know about the parameters before we do the experiment as prior probabilities.\nSo, the process of model building involves prescribing a likelihood that describes the process of data generation. The parameters to be estimated are defined in the likelihood, and from these we construct priors. We will discuss how we go about building likelihoods and priors in this lecture and subsequently in the course.\n\n20.1.1.1 The role of the prior\nAs an aside, I pause to note that there may be some philosophical issues with this approach. I think Gelman, Simpson, and Betancourt clearly stated the dilemma in their 2017 paper with the apt title, \"The Prior Can Often Only Be Understood in the Context of the Likelihood\" (emphasis added by me).\n\nOne can roughly speak of two sorts of Bayesian analyses. In the first sort, the Bayesian formalism can be taken literally: a researcher starts with a prior distribution about some externally defined quantity, perhaps some physical parameter or the effect of some social intervention, and then he or she analyzes data, leading to an updated posterior distribution. Here, the prior can be clearly defined, not just before the data are observed but before the experiment has even been considered. ...[W]e are concerned with a second sort of analysis, what might be patterned Bayesian analysis using default priors, in which a researcher gathers data and forms a model that includes various unknown parameters and then needs to set up a prior distribution to continue with Bayesian inference. This latter approach is standard in Bayesian workflow...\nOne might say that what makes a prior a prior, rather than simply a probability distribution, is that it is destined to be paired with a likelihood. That is, the Bayesian formalism requires that a prior distribution be updated into a posterior distribution based on new data.\n\nWe are taking the second approach the Gelman, Simpson, and Betancourt laid out; the approach where the prior is in service to a likelihood. We are trying to model a data generation process, which requires a likelihood to even identify what the parameters are that require priors.\n\n\n\n20.1.2 Making sense of the posterior\nIf we can write down the likelihood and prior, or more generally the joint distribution \\(\\pi(y,\\theta)\\), we theoretically have the full posterior distribution. We have seen in our previous lessons that for simple models involving one parameter, we can often plot the posterior distribution in order to interpret it. When we have two parameters, it becomes more challenging to make plots, but it can be done. Things get more and more complicated as the number of parameters grow, a manifestation of the curse of dimensionality.\nAt the heart of the technical challenges of Bayesian inference is how to make sense of the posterior. Plotting it is useful, but in almost all cases, we wish to compute expectations of the posterior. An expectation computed from the posterior is of the form\n\\[\\begin{align}\n\\langle \\xi \\rangle = \\int \\mathrm{d}\\theta'\\, \\xi(\\theta')\\,g(\\theta'\\mid y),\n\\end{align}\\]\nwith the integral replaced by a sum for discrete \\(\\theta\\). As an example, we may have \\(\\theta = (\\theta_1, \\theta_2)\\) and we wish to compute a marginal posterior \\(g(\\theta_2 \\mid y)\\). In this case, \\(\\xi(\\theta') = \\delta(\\theta_2' - \\theta_2)\\), a Dirac delta function, and\n\\[\\begin{align}\ng(\\theta_2 \\mid y) = \\int \\mathrm{d}\\theta_1 \\, g(\\theta_1, \\theta_2 \\mid y).\n\\end{align}\\]\nSo, making sense of the posterior typically involves computing integrals (or sums).\nIn addition to using conjagacy, which we have already discussed, in coming lessons, we will explore a few ways to do this.\n\nFinding the values of the parameters \\(\\theta\\) that maximize the posterior and then approximating the posterior as locally Normal. This involves a numerical optimization calculation to find the maximizing parameters, and then uses known analytical results about multivariate Normal distributions to automatically compute the integrals (without actually having to do integration!).\nSampling out of the posterior. As you saw in an earlier homework, we can perform marginalization, and indeed most other integrals, by sampling. The trick is sampling out of the posterior, which is not as easy as the sampling you have done so far. We resort to using the sampling method called Markov chain Monte Carlo (MCMC) to do it.\nPerforming and optimization to find which distribution in a family of distributions most closely approximates the posterior, and then use this approximate distribution as a substitute for the posterior. We choose the family of candidate distributions to have nice properties, known analytical results, ease of sampling, etc., thereby making exploration of the posterior much easier than by full MCMC. This is called variational inference.\n\nFor the rest of this lecture, though, we will focus on model building.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Basics of Bayesian modeling</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/basics_of_bayes.html#bayesian-modeling-example-parameter-estimation-from-repeated-measurements",
    "href": "lessons/bayesian_modeling/basics_of_bayes.html#bayesian-modeling-example-parameter-estimation-from-repeated-measurements",
    "title": "20  Basics of Bayesian modeling",
    "section": "20.2 Bayesian modeling example: parameter estimation from repeated measurements",
    "text": "20.2 Bayesian modeling example: parameter estimation from repeated measurements\nWe will consider one of the simplest examples of parameter estimation, and one that comes up often in research applications. Let’s say we repeat a measurement many times. This could be beak depths of finches on the Galápagos, fluorescence intensity in a cell, etc. The possibilities abound. To have a concrete example in mind for this example, let’s assume we are doing a behavioral assay where a mouse has to descend a 24-inch pole and we measure the amount of time it takes for the mouse to descend. This is a common assay for measuring mouse motor coordination.\nWe define our set of pole descent times as \\(y \\equiv\\{y_1, y_2, \\ldots y_n\\}\\). We will ambiguously define a parameter of interest to be \\(\\mu\\), the typical descent time. We will sharpen our definition of this parameter through specification of the likelihood.\nWe wish to calculate \\(g(\\mu  \\mid y)\\), the posterior probability density function for the parameter \\(\\mu\\), given the data. Values of \\(\\mu\\) for which the posterior probability density is high are more probable (that is, more plausible) than those for which is it low. The posterior \\(g(\\mu \\mid y)\\) codifies our knowledge about \\(\\mu\\) in light of our data \\(y\\).\nTo compute the posterior, we use Bayes’s theorem.\n\\[\\begin{aligned}\ng(\\mu \\mid y) = \\frac{f(y\\mid \\mu)\\,g(\\mu)}{f(y)}.\n\\end{aligned}\\]\nSince the evidence, \\(f(y)\\) does not depend on the parameter of interest, \\(\\mu\\), it is really just a normalization constant, so we do not need to consider it explicitly at this stage. Specification of the likelihood and prior is sufficient for the posterior, since we must have\n\\[\\begin{aligned}\nf(y) = \\int \\mathrm{d}\\mu \\,f(y\\mid \\mu)\\,g(\\mu)\n\\end{aligned}\\]\nto ensure normalization of the posterior \\(g(\\mu \\mid y)\\). So, we have just to specify the likelihood \\(f(y\\mid \\mu)\\) and the prior \\(g(\\mu)\\). We begin with the likelihood.\n\n20.2.1 The likelihood\nTo specify the likelihood, we have to ask what we expect from the data, given a value of \\(\\mu\\). If every mouse descended in exactly the same amount of time and there are no errors or confounding factors at all in our measurements, we expect \\(y_i = \\mu\\) for all \\(i\\). In this case\n\\[\\begin{aligned}\ng(y\\mid \\mu) = \\prod_{i=1}^n\\delta(y_i - \\mu),\n\\end{aligned}\\]\nthe product of Dirac delta functions. Of course, this is really never the case. There will be natural variation in mice and in their behavior and additionally some errors in measurement and/or the system has variables that confound the measurement. What, then should we choose for our likelihood?\nThat choice is of course dependent the story/theoretical modeling behind data generation. For our purposes here, we shall assume our data are generated from a Normal likelihood. Since this distribution gets heavy use, I will pause here to talk a bit more about it, even though we have already seen it.\n\n\n20.2.2 The Normal distribution\nA univariate Normal (also known as Gaussian), probability distribution has a probability density function (PDF) of\n\\[\\begin{aligned}\nf(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\n\\exp\\left[-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right].\n\\end{aligned}\\]\nThe parameter \\(\\mu\\) is a location parameter and in the case of the Normal distribution is called the mean and \\(\\sigma\\) is a scale parameter and is called the standard deviation in this case. The square of this scale parameter is referred to as the variance. Importantly (and confusingly), the terms \"mean,\" \"standard deviation,\" and \"variance\" in this context are names of parameters of the distribution; they are not what you compute directly from data as plug-in estimates. We will therefore use the terms location parameter and scale parameter, though the terms mean and standard deviation are used very widely in the literature.\nThe central limit theorem says that any quantity that emerges from a large number of subprocesses tends to be Normally distributed, provided none of the subprocesses is very broadly distributed. We will not prove this important theorem, but we make use of it when choosing likelihood distributions based on the stories behind the generative process. Indeed, in the simple case of estimating a single parameter where many processes may contribute to noise in the measurement, the Normal distribution is a good choice for a likelihood.\nMore generally, the multivariate Normal distribution for \\(y = (y_1, y_2, \\cdots, y_n)^\\mathsf{T}\\) is\n\\[\\begin{aligned}\nf(y \\mid \\boldsymbol{\\mu}, \\mathsf{\\Sigma}) = (2\\pi)^{-\\frac{n}{2}} \\left(\\det \\mathsf{\\Sigma}\\right)^{-\\frac{1}{2}}\\,\n\\exp\\left[-\\frac{1}{2}(y - \\boldsymbol{\\mu})^\\mathsf{T}\\cdot \\mathsf{\\Sigma}^{-1}\\cdot(y - \\boldsymbol{\\mu})\\right],\n\\end{aligned}\\]\nwhere \\(\\boldsymbol{\\mu} = (\\mu_1, \\mu_2,\\ldots, \\mu_n)^\\mathsf{T}\\) is an array of location parameters. The parameter \\(\\mathsf{\\Sigma}\\) is a symmetric positive definite matrix called the covariance matrix. If off-diagonal entry \\(\\Sigma_{ij}\\) is nonzero, then \\(y_i\\) and \\(y_j\\) are correlated (or anticorrelated). In the case where all \\(y_i\\) are independent, all off-diagonal terms in the covariance matrix are zero, and the multivariate Normal distribution reduces to\n\\[\\begin{aligned}\nf(y \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}}\\,\n\\exp\\left[-\\frac{(y_i - \\mu_i)^2}{2\\sigma_i^2}\\right],\n\\end{aligned}\\]\nwhere \\(\\sigma^2_i\\) is the \\(i`th entry along the diagonal\nof the covariance matrix. This is the variance associated with\nmeasurement :math:`i\\). So, if all independent measurements \\(y_i\\) have the same location and scale parameters, which is to say that the measurements are independent and identically distributed (i.i.d.), the multi-dimensional Gaussian reduces to\n\\[\\begin{aligned}\nf(y \\mid \\mu, \\sigma) = \\left(\\frac{1}{2\\pi \\sigma^2} \\right)^{-\\frac{n}{2}}\\,\n\\exp\\left[-\\frac{1}{2\\sigma^2}\\,\\sum_{i=1}^n (y_i - \\mu)^2\\right].\n\\end{aligned}\\]\n\n\n20.2.3 The likelihood revisited: and another parameter\nFor the purposes of this demonstration of parameter estimation, we assume the Normal distribution is a good choice for our likelihood for repeated measurements (as it often is). We have to decide how the measurements are related to specify how many entries in the covariance matrix we need to specify as parameters. It is often the case that the measurements are i.i.d, so that only a single mean and variance are specified. So, we choose our likelihood to be\n\\[\\begin{aligned}\nf(y\\mid \\mu, \\sigma) = \\left(\\frac{1}{2\\pi \\sigma^2} \\right)^{\\frac{n}{2}}\\,\n\\exp\\left[-\\frac{1}{2\\sigma^2}\\,\\sum_{i=1}^n (y_i - \\mu)^2\\right].\n\\end{aligned}\\]\nBy choosing this as our likelihood, we are saying that we expect our measurements to have a well-defined mean \\(\\mu\\) with a spread described by the variance, \\(\\sigma^2\\).\nBut wait a minute; we had a single parameter, \\(\\mu\\), that we sought to estimate, and now we now have another parameter, \\(\\sigma\\), beyond the one we’re trying to measure. So, our statistical model has two parameters, and Bayes’s theorem now reads\n\\[\\begin{aligned}\ng(\\mu, \\sigma \\mid y) = \\frac{f(y\\mid \\mu, \\sigma)\\,g(\\mu, \\sigma)}\n{f(y)}.\n\\end{aligned}\\]\nAfter we compute the posterior, we can still find the posterior probability distribution we are after by marginalizing.\n\\[\\begin{aligned}\ng(\\mu\\mid y) = \\int_0^\\infty \\mathrm{d}\\sigma\\,g(\\mu, \\sigma \\mid y).\n\\end{aligned}\\]\n\n\n20.2.4 Choice of prior\nNow that we have defined a likelihood, we know what the parameters are and we can define a prior, \\(g(\\mu, \\sigma)\\). As is often the case, we assume \\(\\mu\\) and \\(\\sigma\\) are independent of each other, so that\n\\[\\begin{aligned}\ng(\\mu, \\sigma) = g(\\mu)\\,g(\\sigma).\n\\end{aligned}\\]\nHow might we choose prior distributions for \\(\\mu\\) and \\(\\sigma\\)? Remember, the prior probability distribution captures what we know about the parameter before we measure data. For the current example of mouse pole descent, we can guess that the moues should descend in about ten seconds, but we are not too sure about this. So, we will make the prior distribution broad; we take \\(g(\\mu)\\) to be Normal with a location parameter of 15 seconds, but a scale parameter of 10 seconds. That is,\n\\[\\begin{aligned}\ng(\\mu) = \\frac{1}{\\sqrt{2\\pi \\sigma_\\mu^2}}\\,\\exp\\left[-\\frac{(\\mu-\\mu_\\mu)^2}{2\\sigma^2}\\right],\n\\end{aligned}\\]\nwith \\(\\mu_\\mu = 15\\) seconds and \\(\\sigma_\\mu = 10\\) seconds. There is some trepidation with this model, as there is finite probability that the time to descend is negative, which is of course unphysical. If we wish to use a Normal prior, we will necessarily have to allow for some unallowed parameter values to have finite prior probability, as the support of the Normal distribution is over the whole number line. The advise given in the excellent wiki for advice about choosing priors from the Stan developers has this gem of advise: \"…loss in precision by making the prior a bit too weak (compared to the true population distribution of parameters or the current expert state of knowledge) is less serious than the gain in robustness by including parts of parameter space that might be relevant.\" So, we will take some water with our wine for this prior.\nFor \\(g(\\sigma)\\), we might think that the pole descent time might vary by about 50% of our guessed descent time, but not much more than that. We could again choose a Normal prior, with\n\\[\\begin{aligned}\ng(\\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma_\\sigma^2}}\\,\\exp\\left[-\\frac{(\\mu-\\mu_\\sigma)^2}{2\\sigma_\\sigma^2}\\right],\\end{aligned}\\]\nwith \\(\\mu_\\sigma = 5\\) seconds and \\(\\sigma_\\sigma = 5\\) second.\nAs I have already mentioned, we have the obvious issue that there is nonzero probability that \\(\\mu\\) or \\(\\sigma\\) could be negative, which we know is respectively unphysical or mathematically disallowed. We could refine our prior distribution to make sure this does not happen. With any approach we choose, the prior should roughly match what we would sketch on a piece of paper and cover any reasonable parameter values and exclude any that are unreasonable (or unphysical).\n\n\n20.2.5 Succinctly stating the model\nOur model is complete, which means that we have now completely specified the posterior. We can write it out. First, defining the parametrization of the priors.\n\\[\\begin{aligned}\n\\begin{align}\n&\\mu_\\mu = 15 \\test{ seconds} \\nonumber \\\\[1em]\n&\\sigma_\\mu = 10 \\test{ seconds} \\nonumber \\\\[1em]\n&\\mu_\\sigma = 5 \\test{ seconds} \\nonumber \\\\[1em]\n&\\sigma_\\sigma = 5 \\test{ seconds}.\n\\end{align}\n\\end{aligned}\\]\nThen, the posterior is\n\\[\\begin{aligned}\n\\begin{aligned}\ng(\\mu, \\sigma\\mid y) = &\\;\\frac{1}{f(y)}\\,\\left\\{\\left(\\frac{1}{2\\pi \\sigma^2} \\right)^{\\frac{n}{2}}\\,\n\\exp\\left[-\\frac{1}{2\\sigma^2}\\,\\sum_{i=1}^n (y_i - \\mu)^2\\right] \\right.\\nonumber\\\\\n&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\times \\, \\frac{1}{\\sqrt{2\\pi \\sigma_\\mu^2}}\\,\\exp\\left[-\\frac{(\\mu-\\mu_\\mu)^2}{2 \\sigma_\\mu^2}\\right] \\nonumber\\\\\n&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\times\\,\\left.\\frac{1}{\\sqrt{2\\pi \\sigma_\\sigma^2}}\\,\\exp\\left[-\\frac{(\\sigma-\\mu_\\sigma)^2}{2 \\sigma_\\sigma^2}\\right]\\right\\},\n\\end{aligned}\n\\end{aligned}\\]\nwith\n\\[\\begin{aligned}\nf(y) = \\int \\mathrm{d}\\mu\\,\\int\\mathrm{d}\\sigma\\, \\{\\text{term in braces in the above equation}\\}.\n\\end{aligned}\\]\nOh my, this is a mess, even for this simple model! Even though we have the posterior, it is very hard to make sense of it. Essentially the rest of the course involved making sense of the posterior, which is the challenge. It turns out that writing it down is relatively easy.\nOne of the first things we can do to make sense of our model, and also to specify it, is to use a shorthand for model specification. First of all, we do not need to specify the evidence, since it is always given by integrating the likelihood and prior; that is by fully marginalizing the likelihood. So, we will always omit its specification. Now, we would like to have a notation for stating the likelihood and prior. English works well.\n\n\nThe parameter \\(\\mu\\) is Normally distributed with location parameter 15 seconds and scale parameter 5 seconds.\nThe parameter \\(\\sigma\\) is Normally distributed with location parameter 5 seconds and scale parameter 5 seconds.\nThe descent times are i.i.d. and are Normally distributed with location parameter \\(\\mu\\) and scale parameter \\(\\sigma\\).\n\n\nThis is much easier to understand. We can write this with a convenient, and self evident, shorthand.1\n\\[\\begin{aligned}\n\\begin{aligned}\n&\\mu \\sim \\text{Norm}(\\mu_\\mu, \\sigma_\\mu),\\\\[1em]\n&\\sigma \\sim \\text{Norm}(\\mu_\\sigma, \\sigma_\\sigma),\\\\[1em]\n&y_i \\sim \\text{Norm}(\\mu, \\sigma) \\;\\forall i.\n\\end{aligned}\n\\end{aligned}\\]\nHere, the symbol \\(\\sim\\) may be read as “is distributed as.” The above three lines are completely sufficient to specify our model. Because we will be using a probabilistic programming language in practice, we will almost never need to code up any nasty mathematical expressions in our modeling and we can express a model as we have done here.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Basics of Bayesian modeling</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/basics_of_bayes.html#choosing-likelihoods",
    "href": "lessons/bayesian_modeling/basics_of_bayes.html#choosing-likelihoods",
    "title": "20  Basics of Bayesian modeling",
    "section": "20.3 Choosing likelihoods",
    "text": "20.3 Choosing likelihoods\nIn our example of model building for measurements of mouse descent times, we chose a Normal likelihood for the egg lengths. We did so because the story of the repeated measurements matched that of the Normal distribution via the central limit theorem. When a measurement is the result of many processes, none of which has an enormous variance, the values of the measurement is Normally distributed.\nThis method of choosing a likelihood amounts to story matching. The idea is that we describe the data generation process with a story. We then find a distribution that describes the outcome of the story.\nFor example, one might do an electrophysiology experiment measuring spiking of an individual neuron in response to a constant stimulus and record the time between binding events. A possible model story for this is that the spikes are all independent and without memory; that is their timing does not depend on previous spiking events. This means that we can model spiking as a Poisson process and are interested in the timing between arrivals (spikes) of the Poisson process. This story matches the story of the Exponential distribution, so we would use it for our likelihood.\nThe procedure of story matching is an important part of Bayesian modeling. In a great many cases, there exists a well-known, named distribution that matches the story you are using to model the generation of your data. In cases where no such distribution exists, or you do not know about it, you need to derive a PDF or PMF matching your story, which can be challenging. It is therefore well worth the time investment to know about distributions that can be useful in modeling. You should read the contents of the Distribution Explorer to get yourself familiar with named distributions. This will greatly facilitate choosing likelihoods (and priors).",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Basics of Bayesian modeling</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/basics_of_bayes.html#choosing-a-prior",
    "href": "lessons/bayesian_modeling/basics_of_bayes.html#choosing-a-prior",
    "title": "20  Basics of Bayesian modeling",
    "section": "20.4 Choosing a prior",
    "text": "20.4 Choosing a prior\nChoice of likelihood is not trivial. It often involves physical/biological modeling and substantial domain knowledge. However, compared to the choosing prior distributions, there is typically less subtlety outside of the domain expertise in choosing likelihoods. We therefore reserve choice of priors, and the important related concept of conjugacy, to the next separate lessons.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Basics of Bayesian modeling</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/basics_of_bayes.html#footnotes",
    "href": "lessons/bayesian_modeling/basics_of_bayes.html#footnotes",
    "title": "20  Basics of Bayesian modeling",
    "section": "",
    "text": "I understand that I should be providing units on all parameters that I am specifying with numbers. I am not doing this here, nor throughout the course, to avoid notational clutter and to maintain focus on the modeling.↩︎",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Basics of Bayesian modeling</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/conjugacy.html",
    "href": "lessons/bayesian_modeling/conjugacy.html",
    "title": "21  Conjugacy",
    "section": "",
    "text": "21.1 The Beta-Binomial conjugate pair\n| Download notebook\nWe have talked about Bayes theorem as a model for learning. The idea there was that we know something before (a priori) acquiring data, and then we update our knowledge after (a posteriori). So, we come in with the prior and out with the posterior after acquiring data. It might make sense, then, that the prior and the posterior distributions have the same functional form. That is, the prior and the posterior have the same distribution, and the parameters of the distribution are updated from the prior to the posterior by the likelihood. When the prior and posterior have the same functional form, the prior is said to be conjugate to the likelihood. This seems pleasing: the likelihood serves to update the prior into the posterior, so it should determine the functional form of the prior/posterior such that they are the same.\nConjugate priors are especially useful because they enable analytical calculation of the posterior, typically without have to do any mathematics yourself. This is perhaps best seen through example.\nAs a motivating example of use of a conjugate prior, we will use data from one of the experiments from a paper by Mossman, et al., 2019, in which the authors investigated the age of Drosophila parents on the viability of the eggs of their offspring. In one vial, they mated young (≤ 3 days old) males with old females (≥ 45 days old). Of the 176 eggs laid by their offspring over a 10-day period, 94 of them hatched, while the remainder failed to hatch. In another vial, they mated young males with young females. Of the 190 eggs laid by their offspring over a 10-day period, 154 hatched. (They took many more data than these, but we’re using this experiment as a demonstration of conjugacy.)\nThe story behind the data matches that of the Binomial distribution. Each egg can by thought of as a Bernoulli trial, with success being hatching and failure being failure to hatch. The number \\(n\\) of hatches out of \\(N\\) eggs is Binomially distributed.\n\\[\\begin{align}\nn \\mid N, \\theta \\sim \\text{Binom}(N, \\theta).\n\\end{align}\\]\nWriting out the PMF, this is\n\\[\\begin{align}\nf(n\\mid N, \\theta) = \\begin{pmatrix}N\\\\n\\end{pmatrix} \\theta^n(1-\\theta)^{N-n}.\n\\end{align}\\]\nThis identifies a single parameter to estimate, \\(\\theta\\), which is the probability of hatching. Our goal, then, is to compute \\(g(\\theta\\mid n,N)\\).",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Conjugacy</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/conjugacy.html#the-beta-binomial-conjugate-pair",
    "href": "lessons/bayesian_modeling/conjugacy.html#the-beta-binomial-conjugate-pair",
    "title": "21  Conjugacy",
    "section": "",
    "text": "21.1.1 Finding the conjugate\nWe seek a prior that is conjugate to the likelihood. That is, we want to choose \\(g(\\theta)\\) such that \\(g(\\theta \\mid n, N)\\) and \\(g(\\theta)\\) have the same functional form. Intuitively, we might try a prior that has a similar functional form as the likelihood. The Beta distribution is one such distribution. Its PDF is\n\\[\\begin{align}\ng(\\theta \\mid \\alpha, \\beta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)},\n\\end{align}\\]\nwhere \\(B(\\alpha,\\beta)\\) is a beta function,\n\\[\\begin{align}\nB(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\n\\end{align}\\]\nNote that there are two parameters, \\(\\alpha\\) and \\(\\beta\\) that parametrize the prior. If in fact the Beta distribution is conjugate to the Binomial, it is these two parameters that get updated by the data.\nYou should explore the Beta distribution in the Distribution Explorer to get a feel for its shape and how the parameters \\(\\alpha\\) and \\(\\beta\\) affect its shape. Importantly, if \\(\\alpha = \\beta = 1\\), we get a Uniform distribution on the interval [0, 1].\nWe can now check to see if the Beta distribution is conjugate to the Binomial. If we insert a Beta distribution for the prior, we have\n\\[\\begin{align}\ng(\\theta \\mid n, N, \\alpha, \\beta) &= \\frac{f(n\\mid N, \\theta)\\, g(\\theta\\mid \\alpha, \\beta)}{f(n \\mid N)} \\\\[1em]\n&= \\frac{1}{f(n\\mid N)}\\,\\begin{pmatrix}N\\\\n\\end{pmatrix}\\,\\theta^n(1-\\theta)^{N-n}\\,\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)} \\\\[1em]\n&= \\frac{1}{f(n\\mid N)\\,B(\\alpha,\\beta)}\\,\\begin{pmatrix}N\\\\n\\end{pmatrix}\\,\\theta^{n+\\alpha-1}(1-\\theta)^{N-n+\\beta-1}.\n\\end{align}\\]\nIn looking at this expression, the only bit that depends on \\(\\theta\\) is \\(\\theta^{n+\\alpha-1}(1-\\theta)^{N-n+\\beta-1}\\), which is exactly the \\(\\theta\\)-dependence of a Beta distribution with parameters \\(n+\\alpha\\) and \\(N-n+\\beta\\). Thus, the posterior must also be a Beta distribution. Therefore,\n\\[\\begin{align}\n\\frac{1}{f(n\\mid N)\\,B(\\alpha,\\beta)}\\,\\begin{pmatrix}N\\\\n\\end{pmatrix} = \\frac{1}{B(n+\\alpha, N-n+\\beta)}.\n\\end{align}\\]\nWe have just normalized the posterior without doing any nasty integrals! So, the posterior is\n\\[\\begin{align}\ng(\\theta \\mid n, N, \\alpha, \\beta) = \\frac{\\theta^{n+\\alpha-1}(1-\\theta)^{N-n+\\beta-1}}{B(n+\\alpha, N-n+\\beta)}.\n\\end{align}\\]\nThus, we have prior and posterior distributions\n\\[\\begin{align}\n&\\theta \\sim \\text{Beta}(\\alpha, \\beta),\\\\[1em]\n&\\theta \\mid n, N \\sim \\text{Beta}(n+\\alpha, N-n+\\beta).\n\\end{align}\\]\nSo, we can see that conjugacy is useful. For a given likelihood, if we know its conjugate prior, we can just immediately write down the posterior in a clear form. The Wikipedia page on conjugate priors has a useful table of likelihood-conjugate pairs.\n\n\n21.1.2 Plots of the posteriors\nWe can now rapidly exactly plot the posteriors for the egg hatch probability for old mothers and young mothers.\n\n# Observed data\nn_old = 94\nN_old = 176\nn_young = 154\nN_young = 190\n\n# Assume uniform prior\nalpha = 1\nbeta = 1\n\n# Instantiate the plot\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=200,\n    x_axis_label=\"θ\",\n    y_axis_label=\"g(θ | n, N)\",\n    x_range=[0, 1],\n)\n\n# Theoretical theta values\ntheta = np.linspace(0, 1, 400)\n\n# Old posterior\np.line(\n    theta,\n    st.beta.pdf(theta, n_old + alpha, N_old - n_old + beta),\n    line_width=2,\n    legend_label=\"old mothers\",\n)\n\n# Young posterior\np.line(\n    theta,\n    st.beta.pdf(theta, n_young + alpha, N_young - n_young + beta),\n    line_width=2,\n    color=\"orange\",\n    legend_label=\"young mothers\",\n)\n\np.legend.location = \"top_left\"\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn plotting the posteriors, we see that the young mothers clearly are more likely to have their offsprings’ eggs hatch than the old mothers.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Conjugacy</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/conjugacy.html#comments-on-conjugates",
    "href": "lessons/bayesian_modeling/conjugacy.html#comments-on-conjugates",
    "title": "21  Conjugacy",
    "section": "21.2 Comments on conjugates",
    "text": "21.2 Comments on conjugates\nUse of conjugate priors have advantages and disadvantages. First, the advantages.\n\nThey are conceptually pleasing in that the prior and posterior have the same form, so the update of the prior by the data via the likelihood simply updates the parameters of the distribution describing the parameter values. (See, however, the criticisms of this approach below.)\nThey make evaluation of the posterior analytically tractable, and in fact easy. For example, you now know how to immediately write down the posterior after an experiment consisting of a series of Bernoulli trials (a Binomial likelihood) using a Beta distribution for the prior and posterior. We will also see this tractability in models we consider later, including principal component analysis, hidden Markov models, and Gaussian processes.\n\nThere are disadvantages and criticisms, however.\n\nFinding conjugates for a given likelihood is typically very challenging for all but simple distributions. The table on Wikipedia is pretty complete for known useful conjugates and it contains a paltry few distributions. For complex models, it is hopeless to find conjugates, thereby restricting their utility.\nOur prior information about a parameter might not actually match the form of a conjugate prior. Of example, if someone tells you a coin is slightly biased, but not in which direction, you might choose a prior that is bimodal. This cannot be described with a Beta distribution. Sivia comments on this in his book: “While we might expect our initial understanding of the object of interest to have a bearing on the experiment we conduct, it seems strange that the choice of the prior pdf should have to wait for, and depend in detail upon, the likelihood function.”",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Conjugacy</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/conjugacy.html#computing-environment",
    "href": "lessons/bayesian_modeling/conjugacy.html#computing-environment",
    "title": "21  Conjugacy",
    "section": "21.3 Computing environment",
    "text": "21.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nbokeh     : 3.6.2\njupyterlab: 4.3.7",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Conjugacy</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html",
    "href": "lessons/bayesian_modeling/choice_of_prior.html",
    "title": "22  Choosing priors",
    "section": "",
    "text": "22.1 Uniform priors\nWhile choosing likelihoods often amounts to story matching, choosing priors can be more subtle and challenging. In our example of model building for mouse pole descent times, we assumed Normal priors for the two parameters \\(\\mu\\) and \\(\\sigma\\). We did that because we felt that it best codified in probabilistic terms our knowledge of those parameters before seeing the data. That is one of many ways we can go about choosing priors. In fact, choice of prior is a major topic of (often heated) discussion about how best to go about Bayesian modeling. Some believe that the fact you have to specify a prior in the Bayesian framework invalidates Bayesian inference entirely because it necessarily introduces a modeler's bias into the analysis.\nAmong the many approaches to choose priors are choices of uniform priors, Jeffreys priors, weakly informative priors, conjugate priors, maximum entropy priors, Bernardo's reference priors, and others. We will discuss the first four of these, eventually advocating for weakly informative priors.\nThe principle of insufficient reason is an old rule for assigning probability to events or outcomes. It simply says that if we do not know anything about a set of outcomes, then every possible outcome should be assigned equal probability. Thus, we assume that the prior is flat, or uniform.\nThis notion is quite widely used. In fact, if we attempt to summarize the posterior by a single point in parameter space, namely where the posterior is maximal, and we chose uniform priors for all parameters, we get estimates for the parameters that are the same as if we performed a maximum likelihood estimate in a frequentist approach.\nHowever, using uniform priors has a great many problems. I discuss but a few of them here.\nIn summary, uniform priors, while widely used, and in fact used in the early homeworks of this course, are a pathologically bad idea. (Note, though, that this is still subject to debate, and many respected researchers do not agree with this assessment.)",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#uniform-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#uniform-priors",
    "title": "22  Choosing priors",
    "section": "",
    "text": "If a parameter may take any value along the number line, or any positive value, then a uniform prior is not normalizable. This is because \\(\\int_0^\\infty \\mathrm{d}\\theta\\,(\\text{constant})\\) diverges. Such a prior is said to be an improper prior, since it is not a true probability distribution (nor probability in the discrete case). This means that they cannot actually describe prior knowledge of a parameter value as encoded by the machinery of probability.\nWe can remedy point (1) by specifying bounds on the prior. This is no longer a uniform prior, though, since we are saying that parameter values between the bounds are infinitely more likely than those outside of the bounds. At a small distance \\(\\epsilon\\) from an upper bound, for example, we have \\(\\theta - \\epsilon\\) being infinitely more likely than \\(\\theta + \\epsilon\\), which does not make intuitive sense.\nSurely priors cannot be uniform. For example, if we were trying to measure the speed of a kinesin motor, we know that it does not go faster than the speed of light (because nothing goes faster than the speed of light). With an improper uniform prior, we are saying that before we see and experiment, we believe that kinesin is more likely to go faster than the speed of light than it is to move at a micron per second. This is absurd. We will deal with this issue when discussing weakly informative priors.\nA primary criticism from Fisher and his contemporaries was that the way you choose to parametrize a model can affect how a uniform prior transforms. We illustrate this problem and its resolution when we talk about Jeffreys priors next.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#jeffreys-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#jeffreys-priors",
    "title": "22  Choosing priors",
    "section": "22.2 Jeffreys priors",
    "text": "22.2 Jeffreys priors\nFisher and others complained that application of the principle of insufficient reason to choose uniform priors resulted in different effects based on parametrization of a model. To make this more concrete consider the example of a one-dimensional Normal likelihood. The probability density function is\n\\[\\begin{align}\nf(y\\mid\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\mathrm{e}^{-(y-\\mu)^2/2\\sigma^2}.\n\\end{align}\\]\nInstead of parametrizing by \\(\\sigma\\), we could have instead chosen to parametrize with \\(\\tau \\equiv 1/\\sigma\\), giving a PDF of\n\\[\\begin{align}\nf(y\\mid\\mu,\\tau) = \\frac{\\tau}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-\\tau^2(y-\\mu)^2/2}.\n\\end{align}\\]\nNow, if we choose a uniform prior for \\(\\sigma\\), we should also expect a uniform prior for \\(\\tau\\). But this is not the case. Recall the change of of variables formula.\n\\[\\begin{align}\ng(\\tau) = \\left|\\frac{\\mathrm{d}\\sigma}{\\mathrm{d}\\tau}\\right|g(\\sigma) = \\frac{\\text{constant}}{\\tau^2},\n\\end{align}\\]\nsince \\(g(\\sigma) = \\text{constant}\\) for a uniform prior and \\(|\\mathrm{d}\\sigma/\\mathrm{d}\\tau| = 1/\\tau^2\\). So, if we parametrize the likelihood with \\(\\tau\\) instead of \\(\\sigma\\), the priors are inconsistent. That is, the prior distribution is not invariant to change of variables.\nIf, however, we chose an improper prior of \\(g(\\sigma) = 1/\\sigma = \\tau\\), then we end up with \\(g(\\tau) = 1/\\tau\\), so the priors are consistent. It does not matter which parametrization we choose, \\(\\sigma\\) or \\(\\tau = 1/\\sigma\\), so long as the prior is \\(1/\\sigma\\) or \\(1/\\tau\\), we get the same effect of the prior.\nHarold Jeffreys noticed this and discovered a way to make the priors invariant to change of coordinates. He developed what is now known as the Jeffreys prior, which is given by the square root of the determinant of the Fisher information matrix. If \\(f(y\\mid\\theta)\\) is the likelihood (where \\(\\theta\\) here is a set of parameters), the Fisher information matrix is the negative expectation value of the matrix of second derivatives of the log-likelihood. That is, entry \\(i, j\\) in the Fisher information matrix \\(\\mathcal{I}\\) is\n\\[\\begin{align}\n\\mathcal{I}_{ij}(\\theta) = -\\int\\mathrm{d}y \\,\\frac{\\partial^2 f(y\\mid \\theta)}{\\partial \\theta_i\\partial \\theta_j} \\, f(y\\mid \\theta) \\equiv -\\mathrm{E}\\left[\\frac{\\partial^2 \\ln f(y\\mid \\theta)}{\\partial \\theta_i\\partial \\theta_j}\\right],\n\\end{align}\\]\nwhere \\(\\mathrm{E}[\\cdot]\\) denotes the expectation value over the likelihood. For ease of calculation later, it is useful to know that this is equivalent to\n\\[\\begin{align}\n\\mathcal{I}_{ij}(\\theta) = \\mathrm{E}\\left[\\left(\\frac{\\partial \\ln f(y\\mid \\theta)}{\\partial \\theta_i}\\right)\\left(\\frac{\\partial \\ln f(y\\mid \\theta)}{\\partial \\theta_j}\\right)\\right].\n\\end{align}\\]\nWritten more succinctly, let \\(\\mathsf{B}_\\theta\\) be the Hessian matrix, that is the matrix of partial derivatives of the the log likelihood.\n\\[\\begin{aligned}\n\\begin{align}\n\\mathsf{B}_\\theta = \\begin{pmatrix}\n\\frac{\\partial^2 \\ln f}{\\partial \\theta_1^2} &  \\frac{\\partial^2 \\ln f}{\\partial \\theta_1 \\partial \\theta_2} & \\cdots \\\\\n\\frac{\\partial^2 \\ln f}{\\partial \\theta_2 \\partial \\theta_1} &  \\frac{\\partial^2 \\ln f}{\\partial \\theta_2^2} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\end{align}.\n\\end{aligned}\\]\nThen,\n\\[\\begin{align}\n\\mathcal{I}(\\theta) = -\\mathrm{E}\\left[\\mathsf{B}_\\theta\\right],\n\\end{align}\\]\nDue to its relation to the second derivatives of the likelihood function, the Fisher information matrix is related to the sharpness of a peak in the likelihood.\nThe Jeffreys prior is then\n\\[\\begin{align}\ng(\\theta) \\propto \\sqrt{\\mathrm{det}\\, \\mathcal{I}(\\theta)}.\n\\end{align}\\]\nIt can be shown that the determinant of the Fisher information matrix is strictly nonnegative, so that \\(g(\\theta)\\) as defined above is always real valued. To demonstrate that this choice of prior works to maintain the same functional form of priors under reparametrization, consider a reparametrization from \\(\\theta\\) to \\(\\phi\\). By the multivariate change of variables formula,\n\\[\\begin{align}\ng(\\phi) \\propto \\left|\\mathrm{det}\\,\\mathsf{J}\\right|g(\\theta),\n\\end{align}\\]\nwhere\n\\[\\begin{aligned}\n\\begin{align}\n\\mathsf{J} = \\begin{pmatrix}\n\\frac{\\partial \\theta_1}{\\partial \\phi_1} &  \\frac{\\partial \\theta_1}{\\partial \\phi_2} & \\cdots \\\\\n\\frac{\\partial \\theta_2}{\\partial \\phi_1} &  \\frac{\\partial \\theta_2}{\\partial \\phi_2} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\end{align}\n\\end{aligned}\\]\nis a matrix of derivatives, called the Jacobi matrix. Using the fact that \\(g(\\theta) \\propto \\sqrt{\\mathrm{det}\\,\\mathcal{I}(\\theta)}\\) for a Jeffreys prior, we have\n\\[\\begin{align}\ng(\\phi) \\propto \\left|\\mathrm{det}\\,\\mathsf{J}\\right|\\,\\sqrt{\\mathrm{det}\\,\\mathcal{I}(\\theta)}\n= \\sqrt{\\left(\\mathrm{det}\\,\\mathsf{J}\\right)^2\\,\\mathrm{det}\\,\\mathcal{I}(\\theta)}.\n\\end{align}\\]\nBecause the product of determinants of a set of matrices is equal to the determinant of the product of the matrices, we can write this as\n\\[\\begin{align}\ng(\\phi) \\propto \\sqrt{\\mathrm{det}\\left(\\mathsf{J}\\cdot \\mathcal{I}(\\theta)\\cdot \\mathsf{J}\\right)} = \\sqrt{\\mathrm{det}\\left(\\mathsf{J}\\cdot \\mathrm{E}[\\mathsf{B}_\\theta] \\cdot \\mathsf{J}\\right)}.\n\\end{align}\\]\nBecause \\(\\theta\\) and \\(\\phi\\) are not functions of \\(y\\), and therefore \\(\\mathsf{J}\\) is also not a function of \\(y\\) we may bring the Jacobi matrices into the expectation operation.\n\\[\\begin{align}\ng(\\phi) \\propto \\sqrt{\\mathrm{det}\\,\\mathrm{E}\\left[\\mathsf{J}\\cdot \\mathsf{B}_\\theta \\cdot \\mathsf{J}\\right]}.\n\\end{align}\\]\nWe recognize the quantity \\(\\mathsf{J}\\cdot \\mathsf{B}_\\theta \\cdot \\mathsf{J}\\) as having the same form as the multivariable chain rule for second derivatives. Thus, we are converting \\(\\mathsf{B}_\\theta\\) from being a matrix of second derivatives with respect to \\(\\theta\\) to being a matrix of second derivatives with respect to \\(\\phi\\). Thus,\n\\[\\begin{align}\ng(\\phi) \\propto \\sqrt{\\mathrm{det}\\,\\mathrm{E}\\left[\\mathsf{B}_\\phi\\right]} = \\sqrt{\\mathrm{det}\\,\\mathcal{I}(\\phi)},\n\\end{align}\\]\nthereby demonstrating that a Jeffreys prior is invariant to change of parametrizations.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#example-jeffreys-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#example-jeffreys-priors",
    "title": "22  Choosing priors",
    "section": "22.3 Example Jeffreys priors",
    "text": "22.3 Example Jeffreys priors\nComputing a Jeffreys prior can be difficult. It involves computing derivatives of the likelihood and then computing expectations by performing integrals. As models become more complicated, analytical results for Jeffreys priors become intractable, which is one of the arguments against using them. Nonetheless, for two common likelihoods, we can compute the Jeffreys priors. We will not show the calculations (they involve the tedious calculations I just mentioned), but will state the results.\n\nFor a Normal likelihood, the Jeffreys prior is \\(g(\\sigma) \\propto 1/\\sigma\\). That means that the priors for parameters \\(\\mu\\) and \\(\\sigma\\) are independent and that parameter \\(\\mu\\) should have a uniform prior and that \\(\\sigma\\) has a prior that goes like the inverse of \\(\\sigma\\). This is an example of a Jeffreys prior that is improper.\nFor a Binomial or Bernoulli likelihood, the Jeffreys prior for the parameter \\(\\theta\\), which is the probability of success of a Bernoulli trial, is \\(g(\\theta) = 1/\\pi\\sqrt{\\theta(1-\\theta)}\\), defined on the interval [0, 1]. This is a proper prior. Note that it is highly peaked at zero and at one. This suggests that the probability of success for a Bernoulli trial, a priori, is most likely very close to zero or one.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#why-not-use-jeffreys-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#why-not-use-jeffreys-priors",
    "title": "22  Choosing priors",
    "section": "22.4 Why not use Jeffreys priors?",
    "text": "22.4 Why not use Jeffreys priors?\nJeffreys priors are pleasing in that they deal with Fisher's criticisms. They guarantee that we get the same results, regardless of choice of parametrization of the likelihood. They are also not very informative, meaning that the prior has little influence over the posterior, leaving almost all of the influence to the likelihood. This is also pleasing because it gives a sense of a lack of bias. However, there are still several reasons why not to use Jeffreys priors.\n\nThey can be very difficult or impossible to derive for more complicated models.\nThey can be improper. When they are improper, the prior is not encoding prior knowledge using probability, since an improper prior cannot be a probability or probability density.\nIn the case of hierarchical models, which we will get to later in the term, use of Jeffreys priors can nefariously lead to improper posteriors! It is often difficult to discover that this is the case for a particular model without doing a very careful analysis.\nThey still do not really encode prior knowledge anyway. We still have the problem of a kinesin motor traveling at faster than the speed of light.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#weakly-informative-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#weakly-informative-priors",
    "title": "22  Choosing priors",
    "section": "22.5 Weakly informative priors",
    "text": "22.5 Weakly informative priors\nRemember, the prior probability distribution captures what we know about the parameter before we measure data. When coming up with a prior, I often like to sketch how I think the probability density or mass function of a parameter will look. This is directly encoding my prior knowledge using probability, which is what a prior is supposed to do by definition. When sketching the probability density function, though, I make sure that I draw the distribution broad enough that it covers all parameter values that are even somewhat reasonable. I limit its breadth to rule out absurd values, such as kinesin traveling faster than the speed of light. Such a prior is called a weakly informative prior.\nTo come up with the functional form, or better yet the name, of the prior distribution, I use the Distribution Explorer to find a distribution and parameter set that matches my sketch. If I have to choose between making the prior more peaked or broader, I opt for being broader. This is well-described in the useful Stan wiki on priors, which says, “the loss in precision by making the prior a bit too weak (compared to the true population distribution of parameters or the current expert state of knowledge) is less serious than the gain in robustness by including parts of parameter space that might be relevant.”\nI generally prefer to use weakly informative priors, mostly because they actually encode prior knowledge, separating the sublime from the ridiculous. As we will see when we perform inference using MCMC, there are also practical advantages to using weakly informative priors. In general, prior choice can affect the second main task of Bayesian inference: making sense of the posterior. We will discuss these practical considerations when we start summarizing posteriors using Markov chain Monte Carlo.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#conjugate-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#conjugate-priors",
    "title": "22  Choosing priors",
    "section": "22.6 Conjugate priors",
    "text": "22.6 Conjugate priors\nWe have discussed conjugacy in Chapter 21. Conjugate priors are useful because we can make sense of the posterior analytically; the posterior and prior are the same distribution, differing by the updated parametrization in the posterior. If it is convenient to use a conjugate prior to encode prior information as we have described in our discussion of weakly informative priors, you can do so. There are two difficulties that make this convenience rare in practice.\n\nOnly a few likelihoods have known conjugate priors. Even in cases where the conjugate is known, its probability density function can be a complicated function.\nAs soon as a model grows in complexity beyond one or two parameters, and certainly into hierarchy, conjugate priors are simply not available.\n\nThus, conjugate priors, while conceptually pleasing and parametrizable into weakly informative priors, have limited practical use, at least if we wish to be unrestricted in our model choice. As we have and will see repeatedly, sometimes restricting ourselves to models that are more tractable can be worth the sacrifice of model \"percection.\"",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/choice_of_prior.html#the-bet-the-farm-method-of-specifying-weakly-informative-priors",
    "href": "lessons/bayesian_modeling/choice_of_prior.html#the-bet-the-farm-method-of-specifying-weakly-informative-priors",
    "title": "22  Choosing priors",
    "section": "22.7 The bet-the-farm method of specifying weakly informative priors",
    "text": "22.7 The bet-the-farm method of specifying weakly informative priors\nSince we have decided that weakly informative priors are of greatest utility, I will share a technique I like to use for coming up with priors for positive continuous unbounded parameters, a commonly encountered situation. I like to take what I call the bet-the-farm approach. The idiomatic term \"to bet the farm\" means to make a giant wager on an outcome. You could think instead about betting a year's salary. As an example, would you bet the farm that pigs cannot fly? I would be comfortable waging a year's salary (since I don't have a farm) that they do not. While I certainly hope the Los Angeles Football Club wins the MLS cup in the next few years, I would not be comfortable betting the farm on it.\nThe technique is best described through example. Let's say someone tells me about a new bacterium and I have to guess how long a single cell of that species is.\nTo make my guess, I start absurdly low. Certainly, the cell is bigger than of order nanometer, since that's the diameter of a strand of DNA. I would bet the farm (or a year's salary) on it. I would also bet the farm that it would be bigger than 10 nm without flinching. How about 100 nm? Well, I'm pretty sure that bacteria tend not to be smaller than 100 nm, but I don't think I'd bet the farm. I feel uneasy enough about that that I won't make that bet. So, I put 100 nm as the lower end of my guess.\nNow, let's consider absurdly large sizes. I would bet the farm that it is less than a meter long. How about 10 cm? That's still gigantic, and I would bet the farm that it's smaller than that. How about 1 cm? Still gigantic. How about 1 mm? Well, this is still huge, but there is tremendous diversity among bacteria. I know there are eukaryotic cells this big (for example a Xenopus egg), so, even though I strongly suspect that bacterium would be smaller than 1 mm, I wouldn't bet a farm. So, 1 mm is my upper bound.\nIf we were coming up with an order-of-magnitude estimate, we would take the geometric mean of the high and low boundaries. In this case, we would get \\(\\sqrt{10^{-7}\\cdot 10^{-3}} \\text{ m} = 10^{-5}\\text{ m} =\\) 10 µm, which, perhaps not surprisingly, is within an order of magnitude of \"typical\" bacterial size, for example of E. coli.\nNotice that these order-of-magnitude type of estimates operates on a logarithmic scale. We estimated between \\(10^{-7}\\) and \\(10^{-3}\\) meters. So, for encoding a prior for the parameter, it is convenient to come up with the prior for the base-ten logarithm of the parameter instead (ignoring the mathematical absurdity with taking logarithms of quantities with units), and then transform the variable. In the bacterial size example, I could use a Normal distribution where 95% of the probability mass lies between \\(-7\\) and \\(-3\\). The width of my range of reasonable values from the bet-the-farm approach is 4 log units, so if I choose a Normal distribution centered at \\(-5\\) with scale parameter of 1, I capture this prior information. So, my prior for the bacterial length \\(\\ell\\) is\n\\[\\begin{aligned}\n\\begin{align}\n&\\log_{10} \\ell \\sim \\text{Norm}(-5, 1),\\\\[1em]\n&\\ell = 10^{\\log_{10}\\ell}.\n\\end{align}\n\\end{aligned}\\]\nEquivalently, since \\(\\ln 10 \\approx 2.3\\), we can write this as \\(\\ell \\sim \\text{LogNorm}(-2.3\\cdot 5, 2.3)\\).\nTo summarize the procedure for finding a prior for positive continuous unbounded parameter \\(\\theta\\) is as follows.\n\nStart at absurdly low values for the parameter and work your way up to a value that you would be hesitant to bet the farm on. This is your low estimate, \\(\\theta_\\mathrm{min}\\).\nStart at absurdly high values for the parameter and work your way down to a value that you would be hesitant to bet the farm on. This is your high estimate, \\(\\theta_\\mathrm{max}\\).\nDetermine the center of your two estimates on a logarithmic scale. This is the location parameter \\(\\mu_{10} = (\\theta_\\mathrm{max} + \\theta_\\mathrm{min})/2\\) for the Normal prior of \\(\\log_{10}\\theta\\).\nTake the difference of the high to low estimates and divide it by four. The result is the scale parameter \\(\\sigma_{10} = (\\theta_\\mathrm{max} - \\theta_\\mathrm{min})/4\\) for the Normal prior of \\(\\log_{10}\\theta\\).\nThe prior for the base-ten logarithm of the parameter is then \\(\\log_{10}\\theta \\sim \\text{Norm}(\\mu_{10}, \\sigma_{10})\\). Equivalently, we can say that the prior is distributed as \\(\\theta \\sim \\text{LogNorm}(2.3\\mu_{10}, 2.3\\sigma_{10})\\).\n\nThe result is a broad distribution that contains all conceivable values of the parameters, as determined by you.\nEarlier in this lesson, I came up with a Normal (not Log-Normal as I would get using the bet-the-farm approach) for the length of a C. elegans egg. This is because I had firm prior knowledge about C. elegans eggs; I have looked at countless of them, and they are about 50 µm long. For most cases where I do not have prior knowledge like that, I use the bet the farm approach.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Choosing priors</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/stats_ml_ai.html",
    "href": "lessons/bayesian_modeling/stats_ml_ai.html",
    "title": "23  What about machine learning and artificial intelligence?",
    "section": "",
    "text": "23.1 What is data science?\nThis course is about statistical inference, and, in the service of handling data sets for inference procedures, data science. But these days machine learning and artificial intelligence are at the forefront of any discussion of working with data. So, you may ask, “How is all of this statistical inference we are doing related to ML and AI?”\nIt is a fair question. The answer requires us to clarify what we mean by data science, statistical inference, machine learning, and artificial intelligence. As far as I can tell, there are no clear definitions of these terms, so I will give working definitions for our purposes to clarify what we are trying to do in this workshop.\nIn the previous section, we have just given a description of statistical inference, defining how it fits in the process of doing science, so we will proceed to define the other terms.\nWe will define data science as a giant catch-all for anything involving data, including but not limited to:\nThe possible exception is data acquisition, which falls under experimental science.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>What about machine learning and artificial intelligence?</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/stats_ml_ai.html#sec-what-is-data-science",
    "href": "lessons/bayesian_modeling/stats_ml_ai.html#sec-what-is-data-science",
    "title": "23  What about machine learning and artificial intelligence?",
    "section": "",
    "text": "Data management\nData storage\nData organization\nData visualization\nStatistical inference\nMachine learning\nArtificial intelligence",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>What about machine learning and artificial intelligence?</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/stats_ml_ai.html#sec-what-is-ml",
    "href": "lessons/bayesian_modeling/stats_ml_ai.html#sec-what-is-ml",
    "title": "23  What about machine learning and artificial intelligence?",
    "section": "23.2 What is machine learning?",
    "text": "23.2 What is machine learning?\nI like the simple definition put forward by Sabera Talukder in the first edition of Caltech’s DataSAI workshop, which I paraphrase as\n\nMachine learning involves using data to inform machines how to perform tasks.\n\nIt is important to understand the italicized terms in the above. First data can be observations (quantitative, qualitative, or categorical), synthetic or measured. Some common tasks involve labeling data, categorizing (clustering) data, and making predictions. Informing a machine how to do tasks typically involves proposing a model for how data are generated and then finding good parameters for that model.\nThis definition of machine learning is a concise form of that put forth by Tom Mitchell in his classic book entitled Machine Learning.\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n\n“Experience” here is what we are calling data, and Mitchell’s word “learn” is what we have described as informing a machine. The added notion here is the performance measure which is a way of assessing how well a machine accomplishes a task. Mitchell says that the E, T, and P all need to be clearly defined to specify a machine learning problem.\nAs we will see, both invention of a generative model and the learning by (a.k.a. informing of) the machine are essentially problems in statistical inference. Furthermore, performance measures are often also defined in the statistical inference procedures.\nIn my view, then, machine learning is really statistical inference with a specific task in mind. As we have seen in our view of the cycle of science, given a model describing the data generation process, statistical inference provides us with a plausible set of parameter values as well as a measure of the plausibility of the model itself. We cross into machine learning when we use the parameters we have acquired to perform a task on new data.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>What about machine learning and artificial intelligence?</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/stats_ml_ai.html#sec-what-is-ai",
    "href": "lessons/bayesian_modeling/stats_ml_ai.html#sec-what-is-ai",
    "title": "23  What about machine learning and artificial intelligence?",
    "section": "23.3 What is artificial intelligence?",
    "text": "23.3 What is artificial intelligence?\nI view artificial intelligence as a subset of machine learning in which the tasks are those that have been historically reserved for humans minds. These involve use of language, recognition and creation of images, and even reasoning. These sound very advanced, but still ultimately involve parameterized generative models and then pushing new data through them. Hence, it is a subset of machine learning.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>What about machine learning and artificial intelligence?</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/stats_ml_ai.html#sec-why-stat-inference",
    "href": "lessons/bayesian_modeling/stats_ml_ai.html#sec-why-stat-inference",
    "title": "23  What about machine learning and artificial intelligence?",
    "section": "23.4 So why so much statistical inference?",
    "text": "23.4 So why so much statistical inference?\nThe tricky parts of machine learning (and therefore also AI) are coming up with models and then finding reasonable parameters for them so that they are performant. These both are central to statistical inference, which is why we will focus so heavily on it.\nOne point of confusion will be terminology. Those who refer to themselves as practitioners of machine learning have one set of terminology and those who refer to themselves as statisticians have another. Beyond that, the terminology within both fields is often shorthand for an explicit model or an specific method of finding parameters. We will explicitly define models and describe techniques for working with them (not just show how to do it!).",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>What about machine learning and artificial intelligence?</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/bayes_model_for_learning_and_cognition.html",
    "href": "lessons/bayesian_modeling/bayes_model_for_learning_and_cognition.html",
    "title": "24  Bayes's theorem as a model for learning",
    "section": "",
    "text": "24.1 Bayes's theorem as a model for cognition\nLet's say we did an experiment and got data set \\(y_1\\) as an investigation of hypothesis \\(\\theta\\). Then, our posterior distribution is\n\\[\\begin{aligned}\ng(\\theta\\mid y_1) = \\frac{f(y_1 \\mid \\theta)\\, g(\\theta )}{f(y_1)}.\n\\end{aligned}\\]\nNow, let’s say we did another experiment and got data \\(y_2\\). We already know \\(y_1\\) ahead of this experiment, so our prior is \\(g(\\theta\\mid y_1)\\), which is the posterior from the first experiment. So, we have\n\\[\\begin{aligned}\ng(\\theta\\mid y_1, y_2) = \\frac{f(y_2 \\mid y_1, \\theta)\\, g(\\theta \\mid y_1)}{f(y_2 \\mid y_1)}.\n\\end{aligned}\\]\nNow, we plug in Bayes’s theorem applied to our first data set, giving\n\\[\\begin{aligned}\ng(\\theta\\mid y_1, y_2) = \\frac{f(y_2 \\mid y_1, \\theta)\\,f(y_1 \\mid \\theta)\\, g(\\theta )}{f(y_2 \\mid y_1)\\, f(y_1 )}.\n\\end{aligned}\\]\nBy the product rule, the denominator is \\(f(y_1, y_2 )\\). Also by the product rule,\n\\[\\begin{aligned}\nf(y_2 \\mid y_1, \\theta)\\,f(y_1 \\mid \\theta) = f(y_1, y_2 \\mid \\theta).\n\\end{aligned}\\]\nInserting these expressions into equation the above expression for \\(g(\\theta\\mid y_1, y_2)\\) yields\n\\[\\begin{aligned}\ng(\\theta\\mid y_1, y_2) = \\frac{f(y_1, y_2 \\mid \\theta)\\,g(\\theta)}{f(y_1, y_2)}.\n\\end{aligned}\\]\nSo, acquiring more data gave us more information about our hypothesis in that same way as if we just combined \\(y_1\\) and \\(y_2\\) into a single data set. So, acquisition of more and more data serves to help us learn more and more about our hypothesis or parameter value.\nBayes theorem thus describes how we learn from data. We acquire data, and that updates our posterior distribution. That posterior distribution then becomes the prior distribution for interpreting the next data set we acquire, and so on. Data constantly update our knowledge.\nFILL OUT IDEAS HERE from Griffiths, Chater, and Tenenbaum book.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes\\'s theorem as a model for learning</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/prior_predictive_checks.html",
    "href": "lessons/bayesian_modeling/prior_predictive_checks.html",
    "title": "25  Model building with prior predictive checks",
    "section": "",
    "text": "25.1 Building a generative model\n| Download notebook\nWhen we do generative modeling, it is important to understand what kind of data might be generated from our model before looking at the data. You may have been struggling to come up with good priors for your models. I find that this struggle is a symptom of the fact that we often think about what kind of data we might see in an experiment as opposed to how we might think the parameters of the generative process that produces the data may be distributed.\nIn this lesson, we discuss how prior predictive checks can help build appropriate priors for a model. The procedure is used check to make sure the generative model can actually generate reasonable data sets.\nWhen considering how to build the model, we should think about the data generation process. We generally specify the likelihood first, and then use it to identify what the parameters are, which tells us what we need to specify for the prior. Indeed, the prior can often only be understood in the context of the likelihood.\nNote, though, that the split between the likelihood and prior need even be explicit. When we study hierarchical models, we will see that it is sometimes not obvious how to unambiguously define the likelihood and prior. Rather, in order to do Bayesian inference, we really only need to specify the joint distribution, \\(\\pi(y,\\theta)\\), which is the product of the likelihood and prior.\n\\[\\begin{align}\ng(\\theta \\mid y) = \\frac{f(y\\mid \\theta)\\,g(\\theta)}{f(\\theta)} = \\frac{\\pi(y, \\theta)}{f(\\theta)}.\n\\end{align}\n\\]\nAfter defining the joint distribution, either by defining a likelihood and a prior separately (which is what we usually do) or by directly defining the joint, we should check to make sure that making draws of data sets \\(y\\) out of it give reasonable results. In other words, we need to ask if the data generation process we have prescribed actually produces data that obeys physical constraints and matches our intuition. The procedure of prior predictive checks enables this. We use the joint distribution to generate parameter sets and data sets and then check if the results make sense. We will learn about doing this using both Numpy and Stan in this lesson.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model building with prior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/prior_predictive_checks.html#sec-synthetic-rgc-experiment",
    "href": "lessons/bayesian_modeling/prior_predictive_checks.html#sec-synthetic-rgc-experiment",
    "title": "25  Model building with prior predictive checks",
    "section": "25.2 The experiment",
    "text": "25.2 The experiment\nWe will do a very simple experiment in which we take a sample of retinal tissue and expose it to a constant light source. We measure the spiking activity of a single retinal ganglion cell (RGC) over a time interval. We will consider two models, one with Poissonian spiking and one with Inverse Gaussian-distributed spiking. We will build generative models for each, starting with the Poissonian model. We will use prior predictive checks to hone in on the priors.\nIn order to do the prior predictive checks, we will assume we measure spiking for 30 seconds.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model building with prior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/prior_predictive_checks.html#model-1-spikes-are-poissonian",
    "href": "lessons/bayesian_modeling/prior_predictive_checks.html#model-1-spikes-are-poissonian",
    "title": "25  Model building with prior predictive checks",
    "section": "25.3 Model 1: Spikes are Poissonian",
    "text": "25.3 Model 1: Spikes are Poissonian\nFor our first model, we assume that spikes arise from a Poisson process, such that the interspike intervals are exponentially distributed.\n\n25.3.1 The likelihood\nLet \\(y\\) be the set of observed interspike intervals. Then, our likelihood is i.i.d. Exponentially distributed interspike intervals with rate parameter \\(\\beta\\). Our likelihood is then\n\\[\\begin{align}\ny_i \\sim \\text{Expon}(\\beta) \\;\\forall i.\n\\end{align}\n\\]\nThis makes clear that the parameter we need to estimate is \\(\\beta\\), so we will need to provide a prior for it.\n\n\n25.3.2 The prior\nWith our likelihood specified, we need to give a prior for \\(\\beta\\), \\(g(\\beta)\\). For me, it is easier to think about a time scale than a rate, so I will think about priors of \\(\\tau = 1/\\beta\\). So, I ask, “What are reasonable values of waiting times for spikes?” I know that refractory times can be a few milliseconds, and we may have very sluggish spiking, so I want a prior that has probability mass between, say 1 ms and 1 s. I could choose a Log Normal prior for \\(\\tau\\), since my prior knowledge ranges over three orders of magnitude. The Log Normal distribution also has the added benefit that \\(\\beta\\) is guaranteed to be positive, which we need to properly parametrize the likelihood.\nI will use the trick that if I want 95% of probability mass of a Normally distributed parameter to lie between two bounds, I choose a location parameter as the midpoint between the bounds and the scale parameter as a quarter of the distance between the two bounds. In this case, I want \\(\\log_{10} \\tau\\) to lie between 0 and 3, where \\(\\tau\\) is in units of milliseconds. So, I choose the following prior.\n\\[\\begin{align}\n&\\log_{10} \\tau \\sim \\text{Normal}(1.5, 0.75),\\\\[1em]\n&\\beta = 10^{- \\log_{10} \\tau}.\n\\end{align}\n\\]\nI have done the egregious sin of taking the logarithm of a quantity with units, but we understand that \\(\\tau\\) is in units of milliseconds and \\(\\beta\\) is in units of inverse milliseconds.\nSo, we now have a complete model; the likelihood and prior, and therefore the joint distribution, are specified. Here it is (all units are ms):\n\\[\\begin{align}\n&\\log_{10} \\tau \\sim \\text{Normal}(1.5, 0.75),\\\\[1em]\n&\\beta = 10^{- \\log_{10} \\tau},\\\\[1em]\n&y_i \\sim \\text{Expon}(\\beta) \\;\\forall i.\n\\end{align}\n\\]\n\n\n25.3.3 Prior predictive checks\nLet us now generate samples out of this generative model. This procedure is known as prior predictive checking. We first generate parameter values drawing out of the prior distribution for \\(\\beta\\). We then use those parameter values to generate a data set using the likelihood. We repeat this over and over again to see what kind of data sets we might expect out of our generative model. Each of these generated data sets is called a prior predictive sample. We can do this efficiently using Numpy’s random number generators.\n\n# Time of recording\nT = 30_000  # ms\n\n# Instantiate random number generator\nrng = np.random.default_rng()\n\n# Number of prior predictive check samples\nn_ppc_samples = 1000\n\n# Draw parameters out of the prior\nlog_tau = rng.normal(1.5, 0.75, size=n_ppc_samples)\ntau = 10**log_tau\nbeta = 1 / tau\n\n# Draw data sets out of the likelihood for each set of prior params\ndef draw_spikes(beta, T):\n    tau = 1 / beta\n\n    t = rng.exponential(tau)\n    spikes = []\n    while t &lt; T:\n        spikes.append(t)\n        isi = rng.exponential(tau)\n        t += isi\n\n    return np.array(spikes)\n\n\nspikes = [draw_spikes(b, T) for b in beta]\n\nThere are many ways we could visualize the results. One informative plot is to make an ECDF the ISIs for each of the data sets we generated.\n\np = None\nfor spike_vals in spikes[::20]:\n    p = iqplot.ecdf(\n        np.diff(spike_vals),\n        p=p,\n        line_kwargs=dict(line_alpha=0.5, line_width=1),\n        x_axis_label=\"ISI (ms)\",\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is perhaps better viewed on a logarithmic scale.\n\np = None\nfor spike_vals in spikes[::20]:\n    p = iqplot.ecdf(\n        np.diff(spike_vals),\n        p=p,\n        line_kwargs=dict(line_alpha=0.5, line_width=1),\n        x_axis_label=\"ISI (ms)\",\n        x_axis_type='log',\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks reasonable, except that we do get some very short (and indeed also very long) interspike intervals. We could hone the model to try to get a narrower set of ISIs if our domain knowledge dictates that. For me, I am content to have extra broad priors.\nAnother option for display is to plot percentiles of the ECDFs. We can do this using the bebi103.viz.predictive_ecdf() function. It expects input as a Numpy array of shape \\(n_s \\times N\\), where \\(n_s\\) is the number of samples and \\(N\\) is the number of data points. We would have to collect the ISIs in a bit of a different manner, choosing a specific number of ISIs to observe, instead of a total time for observation. We can do that for illustrative purposes, drawing 1,000 ISIs for each prior sample of \\(\\beta\\).\n\n# Draw 1,000 ISIs\nN = 1000\nisis = np.array([rng.exponential(1 / b, size=N) for b in beta])\n\nbokeh.io.show(bebi103.viz.predictive_ecdf(isis, x_axis_label=\"ISI (ms)\", x_axis_type='log'))\n\n\n  \n\n\n\n\n\nIn this plot, the median ECDF is shown in the middle, the darker blue filled region contains 68% of the samples, and the light blue contains 95% of the samples. The extent of the ECDF gives an indication of the extreme values in the prior predictive data set. The bulk of the ISIs lie in a reasonable region, somewhere between 1 and 100 ms.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model building with prior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/prior_predictive_checks.html#prior-predictive-checks-with-stan",
    "href": "lessons/bayesian_modeling/prior_predictive_checks.html#prior-predictive-checks-with-stan",
    "title": "25  Model building with prior predictive checks",
    "section": "25.4 Prior predictive checks with Stan",
    "text": "25.4 Prior predictive checks with Stan\nWhile generating the data sets to be used in prior predictive checks is intuitive and easy using Numpy, I often find it is more convenient to do it in Stan. This may not be obvious now, but will become clearer later when we construct an entire workflow including prior and posterior predictive checks and use simulation-based calibration.\nFor ease, we will not generate spike times for a total of 30 minutes, but will rather generate \\(N\\) interspike intervals (from which the spike times may be easily calculated post facto.\nGenerating prior predictive samples typically does not require Markov chain Monte Carlo, but the random number generation we are more familiar with. Stan does allow for this kind of random number generation. If you want to draw a sample out of one of Stan’s distributions, append the name of the distribution with _rng. Unlike Numpy’s and Scipy’s random number generators, Stan’s RNGs can only draw one sample at a time (though they may be vectorized with array inputs). Therefore, we have to put the random number generation in a for loop. The code below demonstrates this.\ndata {\n  int&lt;lower=1&gt; N;\n}\n\n\ngenerated quantities {\n  // Parameters\n  real log_tau;\n  real&lt;lower=0&gt; tau;\n  real&lt;lower=0&gt; beta_;\n\n  // Data\n  array[N] real isi;\n\n  log_tau = normal_rng(1.5, 0.75);\n  tau = 10^log_tau;\n  beta_ = 1.0 / tau;\n  \n  for (i in 1:N) {\n    isi[i] = exponential_rng(beta_);\n  }\n}\nThe data block contains N, the number of ISI values we want to generate. The generated quantities block defines variables we want to store, in this case beta_ and isi. We first generate values for beta_ and then use those values to generate data isi. Note that though we are not using this syntax here, parameters in the generated quantities block that are enclosed in braces (outside of if statements and for loops, of course) are not saved in the output.\nThe above code will be useful for our prior predictive checks, but if we want to tweak some of the parametrizations of the priors (e.g., we might want \\(\\log_{10}\\tau\\sim \\text{Norm}(1, 1)\\) instead of \\(\\log_{10}\\tau \\sim \\text{Norm}(1.5, 0.75)\\)), we will have to adjust the Stan code and recompile. For the purposes of prior predictive checks, we may also want to include these values in the data block and pass them in as the data kwarg when sampling.\ndata {\n  int&lt;lower=1&gt; N;\n  real log_tau_mu;\n  real&lt;lower=0&gt; log_tau_sigma;\n}\n\n\ngenerated quantities {\n  // Parameters\n  real log_tau;\n  real&lt;lower=0&gt; tau;\n  real&lt;lower=0&gt; beta_;\n\n  // Data\n  array[N] real isi;\n\n  log_tau = normal_rng(log_tau_mu, log_tau_sigma);\n  tau = 10^log_tau;\n  beta_ = 1.0 / tau;\n  \n  for (i in 1:N) {\n    isi[i] = exponential_rng(beta_);\n  }\n}\nLet’s build this Stan model.\n\nwith bebi103.stan.disable_logging():\n    sm_prior_pred = cmdstanpy.CmdStanModel(\n        stan_file=\"poisson_spiking_prior_predictive.stan\"\n    )\n\nTo draw the samples, we specify the contents of the inputted data block, and then sample. We need to use the kwarg fixed_param=True to alert Stan that it will not need to do any MCMC sampling, but rather just run the generated quantities block. We will also use the chains=1 kwarg since we do not need to do parallel sampling.\n\ndata = {\n    \"N\": N,\n    \"log_tau_mu\": 1.5,\n    \"log_tau_sigma\": 0.75,\n}\n\nwith bebi103.stan.disable_logging():\n    samples = sm_prior_pred.sample(\n        data=data, iter_sampling=1000, fixed_param=True, chains=1, show_progress=False\n    )\n\nAs usual, we would like to convert the output to an ArviZ InferenceData instance.\n\nsamples = az.from_cmdstanpy(prior=samples, prior_predictive=['isi'])\n\nWe can pass the array stored in samples.prior_predictive['isi'] directly into the bebi103.viz.predictive_ecdf() function to get the predictive ECDF. (We can do this because we only used one chain. If we had more chains, we would have to reshape the samples into a 2D array, where the row index is the sample and the column index is the index of the data.)\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(\n        samples.prior_predictive['isi'].squeeze(),\n        x_axis_type='log',\n        x_axis_label='ISI (ms)',\n    )\n)\n\n\n  \n\n\n\n\n\nThe result is of course the same as when generating data sets with Numpy.\nAs you can see, it is a bit more verbose to use Stan to generate the prior predictive samples. The calculation time is also substantially longer because of the time required for compilation. Nonetheless, it is often advantageous to use Stan for this stage of an analysis pipeline because when we do simulation based calibration (SBC) of our models, it is useful to have everything written in Stan.",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model building with prior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/prior_predictive_checks.html#model-2-spikes-are-inverse-gaussian-distributed",
    "href": "lessons/bayesian_modeling/prior_predictive_checks.html#model-2-spikes-are-inverse-gaussian-distributed",
    "title": "25  Model building with prior predictive checks",
    "section": "25.5 Model 2: Spikes are Inverse Gaussian distributed",
    "text": "25.5 Model 2: Spikes are Inverse Gaussian distributed\nWe leave building this model with prior predictive checks as an exercise.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model building with prior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/bayesian_modeling/prior_predictive_checks.html#computing-environment",
    "href": "lessons/bayesian_modeling/prior_predictive_checks.html#computing-environment",
    "title": "25  Model building with prior predictive checks",
    "section": "25.6 Computing environment",
    "text": "25.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,cmdstanpy,arviz,bokeh,iqplot,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.3\ncmdstanpy : 1.2.5\narviz     : 0.21.0\nbokeh     : 3.6.2\niqplot    : 0.3.7\nbebi103   : 0.1.27\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Bayesian modeling and inference",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model building with prior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/inference_by_mcmc.html",
    "href": "lessons/inference_by_mcmc/inference_by_mcmc.html",
    "title": "Statistical inference with Markov chain Monte Carlo",
    "section": "",
    "text": "In this section, we will unleash Markov chain Monte Carlo on inference problems. The ability to sample out of an arbitrary distribution comes in very handy, since posterior distributions are arbitrary in the sense that they are what you, the modeler, decides they are!",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "",
    "text": "26.1 The data set\n| Download notebook\nData set download\nIn this lesson, we will learn how to use Markov chain Monte Carlo to do parameter estimation. To get the basic idea behind MCMC, imagine for a moment that we can draw samples out of the posterior distribution. This means that the probability of choosing given values of a set of parameters is proportional to the posterior probability of that set of values. If we drew many many such samples, we could reconstruct the posterior from the samples, e.g., by making histograms. That’s a big thing to imagine: that we can draw properly weighted samples. But, it turns out that we can! That is what MCMC allows us to do.\nWe discussed some theory behind this seemingly miraculous capability in lecture. For this lesson, we will just use the fact that we can do the sampling to learn about posterior distributions in the context of parameter estimation.\nThe data come from the Elowitz lab, published in Singer et al., Dynamic Heterogeneity and DNA Methylation in Embryonic Stem Cells, Molec. Cell, 55, 319-331, 2014, available here. In the following paragraphs, I repeat the description of the data set and EDA from last term:",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#the-data-set",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#the-data-set",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "",
    "text": "In this paper, the authors investigated cell populations of embryonic stem cells using RNA single molecule fluorescence in situ hybridization (smFISH), a technique that enables them to count the number of mRNA transcripts in a cell for a given gene. They were able to measure four different genes in the same cells. So, for one experiment, they get the counts of four different genes in a collection of cells.\nThe authors focused on genes that code for pluripotency-associated regulators to study cell differentiation. Indeed, differing gene expression levels are a hallmark of differentiated cells. The authors do not just look at counts in a given cell at a given time. The temporal nature of gene expression is also important. While the authors do not directly look at temporal data using smFISH (since the technique requires fixing the cells), they did look at time lapse fluorescence movies of other regulators. We will not focus on these experiments here, but will discuss how the distribution of mRNA counts acquired via smFISH can serve to provide some insight about the dynamics of gene expression.\nThe data set we are analyzing now comes from an experiment where smFISH was performed in 279 cells for the genes rex1, rest, nanog, and prdm14. The data set may be downloaded at https://s3.amazonaws.com/bebi103.caltech.edu/data/singer_transcript_counts.csv.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#sec-eda-smfish",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#sec-eda-smfish",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.2 ECDFs of mRNA counts",
    "text": "26.2 ECDFs of mRNA counts\nWe will do a quick EDA to get a feel for the data set by generating ECDFs for the mRNA counts for each of the four genes.\n\ndf = pl.read_csv(os.path.join(data_path, \"singer_transcript_counts.csv\"), comment_prefix=\"#\")\n\ngenes = [\"Nanog\", \"Prdm14\", \"Rest\", \"Rex1\"]\n\nplots = [\n    iqplot.ecdf(\n        data=df[gene],\n        q=gene,\n        x_axis_label=\"mRNA count\",\n        title=gene,\n        frame_height=150,\n        frame_width=200,\n    )\n    for gene in genes\n]\n\nbokeh.io.show(\n    bokeh.layouts.column(bokeh.layouts.row(*plots[:2]), bokeh.layouts.row(*plots[2:]))\n)\n\n\n  \n\n\n\n\n\nNote the difference in the \\(x\\)-axis scales. Clearly, prdm14 has far fewer mRNA copies than the other genes. The presence of two inflection points in the Rex1 EDCF implies bimodality.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#building-a-generative-model",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#building-a-generative-model",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.3 Building a generative model",
    "text": "26.3 Building a generative model\nWe can model the transcript counts, which result from bursty gene expression, as being Negative Binomially distributed. (The details behind this model are a bit nuanced, and you can read about them here.) For a given gene, the likelihood for the counts is\n\\[\n\\begin{align}\nn_i \\mid \\alpha, b \\sim \\text{NegBinom}(\\alpha, 1/b) \\;\\forall i,\n\\end{align}\n\\tag{26.1}\\]\nwhere \\(\\alpha\\) is the burst frequency (higher \\(\\alpha\\) means gene expression comes on more frequently) and \\(b\\) is the burst size, i.e., the typical number of transcripts made per burst. We have therefore identified the two parameters we need to estimate, \\(\\alpha\\) and \\(b\\).\nBecause the Negative Binomial distribution is often parametrized in terms of \\(\\alpha\\) and \\(\\beta= 1/b\\), we can alternatively state our likelihood as\n\\[\n\\begin{align}\n&\\beta = 1/b,\\\\[1em]\n&n_i \\mid \\alpha, \\beta \\sim \\text{NegBinom}(\\alpha, \\beta)\\;\\; \\forall i.\n\\end{align}\n\\tag{26.2}\\]\nGiven that we have a Negative Binomial likelihood, we are left to specify priors the burst size \\(b\\) and the burst frequency \\(\\alpha\\).\n\n26.3.1 Priors for burst size and inter-burst time\nWe will apply the bet-the-farm technique to get our priors for the burst size and inter-burst times. I would expect the time between bursts to be longer than a second, since it takes time for the transcriptional machinery to assemble. I would expect it to be shorter than a few hours, since an organism would need to adapt its gene expression based on environmental changes on that time scale or faster. The time between bursts needs to be in units of RNA lifetimes, and bacterial RNA lifetimes are of order minutes. So, the range of values of \\(\\alpha\\) is \\(10^{-2}\\) to \\(10^2\\), leading to a prior of\n\\[\\begin{align}\n\\log_{10} \\alpha \\sim \\text{Norm}(0, 1).\n\\end{align}\n\\]\nI would expect the burst size to depend on promoter strength and/or strength of transcriptional activators. I could imagine anywhere from a few to a few thousand transcripts per burst, giving a range of \\(10^0\\) to \\(10^4\\), and a prior of\n\\[\\begin{align}\n\\log_{10} b \\sim \\text{Norm}(2, 1).\n\\end{align}\n\\]\nWe then have the following model.\n\\[\n\\begin{align}\n&\\log_{10} \\alpha \\sim \\text{Norm}(0, 1),\\\\[1em]\n&\\log_{10} b \\sim \\text{Norm}(2, 1),\\\\[1em]\n&\\beta = 1/b,\\\\[1em]\n&n_i \\sim \\text{NegBinom}(\\alpha, \\beta) \\;\\forall i.\n\\end{align}\n\\tag{26.3}\\]",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#sampling-the-posterior",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#sampling-the-posterior",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.4 Sampling the posterior",
    "text": "26.4 Sampling the posterior\nTo draw samples out of the posterior, we need to use some new Stan syntax. Here is the Stan code we will use with some notes about Stan syntax.\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0&gt; n;\n}\n\n\nparameters {\n  real log10_alpha;\n  real log10_b;\n}\n\n\ntransformed parameters {\n  real alpha = 10^log10_alpha;\n  real b = 10^log10_b;\n  real beta_ = 1.0 / b;\n}\n\n\nmodel {\n  // Priors\n  log10_alpha ~ normal(0, 1);\n  log10_b ~ normal(2, 1);\n\n  // Likelihood\n  n ~ neg_binomial(alpha, beta_);\n}\n\nNote that the raise-to-power operator is ^, not ** as in Python.\nThe data block contains the counts \\(n\\) of the mRNA transcripts. There are \\(N\\) cells that are measured. Most data blocks look like this. There is an integer parameter that specifies the size of the data set, and then the data set is given as an array. The declaration that n is an array of length N is array [N], followed by the type of data, is integer. We specified a lower bound on the data (as we will do on the parameters) using the &lt;lower=0&gt; syntax.\nThe parameters block tells us what the parameters of the posterior are. In this case, we wish to sample out of the posterior \\(g(\\alpha, b \\mid \\mathbf{n})\\), where \\(\\mathbf{n}\\) is the set of transcript counts for the gene. So, the two parameters are \\(\\alpha\\) and \\(b\\). However, since defining the prior was more easily defined in terms of logarithms, we specify \\(\\log_{10} \\alpha\\) and \\(\\log_{10} b\\) as the parameters.\nThe transformed parameters block allows you to do any transformation of the parameters you are sampling for convenience. In this case, Stan’s Negative Binomial distribution is parametrized by \\(\\beta = 1/b\\), so we make the transformation of the b to beta_. Notice that I have called this variable beta_ and not beta. I did this because beta is one of Stan’s distributions, and you should avoid naming a variable after a word that is already in the Stan language. The other transformations we need to make involve converting the logarithms to the actual parameter values.\nFinally, the model block is where the model is specified. The syntax of the model block is almost identical to that of the hand-written model.\n\nNow that we have specified our model, we can compile it.\n\nsm = cmdstanpy.CmdStanModel(stan_file='smfish.stan')\n\nWith our compiled model, we just need to specify the data and let Stan’s sampler do the work! When using CmdStanPy, the data has to be passed in as a dictionary with keys corresponding to the variable names declared in the data block of the Stan program and values as Numpy arrays with the appropriate data type. For this calculation, we will use the data set for the rest gene.\n\n# Construct data dict, making sure data are ints\ndata = dict(N=len(df), n=df[\"Rest\"].to_numpy())\n\n# Sample using Stan\nsamples = sm.sample(    \n    data=data,\n    chains=4,\n    iter_sampling=1000,\n)\n\n# Convert to ArviZ InferenceData instance\nsamples = az.from_cmdstanpy(posterior=samples)\n\n12:15:48 - cmdstanpy - INFO - CmdStan start processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\n12:15:48 - cmdstanpy - INFO - CmdStan done processing.\n12:15:48 - cmdstanpy - WARNING - Non-fatal error during sampling:\nException: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\nException: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Inverse scale parameter is 0, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\nException: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Inverse scale parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\nException: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Inverse scale parameter is 0, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\n    Exception: neg_binomial_lpmf: Shape parameter is inf, but must be positive finite! (in 'smfish.stan', line 26, column 2 to column 33)\nConsider re-running with show_console=True if the above output is unclear!\n\n\n\n\n\nWe got lots of warnings! In particular, we get warnings that some of the parameters fed into the Negative Binomial distribution are invalid, being either zero or infinite. These warnings are arising during Stan’s warm-up phase as it is assessing optimal settings for sampling, and should not be of concern. It is generally a bad idea to silence warnings, but if you are sure that the warnings that the sampler will throw are of no concern, you can silence logging using the bebi103.stan.disable_logging context. In most notebooks in these notes, to avoid clutter for pedagogical purposes, we will disable the warnings.\n\nwith bebi103.stan.disable_logging():\n    samples = sm.sample(    \n        data=data,\n        chains=4,\n        iter_sampling=1000,\n    )\n\n# Convert to ArviZ InferenceData instance\nsamples = az.from_cmdstanpy(posterior=samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nNow, let’s take a quick look at the samples.\n\nsamples.posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 168kB\nDimensions:      (chain: 4, draw: 1000)\nCoordinates:\n  * chain        (chain) int64 32B 0 1 2 3\n  * draw         (draw) int64 8kB 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    log10_alpha  (chain, draw) float64 32kB 0.5804 0.6351 ... 0.646 0.6322\n    log10_b      (chain, draw) float64 32kB 1.32 1.252 1.21 ... 1.237 1.241\n    alpha        (chain, draw) float64 32kB 3.805 4.316 4.407 ... 4.425 4.288\n    b            (chain, draw) float64 32kB 20.91 17.89 16.2 ... 17.27 17.42\n    beta_        (chain, draw) float64 32kB 0.04783 0.05591 ... 0.05789 0.05742\nAttributes:\n    created_at:                 2025-08-20T09:15:49.005458+00:00\n    arviz_version:              0.22.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.5xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (5)log10_alpha(chain, draw)float640.5804 0.6351 ... 0.646 0.6322array([[0.580387, 0.635072, 0.644183, ..., 0.633148, 0.655338, 0.703629],\n       [0.607035, 0.604947, 0.660854, ..., 0.720163, 0.71799 , 0.593938],\n       [0.713801, 0.715316, 0.703968, ..., 0.630683, 0.628306, 0.706312],\n       [0.610528, 0.600729, 0.641074, ..., 0.598044, 0.645953, 0.632221]],\n      shape=(4, 1000))log10_b(chain, draw)float641.32 1.252 1.21 ... 1.237 1.241array([[1.32032, 1.25249, 1.20962, ..., 1.22404, 1.23303, 1.16735],\n       [1.2667 , 1.25687, 1.23603, ..., 1.15223, 1.15658, 1.29306],\n       [1.15525, 1.16491, 1.15836, ..., 1.24806, 1.24704, 1.16517],\n       [1.26757, 1.26331, 1.22422, ..., 1.26765, 1.23738, 1.24093]],\n      shape=(4, 1000))alpha(chain, draw)float643.805 4.316 4.407 ... 4.425 4.288array([[3.80528, 4.31591, 4.40741, ..., 4.29682, 4.52207, 5.05393],\n       [4.04608, 4.02668, 4.57988, ..., 5.25005, 5.22384, 3.92589],\n       [5.1737 , 5.19178, 5.05787, ..., 4.27251, 4.24919, 5.08524],\n       [4.07876, 3.98776, 4.37596, ..., 3.96318, 4.42541, 4.28767]],\n      shape=(4, 1000))b(chain, draw)float6420.91 17.89 16.2 ... 17.27 17.42array([[20.9083, 17.885 , 16.2038, ..., 16.751 , 17.1012, 14.7012],\n       [18.4798, 18.0665, 17.2197, ..., 14.1982, 14.3409, 19.6364],\n       [14.2971, 14.6188, 14.3999, ..., 17.7034, 17.6622, 14.6274],\n       [18.5168, 18.3363, 16.7579, ..., 18.5204, 17.2736, 17.4153]],\n      shape=(4, 1000))beta_(chain, draw)float640.04783 0.05591 ... 0.05789 0.05742array([[0.0478279, 0.0559126, 0.0617138, ..., 0.059698 , 0.0584756,\n        0.0680215],\n       [0.0541131, 0.055351 , 0.058073 , ..., 0.0704317, 0.0697308,\n        0.0509258],\n       [0.0699441, 0.0684051, 0.0694452, ..., 0.0564862, 0.0566181,\n        0.068365 ],\n       [0.054005 , 0.0545367, 0.0596733, ..., 0.0539944, 0.0578917,\n        0.0574209]], shape=(4, 1000))Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2025-08-20T09:15:49.005458+00:00arviz_version :0.22.0inference_library :cmdstanpyinference_library_version :1.2.5\n\n\nAs we have already seen, the samples are indexed by chain and draw. Parameters represented in the parameters and transformed parameters blocks are reported.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#plots-of-the-samples",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#plots-of-the-samples",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.5 Plots of the samples",
    "text": "26.5 Plots of the samples\nThere are many ways of looking at the samples. In this case, since we have two parameters of interest, the pulse frequency and pulse size, we can plot the samples as a scatter plot to get the approximate density.\n\ndef plot_scatter(samples, title=None):\n    p = bokeh.plotting.figure(\n        frame_width=200,\n        frame_height=200,\n        x_axis_label='α',\n        y_axis_label='b',\n        title=title,\n    )\n    \n    p.scatter(\n        samples.posterior['alpha'].values.ravel(), \n        samples.posterior['b'].values.ravel(),\n        size=2,\n        alpha=0.2,\n    )\n\n    return p\n\nbokeh.io.show(plot_scatter(samples))\n\n\n  \n\n\n\n\n\nWe see very strong correlation between \\(\\alpha\\) and \\(b\\). This does not necessarily mean that they depend on each other. Rather, it means that our degree of belief about their values depends on both in a correlated way. The measurements we made cannot effectively separate the effects of \\(\\alpha\\) and \\(b\\) on the transcript counts.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#marginalizing-the-posterior",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#marginalizing-the-posterior",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.6 Marginalizing the posterior",
    "text": "26.6 Marginalizing the posterior\nWe can also plot the marginalized posterior distributions. Remember that the marginalized distributions properly take into account the effects of the other variable, including the strong correlation I just mentioned. To obtain the marginalized distribution, we simply ignore the samples of the parameters we are marginalizing out. It is convenient to look at the marginalized distributions as ECDFs.\n\nplots = [\n    iqplot.ecdf(\n        samples.posterior[param].values.ravel(),\n        q=param,\n        frame_height=200,\n        frame_width=250,\n    )\n    for param in [\"alpha\", \"b\"]\n]\n\nbokeh.io.show(bokeh.layouts.row(*plots))\n\n\n  \n\n\n\n\n\nAlternatively, we can visualize the marginalized posterior PDFs as histograms. Because we have such a large number of samples, binning bias from histograms is less of a concern.\n\nplots = [\n    iqplot.histogram(\n        samples.posterior[param].values.ravel(),\n        q=param,\n        rug=False,\n        frame_height=200,\n        frame_width=250,\n    )\n    for param in [\"alpha\", \"b\"]\n]\n\nbokeh.io.show(bokeh.layouts.row(*plots))",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#sec-corner-plots",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#sec-corner-plots",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.7 Corner plots",
    "text": "26.7 Corner plots\nWe now have a two-dimensional posterior distribution, with our two parameters being \\(\\alpha\\) and \\(b\\). We can combine a plot of the samples from the full posterior with the histograms (or ECDFs) of samples from the marginal distributions in a corner plot, available via bebi103.viz.corner().\n\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples, parameters=[('alpha', 'α'), 'b']\n    )\n)\n\n\n  \n\n\n\n\n\nCorner plots generalize to dimensions beyond two. The off-diagonal plots are of samples from marginal distributions where two parameters remain and the diagonals are plots from univariate marginal distributions.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#computing-environment",
    "href": "lessons/inference_by_mcmc/parameter_estimation_with_mcmc_1.html#computing-environment",
    "title": "26  Parameter estimation with Markov chain Monte Carlo",
    "section": "26.8 Computing environment",
    "text": "26.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,cmdstanpy,arviz,bokeh,iqplot,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\npolars    : 1.31.0\ncmdstanpy : 1.2.5\narviz     : 0.22.0\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Parameter estimation with Markov chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_summaries.html",
    "href": "lessons/inference_by_mcmc/posterior_summaries.html",
    "title": "27  Reporting summaries of the posterior",
    "section": "",
    "text": "27.1 Reporting summaries of MCMC samples\n| Download notebook\nPerforming MCMC calculations gives you full information about the posterior, which you can summarize in beautiful and informative corner plots. But very often, we wish to summarize the posterior in a few simple numbers. In particular, we wish to report a credible interval for a single parameter. This is computed from the marginalized posterior. The credible interval is a region in parameter space where we might expect a parameter value to lie. This credible interval is often reported and plotted as an error bar.\nWe will consider three commonly used ways of plotting a value plus error bar. We will use a 95% credible interval for this demonstration.\nTo illustrate the relative merits of these reporting schemes, we’ll draw samples samples out of some artificial distributions.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting summaries of the posterior</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_summaries.html#reporting-summaries-of-mcmc-samples",
    "href": "lessons/inference_by_mcmc/posterior_summaries.html#reporting-summaries-of-mcmc-samples",
    "title": "27  Reporting summaries of the posterior",
    "section": "",
    "text": "mean ± standard deviation: The most commonly used credible interval is \\(\\mu \\pm k\\sigma\\), where \\(k\\) is chosen to give the appropriate credible interval, approximating the marginalized posterior as Normal. We’ll do 95%, which means \\(k = 1.96\\).\nmedian with quantile: The posterior need not be Normal. If it is not, we would like a more robust way to summarize it. A simple method is to report the median, and then give lower and upper bounds to the error bar based on quantile. In our case, we would report the 2.5th percentile and the 97.5th percentile.\nmode with HPD: This method uses the highest posterior density interval, or HPD (also known as the HDI for highest density interval). If we’re considering a 95% credible interval, the HPD interval is the shortest interval that contains 95% of the probability mass of the posterior. So, we report the mode (the most probable parameter value) and then the bounds on the HPD interval.\nmedian with HPD: We report the HPD, but report the median parameter value from our samples instead of the mode.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting summaries of the posterior</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_summaries.html#some-distributions-to-sample",
    "href": "lessons/inference_by_mcmc/posterior_summaries.html#some-distributions-to-sample",
    "title": "27  Reporting summaries of the posterior",
    "section": "27.2 Some distributions to sample",
    "text": "27.2 Some distributions to sample\nWe will generate some distributions to sample. We will consider an exponential distribution, a Normal, the sum of two Normals, and a distribution with a long tail. We choose these to illustrate how various choices of the credible interval will be reported.\nLet’s define the models and draw samples. MCMC is not necessary here for this illustrative exercise, so I will just draw the samples using Numpy.\n\n# Parametrize models\nmu = 1.0\nsigma = 0.25\nmu_2 = np.array([mu, 3.0])\nsigma_2 = np.array([sigma, 0.5])\n\n# Instantiate RNG\nrng = np.random.default_rng(seed=3252)\n\n# Draw out of the distributions, start with exponential\nx_expon = rng.exponential(mu, size=15000)\n\n# Normal\nx_norm = rng.normal(mu, sigma, size=15000)\n\n# Bimodal mixture of Normals\nwhich_norm = rng.choice([0, 1], 15000)\nx_2norm = rng.normal(mu_2[which_norm], sigma_2[which_norm])\n\n# Heavy tailed Pareto\nx_heavytail = rng.pareto(1.2, size=15000)\n\n# Store samples in a list for easy access\nsamples = [x_expon, x_norm, x_2norm, x_heavytail]\n\nLet’s look at what we got by plotting the histograms.\n\nnames = [\"Exponential\", \"Normal\", \"Two Normals\", \"Heavy tail\"]\nplots = [\n    iqplot.histogram(\n        x,\n        density=True,\n        rug=False,\n        frame_width=250,\n        frame_height=200,\n        x_axis_label=\"x\",\n        y_axis_label=\"pdf\",\n        title=t,\n        line_kwargs={\"line_width\": 2, \"line_color\": \"gray\"},\n    )\n    for x, t in zip(samples[:-1], names[:-1])\n]\n\n# Make plot for heavy tail (needs different bins because of heavy tail)\nplots.append(\n    iqplot.histogram(\n        samples[-1],\n        bins=np.arange(21),\n        density=True,\n        rug=False,\n        frame_width=250,\n        frame_height=200,\n        x_axis_label=\"x\",\n        y_axis_label=\"pdf\",\n        title=\"Heavy tail\",\n        line_kwargs={\"line_width\": 2, \"line_color\": \"gray\"},\n    )\n)\n\n# Show in a grid\nbokeh.io.show(bokeh.layouts.column(bokeh.layouts.row(*plots[:2]), bokeh.layouts.row(*plots[2:])))",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting summaries of the posterior</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_summaries.html#summarizing-the-mcmc-results-with-error-bars",
    "href": "lessons/inference_by_mcmc/posterior_summaries.html#summarizing-the-mcmc-results-with-error-bars",
    "title": "27  Reporting summaries of the posterior",
    "section": "27.3 Summarizing the “MCMC” results with error bars",
    "text": "27.3 Summarizing the “MCMC” results with error bars\nWe will compute several summary statistics of our results. The mean, standard deviation, median, and quantiles are easy to compute with NumPy. We could obtain the mode from MCMC samples using the log posterior values, the finding sample for which the log posterior is maximal. But since we quickly generated our samples using Numpy, we do not have log posterior values readily available (though we could calculated them), so I will hand-code the known modes.\nComputation of the HPD is a little trickier. The idea is that we rank-order the MCMC trace. We know that the number of samples that are included in the HPD is 0.95 times the total number of MCMC samples. We then consider all intervals that contain that many samples and find the shortest one. This is accomplished using the ArviZ function az.hdi() function.\nLet’s compute all of these summaries and store them in a data frame for convenient reference.\n\n# DataFrame to store summary stats\ndf_summary = pl.DataFrame(\n    [['mean', 'std', '2.5', 'median', '97.5', 'mode', 'hpd_low', 'hpd_high']], \n    schema=['statistic']\n)\n\n# Add in summary statistics of dists (with known modes)\nmodes = [0, mu, mu, 1]\nfor x, name, mode in zip(samples, names, modes):\n    s = pl.Series(\n        np.concatenate(\n            ([x.mean(), x.std()], \n             np.percentile(x, [2.5, 50, 97.5]), \n             [mode], \n             az.hdi(x, hdi_prob=0.95))\n        )\n    )\n    df_summary = df_summary.with_columns(s.alias(name))\n\n# Take a look\ndf_summary\n\n\nshape: (8, 5)\n\n\n\nstatistic\nExponential\nNormal\nTwo Normals\nHeavy tail\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"mean\"\n1.007843\n1.001447\n2.004005\n4.134071\n\n\n\"std\"\n1.005351\n0.24914\n1.07385\n44.580247\n\n\n\"2.5\"\n0.024875\n0.511552\n0.590438\n0.02307\n\n\n\"median\"\n0.701092\n1.000698\n1.767975\n0.79814\n\n\n\"97.5\"\n3.649541\n1.49205\n3.804977\n20.613608\n\n\n\"mode\"\n0.0\n1.0\n1.0\n1.0\n\n\n\"hpd_low\"\n0.000088\n0.531863\n0.561362\n0.000057\n\n\n\"hpd_high\"\n2.996549\n1.507314\n3.755653\n11.296449\n\n\n\n\n\n\nIt is easier to visualize these summaries as error bars on plots. There’s a bit of code below to generate the plots, but bear with me.\n\n# y_values for bars on plots\ny_vals = [\n    [0.8, 0.6, 0.4, 0.2],  # Exponential\n    [1.33, 1, 0.66, 0.33],  # Normal\n    [0.6, 0.45, 0.3, 0.15],  # Two Normals\n    [0.5, 0.4, 0.3, 0.2],  # Heavy tail\n]\n\n# Color scheme\ncolors = bokeh.palettes.Category10[10]\n\n\ndef plot_interval(x, y, barx, color, legend_label, p):\n    p.scatter([x], [y], size=10, color=color, legend_label=legend_label)\n    p.line(barx, [y, y], line_width=4, color=color)\n\n\nlegends = [\"mean ± std\", \"quantile\", \"mode/HPD\", \"median/HPD\"]\nfor p, name, y in zip(plots, names, y_vals):\n    # Mean ± std\n    std = df_summary.filter(pl.col('statistic') == \"std\")[name].item()\n    x = df_summary.filter(pl.col('statistic') == \"mean\")[name].item()\n    barx = x + np.array([1, -1]) * 1.96 * std\n    plot_interval(x, y[0], barx, colors[0], legends[0], p)\n\n    # Median with central 95% interval\n    x = df_summary.filter(pl.col('statistic') == \"median\")[name].item()\n    barx = df_summary.filter(\n        pl.col('statistic').is_in(['2.5', '97.5'])\n    )[name].to_numpy().flatten()\n    plot_interval(x, y[1], barx, colors[1], legends[1], p)\n\n    # Mode with HPD\n    x = df_summary.filter(pl.col('statistic') == \"mode\")[name].item()\n    barx = df_summary.filter(\n        pl.col('statistic').is_in(['hpd_low', 'hpd_high'])\n    )[name].to_numpy().flatten()\n    plot_interval(x, y[2], barx, colors[2], legends[2], p)\n\n    # Median with HPD\n    x = df_summary.filter(pl.col('statistic') == \"median\")[name].item()\n    barx = df_summary.filter(\n        pl.col('statistic').is_in(['hpd_low', 'hpd_high'])\n    )[name].to_numpy().flatten()\n    plot_interval(x, y[3], barx, colors[3], legends[3], p)\n\nplots[0].legend.visible = True\nfor i in [1, 2, 3]:\n    plots[i].legend.visible = False\n\nbokeh.io.show(\n    bokeh.layouts.column(bokeh.layouts.row(*plots[:2]), bokeh.layouts.row(*plots[2:]))\n)",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting summaries of the posterior</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_summaries.html#relative-merits-of-each-method",
    "href": "lessons/inference_by_mcmc/posterior_summaries.html#relative-merits-of-each-method",
    "title": "27  Reporting summaries of the posterior",
    "section": "27.4 Relative merits of each method",
    "text": "27.4 Relative merits of each method\n\nThe mean/std does not respect bounds on the posterior, nor any asymmetry. Unless we are going for speed and using a MAP finder/Normal approximation, there is no need for this method or summarizing the posterior.\n\n\n\nThe primary advantage of the quantile approach is that it is very easy to interpret, especially for the researcher uninitiated to Bayesian statistics. It does not suffer from the problems that the mean/std method does. It does not rely on any approximations.\n\n\n\nThe mode/HPD method gives just that: where the parameter value is most likely to fall, which is not necessarily the interquantile region with the median at its center. It may also be nice to know the most probable parameter value (the MAP), though this is seldom as useful as expectations. The drawback is the possible difficulty of interpretability for the uninitiated. Furthermore, I think it places too much emphasis on the MAP. This is not always the most relevant thing to know.\n\n\n\nThe median/HPD method is my personal favorite. It gives the HPD (with the advantages I just stated) and also the median, which to me is a more useful statistic than the mode.\n\n\nIn any case, attempting to describe a multi-modal posterior with an error bar is misleading and futile. A distribution with a long tail can also be deceiving. Even if you report a 95% credible interval, there is still a 5% chance the parameter value would be reeeeally big.\n\n27.4.1 How to display the summary in text.\nOne issue that may be worrying you is how to report the asymmetric error bars in text. This is best seen by example. For the example of the Exponential, we may report the median with HPD as \\(0.71^{+2.31}_{-0.70}\\).",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting summaries of the posterior</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_summaries.html#computing-environment",
    "href": "lessons/inference_by_mcmc/posterior_summaries.html#computing-environment",
    "title": "27  Reporting summaries of the posterior",
    "section": "27.5 Computing environment",
    "text": "27.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,arviz,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.30.0\narviz     : 0.21.0\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.7",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Reporting summaries of the posterior</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_predictive_checks.html",
    "href": "lessons/inference_by_mcmc/posterior_predictive_checks.html",
    "title": "28  Posterior predictive checks",
    "section": "",
    "text": "28.1 Getting posterior predictive samples using Stan\n| Download notebook\nData set download\nIn Chapter 26, we performed parameter estimation using MCMC. We were thrilled to get parameter estimates! But how do we know if our model is a reasonable approximation of the true data generation process?\nOne obvious approach is to consider parameter values suggested by our posterior and then use those to parametrize the likelihood to generate new data sets from the model. We can then check to see if the observed data are consistent with those parametried by the posterior-parametrized model. This apprrach is referred to as posterior predictive checks. The procedure is the remarkably similar to prior predictive checks with one difference (highlighted in bold below).\nConveniently, we get samples of parameter values out of the posterior from Markov chain Monte Carlo. Once we have the generated data sets, we can compare them to the measured data. This helps answer the question: Could this generative model actually produce the observed data? If the answer is yes, the generative model is not ruled out by the data (though it still may be a bad model). If the answer is no, then the generative model cannot fully describe the process by which the data were generated.\nPart of the art of performing a posterior predictive check (or a prior predictive check for that matter) is choosing good summaries of the measured data that can be clearly and quantitatively visualized. Which summaries you choose to plot is up to you, and is often not a trivial choice; as Michael Betancourt says, “Constructing an interpretable yet informative summary statistic is very much a fine art.” For univariate measurements, the ECDF is a good summary. You may need to choose particular summaries that are best for your modeling task at hand.\nLet us proceed with posterior predictive checked on our inferences in Chapter 26. As a reminder, the generative model, given by Equation 26.3, is below.\n\\[\\begin{align}\n&\\log_{10} \\alpha \\sim \\text{Norm}(0, 1),\\\\[1em]\n&\\log_{10} b \\sim \\text{Norm}(2, 1),\\\\[1em]\n&\\beta = 1/b,\\\\[1em]\n&n_i \\sim \\text{NegBinom}(\\alpha, \\beta) \\;\\forall i.\n\\end{align}\n\\]\nWe could always get samples out of the posterior and then use Numpy to generate posterior predictive samples. However, it is more convenient to use Stan to do it. As such, we augment our Stan code with posterior predictive checks in the generated quantities block. Our updated Stan code is\nLet’s grab our samples, including the posterior predictive checks!\n# Load in as dataframe\ndf = pl.read_csv(os.path.join(data_path, \"singer_transcript_counts.csv\"), comment_prefix=\"#\")\n\n# Construct data dict, making sure data are ints\ndata = dict(N=len(df), n=df[\"Rest\"].to_numpy())\n\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='smfish_with_ppc.stan')\n    samples = sm.sample(data=data)\nWhen we convert the samples into an ArviZ instance, we should specify that the variable n_ppc is a posterior predictive variable.\nsamples = az.from_cmdstanpy(samples, posterior_predictive='n_ppc')\nNow, let’s look at the posterior predictive checks. Note that the n_ppc variable has an entire data set of \\(N\\) mRNA counts for each posterior sample of \\(\\alpha\\) and \\(b\\). This can be verified by looking at its dimension.\nsamples.posterior_predictive.n_ppc.shape\n\n(4, 1000, 279)\nIndeed, for four chains, we have 1000 samples each, with each sample containing 279 mRNA counts. To perform a graphical posterior predictive check, we can plot all 4000 ECDFs of the posterior-sampled mRNA copy numbers. For each point along the \\(n\\)-axis, we compute percentiles of the ECDF values, and this gives us intervals within which we might expect data sets to lie. We can then overlay the measured data and compare.\nThe bebi103.viz.predictive_ecdf() function does this for us. It expects input having n_samples rows and N columns, where N is the number of data points and n_samples is the total number of posterior predictive data sets we generated. Because we sampled with four chains, the posterior predictive array is three-dimensional. The first index is the chain, the second the draw, and the third is the number of data points. The samples are stored as an xarray, which we can reshape using the stack function. We will collapse the chain and draw indexes into a single sample index. We also want to be sure to specify the ordering of the indexes; samples should go first, followed by the number of the data point. We can do this using the transpose() method of an xarray DataArray, which lets us specify the ordering of the indexes. We can then make the predictive ECDF plot, passing in our measured data using the datakeyword argument.\nn_ppc = (\n    samples.posterior_predictive['n_ppc']\n    .stack({\"sample\": (\"chain\", \"draw\")})\n    .transpose(\"sample\", \"n_ppc_dim_0\")\n)\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(n_ppc, data=df['Rest'])\n)\nThe dark line is the median posterior-parametrized ECDF, and the shaded regions contain 68% and 95% of the samples. The data seem consistent with the model. This can be seen more clearly be comparing the difference of the ECDFs from the median ECDF, which is accomplished using the diff='ecdf' keyword argument.\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(n_ppc, data=df['Rest'], diff='ecdf')\n)\nWe see more clearly that the observed data set is consistent with data sets generated by the model.",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_predictive_checks.html#getting-posterior-predictive-samples-using-stan",
    "href": "lessons/inference_by_mcmc/posterior_predictive_checks.html#getting-posterior-predictive-samples-using-stan",
    "title": "28  Posterior predictive checks",
    "section": "",
    "text": "data {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0&gt; n;\n}\n\n\nparameters {\n  real log10_alpha;\n  real log10_b;\n}\n\n\ntransformed parameters {\n  real alpha = 10^log10_alpha;\n  real b = 10^log10_b;\n  real beta_ = 1.0 / b;\n}\n\n\nmodel {\n  // Priors\n  log10_alpha ~ normal(0, 1);\n  log10_b ~ normal(2, 1);\n\n  // Likelihood\n  n ~ neg_binomial(alpha, beta_);\n}\n\n\ngenerated quantities {\n  array[N] int&lt;lower=0&gt; n_ppc;\n  n_ppc = neg_binomial_rng(alpha, beta_);\n}",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_predictive_checks.html#how-about-rex1",
    "href": "lessons/inference_by_mcmc/posterior_predictive_checks.html#how-about-rex1",
    "title": "28  Posterior predictive checks",
    "section": "28.2 How about Rex1?",
    "text": "28.2 How about Rex1?\nLet us now do the same analysis with the Rex1 gene, which, if you recall the EDA (Section 26.2), exhibited bimodality and a Negative Binomial model may not be our best option.\n\n# Construct data dict for Rex1\ndata_rex1 = dict(N=len(df), n=df[\"Rex1\"].to_numpy())\n\n# Grab samples\nwith bebi103.stan.disable_logging():\n    samples = az.from_cmdstanpy(\n        sm.sample(data=data_rex1), \n        posterior_predictive='n_ppc'\n    )\n\n# Plot posterior predictive check\nn_ppc = (\n    samples.posterior_predictive['n_ppc']\n    .stack({\"sample\": (\"chain\", \"draw\")})\n    .transpose(\"sample\", \"n_ppc_dim_0\")\n)\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(n_ppc, data=df['Rex1'])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n  \n\n\n\n\n\nThis already looks bad. Let’s look at the difference of the ECDFs to get more clarity.\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(n_ppc, data=df['Rex1'], diff='ecdf')\n)\n\n\n  \n\n\n\n\n\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/inference_by_mcmc/posterior_predictive_checks.html#computing-environment",
    "href": "lessons/inference_by_mcmc/posterior_predictive_checks.html#computing-environment",
    "title": "28  Posterior predictive checks",
    "section": "28.3 Computing environment",
    "text": "28.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,cmdstanpy,arviz,bokeh,iqplot,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\npolars    : 1.31.0\ncmdstanpy : 1.2.5\narviz     : 0.22.0\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Statistical inference with Markov chain Monte Carlo",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "lessons/principled/principled.html",
    "href": "lessons/principled/principled.html",
    "title": "Principled inference pipelines",
    "section": "",
    "text": "We have just begun exploring the tools of statistical inference, with much of our focus being on MCMC. Regardless of what tool we use, we should bear in mind that statistical inference is fragile. There are many failure modes.\nIn what follows, we will lay out an approach to help prevent us from falling into common traps. We do this in the context of MCMC-based inference, but the principles we lay out here apply to any approach, including variational and optimization approaches. The key idea is that we throw all sorts of possible data sets at our inference pipeline and investigate its performance, sniffing out aspects of data sets that make it brittle.",
    "crumbs": [
      "Principled inference pipelines"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html",
    "href": "lessons/principled/mcmc_diagnostics.html",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "",
    "text": "29.1 The funnel of hell\n| Download notebook\nIn previous lessons, we have seen that we can sample out of arbitrary probability distributions, most notably posterior probability distributions in the context of Bayesian inference, using Markov chain Monte Carlo. However, there are a few questions we need to answer to make sure our MCMC samplers are in fact sampling the target distribution.\nThere are diagnostic checks we can do to address these questions, and these checks are the topic of this lesson.\nWhile we will not be directly considering the posterior, it is worthwhile to work with a distribution that is difficult to sample out of when learning how to diagnose issues with the sampler. We will therefore consider the funnel of hell that we visited in ?exr-funnel-of-hell. As you will see in forthcoming lessons, it has many of the same pathologies that are often present in hierarchical models. Here is our simple-looking, but difficult distribution for sampling.\n\\[\\begin{align}\n& v \\sim \\text{Norm}(0, 3),\\\\[1em]\n& \\theta \\sim \\text{Norm}(0, \\mathrm{e}^{v/2}).\n\\end{align}\\]\nThat is, \\(v\\) is Normally distribution with mean zero and variance 9, and \\(\\theta\\) is Normally distributed with mean zero and variance \\(\\mathrm{e}^v\\). The joint distribution is then\n\\[\\begin{align}\nP(\\theta, v) = P(\\theta\\mid v) \\,P(v) = \\frac{\\mathrm{e}^{-v/2}}{6\\pi}\\,\\exp\\left[-\\frac{1}{2}\\left(\\frac{v^2}{9} + \\frac{\\theta^2}{\\mathrm{e}^v}\\right)\\right]\n\\end{align}\\]\nWe can compute this analytically, so let’s make a plot of it so we know what we’re sampling out of.\ntheta = np.linspace(-4, 4, 400)\nv = np.linspace(-15, 5, 400)\n\nTHETA, V = np.meshgrid(theta, v)\nP = np.exp(-V/2) / 6 / np.pi * np.exp(-(V**2 / 9 + THETA**2 / np.exp(V))/2)\n\n# Show it hacking contour to show image, but no contours\nbokeh.io.show(bebi103.viz.contour(THETA, V, P, overlaid=True, line_kwargs=dict(alpha=0)))\nMuch of the probability density lies deep in the funnel, which is a region of high curvature. The sampler may have some real troubles down there.\nBefore proceeding to attempt to sample this, I note that use of this funnel originates from section 8 of this paper by Radford Neal, and this section of this tutorial draws from this paper by Betancourt and Girolami.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#the-funnel-of-hell",
    "href": "lessons/principled/mcmc_diagnostics.html#the-funnel-of-hell",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "",
    "text": "29.1.1 Sampling out of the funnel\nNow, we’ll code up a Stan model for the funnel and draw some samples using MCMC. The Stan code is short and simple.\nparameters {\n  real theta;\n  real v; \n}\n\n\nmodel {\nv ~ normal(0, 3);\ntheta ~ normal(0, exp(v/2));\n}\nLet’s compile and sample!\n\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='funnel.stan')\n    samples = sm.sample(seed=3252)\n\nsamples = az.from_cmdstanpy(samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nWith samples in hand, we will proceed to define and compute diagnostics for the sampler.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#diagnostics-for-any-mcmc-sampler",
    "href": "lessons/principled/mcmc_diagnostics.html#diagnostics-for-any-mcmc-sampler",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "29.2 Diagnostics for any MCMC sampler",
    "text": "29.2 Diagnostics for any MCMC sampler\nWe will first investigate diagnostics that apply to any MCMC sampler, not just Hamiltonian Monte Carlo samplers like Stan uses.\n\n29.2.1 The Gelman-Rubin R-hat statistic\nThe Gelman-Rubin R-hat statistic is a useful metric to determine if we have achieved stationarity with our chains. The idea is that we run multiple chains in parallel (at least four). For a given parameter, we then compute the variance in the samples between the chains, and then the variance of samples within the chains. The ratio of these two is the Gelman-Rubin R-hat statistic, usually denoted as \\(\\hat{R}\\), and we compute \\(\\hat{R}\\) for each chain.\n\\[\\begin{align}\n\\hat{R} = \\frac{\\text{variance between chains}}{\\text{variance within chains}}.\n\\end{align}\\]\nThe value of \\(\\hat{R}\\) approaches unity if the chains are properly sampling the target distribution because the chains should be identical in their sampling of the posterior if they have all reached the limiting distribution. As a rule of thumb, recommended by Vehtari, et al., 2021, the value of \\(\\hat{R}\\) should be less than 1.01. There are more details involved in calculation of \\(\\hat{R}\\), and you may read about them in the Vehtari, et al. paper.\nArviZ automatically computes \\(\\hat{R}\\) using state-of-the-art rank normalization techniques (published in Vehtari, et al.).\n\naz.rhat(samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 16B\nDimensions:  ()\nData variables:\n    theta    float64 8B 1.054\n    v        float64 8B 1.099xarray.DatasetDimensions:Coordinates: (0)Data variables: (2)theta()float641.054array(1.05383965)v()float641.099array(1.09923991)Indexes: (0)Attributes: (0)\n\n\nWe see that Rhat for each of the two parameters is above 1.01, violating the rule of thumb.\nIf we want to see a quick summary of the results of MCMC, including mean parameter values, we can use az.summary(). This gives a Pandas data frame (which has an index, in this case the names of the parameters), which is convenient for display.\n\naz.summary(samples)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ntheta\n0.060\n7.233\n-9.078\n8.971\n0.478\n1.507\n467.0\n298.0\n1.05\n\n\nv\n0.667\n2.627\n-4.464\n5.145\n0.392\n0.313\n41.0\n18.0\n1.10\n\n\n\n\n\n\n\nWe will discuss what some of these other statistics aside from \\(\\hat{R}\\) mean momentarily.\nWe can take more samples to boost \\(\\hat{R}\\), so let’s do that.\n\nwith bebi103.stan.disable_logging():\n    samples = sm.sample(iter_sampling=100_000, seed=3252)\n\nsamples = az.from_cmdstanpy(samples)\n\n# Check R-hat\naz.rhat(samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 16B\nDimensions:  ()\nData variables:\n    theta    float64 8B 1.003\n    v        float64 8B 1.005xarray.DatasetDimensions:Coordinates: (0)Data variables: (2)theta()float641.003array(1.00256057)v()float641.005array(1.00545998)Indexes: (0)Attributes: (0)\n\n\nWe are now in compliance with the rule of thumb. However, taking so many samples to achieve a good \\(\\hat{R}\\) is indicative of other problems with the sampler. We will therefore continue out tour of diagnostics with the standard 1000 samples per chain.\n\nwith bebi103.stan.disable_logging():\n    samples = sm.sample(seed=3252, show_progress=False)\n\nsamples = az.from_cmdstanpy(samples)\n\n\n\n29.2.2 Effective samples size\nRecall that MCMC samplers do not draw independent samples from the target distribution. Rather, the samples are correlated. Ideally, though, we would draw independent samples. We would like to get an estimate for the number of effectively independent samples we draw. This is referred to either as effective samples size (ESS) or number of effective samples (\\(n_\\mathrm{eff}\\)).\nArviZ computes ESS according to the prescription laid out in the Vehtari, et al. paper using az.ess(). In the summary, this is given in the ess_bulk column.\n\naz.summary(samples)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ntheta\n0.060\n7.233\n-9.078\n8.971\n0.478\n1.507\n467.0\n298.0\n1.05\n\n\nv\n0.667\n2.627\n-4.464\n5.145\n0.392\n0.313\n41.0\n18.0\n1.10\n\n\n\n\n\n\n\nWe took a total of 4000 steps (1000 on each of four chains), and got an ESS of about 450 for \\(\\theta\\), but a tiny 40 for \\(v\\). As a rule of thumb, according to Vehtari, et al., you should have ESS &gt; 400, so we have a problem here.\nWe will also consider ess_tail, commonly referred to as tail-ESS. Again, I will not go into detail of how this is calculated, but this is the effective sample size when considering the more extreme values of the posterior (by default the lower and upper 5th percentiles). Note that this is not the number of samples that landed in the tails, but rather a measure of what the total number of effective samples would be if we were effectively sampling the tails. Again, we want tail-ESS to be greater than 400 as a rule of thumb. We have problems here.\nBear in mind that the ESS calculation is approximate and subject to error. There are, as usual, other caveats, which are discussed in the Vehtari, et al. paper and the Stan manual.\n\n\n29.2.3 Monte Carlo standard error\nThe Monte Carlo standard errors (MCSE) are reported as msce_mean and mcse_sd. They are measurements of the standard error of the mean and the standard error of the standard deviation of the chains. They provide an estimate as to how accurate the expectation values given from MCMC samples of the mean and standard deviation are. In practice, if the MCSE of the mean is less than the standard deviation of the samples themselves (that is the mcse_mean column is much less than the sd column), we have taken plenty of samples. The only reason to use the MCSE is if we have a particular strong interest in getting very precise measurement of the mean in particular.\nI was hesitant to even discuss this here, since I agree with Gelman, “For Bayesian inference, I don’t think it’s generally necessary or appropriate to report Monte Carlo standard errors of posterior means and quantiles…”",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#diagnostics-for-hmc",
    "href": "lessons/principled/mcmc_diagnostics.html#diagnostics-for-hmc",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "29.3 Diagnostics for HMC",
    "text": "29.3 Diagnostics for HMC\nBoth \\(\\hat{R}\\) and ESS are useful diagnostics for any MCMC sampler, but Hamiltonian Monte Carlo offers other diagnostics to help ensure that the sampling is going as it should. It is important to note that these diagnostics are a feature of HMC, not a bug. By that I mean that the absence of these diagnostics, particularly divergences, from other sampling methods means that it is harder to ensure that they are sampling properly. The ability to check that it is working properly makes HMC all the more powerful.\n\n29.3.1 Divergences\nHamiltonian Monte Carlo enables large step sizes by taking into account the shape of the target distribution and tracing trajectories along it. (This is of course a very loose description. You should read Michael Betancourt’s wonderful introduction to HMC to get a more complete picture.) When a trajectory encounters a region of parameter space where the posterior (target) distribution has high curvature, the trajectory can veer sharply. These events can be detected and are registered as divergences. A given Monte Carlo step ends in a divergence if this happens. This does not necessarily mean that there is a problem with the sample, but there is a good chance that there is.\nStan keeps track of divergences and reports them. In ArviZ InferenceData objects, they are stored in the sample_stats attribute. Let’s look first at our good samples where we properly warmed up the sampler.\n\nsamples.sample_stats.diverging\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'diverging' (chain: 4, draw: 1000)&gt; Size: 4kB\narray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [ True, False,  True, ..., False, False, False],\n       [ True, False, False, ..., False, False, False]])\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999xarray.DataArray'diverging'chain: 4draw: 1000False False False False False False ... False False False False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [ True, False,  True, ..., False, False, False],\n       [ True, False, False, ..., False, False, False]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (0)\n\n\nWe can check how many divergences we had by summing them.\n\nint(np.sum(samples.sample_stats.diverging))\n\n222\n\n\nOof! The funnel gave us 222 divergences. This is indicative of a sampler in trouble! We will deal with this momentarily.\n\n\n29.3.2 Tree depth\nThe explanation of this diagnostic is a little computer-sciencey, so you can skip to the last sentence of this section if the CS terms are unfamiliar to you.\nThe HMC algorithm used by Stan uses [recursion](https://en.wikipedia.org/wiki/Recursion_(computer_science). In practice when doing recursive calculations, you need to put a bound on how deep the recursion can go, i.e., you need to cap the tree depth, lest you get stack overflow. Stan therefore has to have a limit on tree depth, the default of which is 10. If this tree depth is hit while trying to take a sample, the sampling is not wrong, but less efficient. Stan therefore reports the tree depth information for each sample. These are also included in the sample_stats.\n\nsamples.sample_stats.tree_depth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'tree_depth' (chain: 4, draw: 1000)&gt; Size: 32kB\narray([[2, 4, 4, ..., 2, 3, 4],\n       [3, 3, 2, ..., 2, 1, 2],\n       [1, 1, 4, ..., 4, 4, 4],\n       [1, 1, 1, ..., 4, 2, 2]])\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999xarray.DataArray'tree_depth'chain: 4draw: 10002 4 4 3 3 2 3 2 2 3 3 3 4 1 3 3 4 ... 3 4 4 3 4 4 4 2 3 3 2 4 4 4 2 2array([[2, 4, 4, ..., 2, 3, 4],\n       [3, 3, 2, ..., 2, 1, 2],\n       [1, 1, 4, ..., 4, 4, 4],\n       [1, 1, 1, ..., 4, 2, 2]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (0)\n\n\nWe can look how many hit a tree depth of 10.\n\nint(np.sum(samples.sample_stats.tree_depth == 10))\n\n0\n\n\nSo, in this case, we never hit the tree depth. When we do hit the tree depth often, it typically results in a less efficient sampler and the ESS will decrease.\n\n\n29.3.3 E-BFMI\nThe energy-Bayes fraction of missing information, or E-BFMI is another metric that is specific to HMC samplers. Loosely speaking (again), it is a measure of how effective the sampler is at taking long steps. Some details are given in the Betancourt paper on HMC, and we will not go into them here, but say that as a rule of thumb, values below 0.2 can be indicative of inefficient sampling.\nStan also automatically computes the E-BFMI.\n\nsamples.sample_stats.energy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'energy' (chain: 4, draw: 1000)&gt; Size: 32kB\narray([[ 2.38275 ,  4.71404 ,  4.36306 , ...,  1.57001 ,  3.59906 ,\n         3.32988 ],\n       [ 0.717477,  2.60399 ,  3.26155 , ...,  0.396415,  2.11211 ,\n         2.62692 ],\n       [-0.789693, -0.621038,  2.87434 , ...,  3.22471 ,  1.53535 ,\n         1.90595 ],\n       [ 0.604643, -0.27291 ,  1.58246 , ...,  2.04745 ,  0.59004 ,\n         0.525632]])\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999xarray.DataArray'energy'chain: 4draw: 10002.383 4.714 4.363 4.343 2.696 3.215 ... 6.306 4.676 2.047 0.59 0.5256array([[ 2.38275 ,  4.71404 ,  4.36306 , ...,  1.57001 ,  3.59906 ,\n         3.32988 ],\n       [ 0.717477,  2.60399 ,  3.26155 , ...,  0.396415,  2.11211 ,\n         2.62692 ],\n       [-0.789693, -0.621038,  2.87434 , ...,  3.22471 ,  1.53535 ,\n         1.90595 ],\n       [ 0.604643, -0.27291 ,  1.58246 , ...,  2.04745 ,  0.59004 ,\n         0.525632]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (0)\n\n\nWe see some problematic energies; let’s do a quick check to see how many small ones we have.\n\nint(np.sum(samples.sample_stats.energy &lt; 0.2))\n\n566\n\n\nWe do have several small values, but we will not dwell on that here.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#quickly-checking-the-diagnostics",
    "href": "lessons/principled/mcmc_diagnostics.html#quickly-checking-the-diagnostics",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "29.4 Quickly checking the diagnostics",
    "text": "29.4 Quickly checking the diagnostics\nI wrote a function, based on work by Michael Betancourt, to quickly check these diagnostics for a set of samples. It is available in the bebi103.stan submodule.\n\nbebi103.stan.check_all_diagnostics(samples)\n\ntail-ESS for parameter theta is 297.52415027291516.\nESS for parameter v is 40.841114456860105.\ntail-ESS for parameter v is 17.763583583602472.\n  ESS or tail-ESS below 100 per chain indicates that expectation values\n  computed from samples are unlikely to be good approximations of the\n  true expectation values.\n\nRhat for parameter theta is 1.0538396525577298.\nRhat for parameter v is 1.0992399132159116.\n  Rank-normalized Rhat above 1.01 indicates that the chains very likely have not mixed.\n\n222 of 4000 (5.55%) iterations ended with a divergence.\n  Try running with larger adapt_delta to remove divergences.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n7\n\n\nThis is a quick check you can do to make sure everything is in order after obtaining samples. But it is very important to note that passing all of these diagnostic checks does not ensure that you achieved effective sampling. And perhaps even more importantly, getting effective sampling certainly does not guarantee that your model is a good one. Nonetheless, good, identifiable models tend to pass the diagnostic checks more often than poor ones.\nThe funnel is a tough one, and we are failing in many diagnostics.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#fixing-the-model-be-nice-to-your-sampler",
    "href": "lessons/principled/mcmc_diagnostics.html#fixing-the-model-be-nice-to-your-sampler",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "29.5 Fixing the model: Be nice to your sampler",
    "text": "29.5 Fixing the model: Be nice to your sampler\nThe diagnostics indicated several divergences, which, as I mentioned before, tend to happen in regions where the target distribution has high curvature. We also have poor effective sample sizes for the parameter \\(v\\), and the R-hats are large.\nLet’s look at a plot of the samples, overlaid with the samples we trust that we can compute by sampling directly with Numpy as we did in ?exr-funnel-of-hell. (You can click on the legend to display or hide respective samples.)\n\n# Sample out of distribution using Numpy\nnp.random.seed(3252)\nv = np.random.normal(0, 3, size=4000)\ntheta = np.random.normal(0, np.exp(v / 2))\n\np = bokeh.plotting.figure(\n    height=400, width=450, x_range=[-100, 100], x_axis_label=\"θ\", y_axis_label=\"v\"\n)\np.scatter(theta, v, alpha=0.3, color=\"#66c2a5\", legend_label=\"indep. samples\")\np.legend.location = \"bottom_left\"\n\n# Overlay MCMC samples\np.scatter(\n    samples.posterior.theta.values.flatten(),\n    samples.posterior.v.values.flatten(),\n    color=\"#fc8d62\",\n    alpha=0.3,\n    legend_label=\"default sampling\",\n)\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nStan’s sampler is clearly not penetrating to the lower regions of the funnel. If we did not have the correctly generated independent samples to compare to, we might not ever discover that this is an issue. So how can we be aware of sampling issues like this?\nFirst off, the divergences clue us in that there is a problem. We can start to investigate what the chains are doing by taking a graphical approach. We can start with the trace plot.\n\nbokeh.io.show(bebi103.viz.trace(samples, parameters=['theta', 'v']))\n\n\n  \n\n\n\n\n\nWe immediately see a pathology in the trace plot for \\(v\\). When \\(v\\) is small, the chains get stuck and keep rejecting steps. They cannot move. This is because the proposal steps keep ending in divergences and the steps cannot be taken.\nWe can look at this another way using a parallel coordinate plot. To allow for easy comparison, we will apply a transformation to \\(\\theta\\) such that we show its logarithm (of the absolute value). The function bebi103.viz.parcoord() displays divergent samples in orange.\n\nbokeh.io.show(\n    bebi103.viz.parcoord(\n        samples,\n        transformation={'theta': lambda x: np.log10(np.abs(x))},\n        divergence_kwargs={\"line_width\": 1, \"line_alpha\": 0.15},\n    )\n)\n\n\n  \n\n\n\n\n\nFrom the parallel coordinate plot, most divergences come when \\(v\\) is small and \\(\\theta\\) is close to zero, which is the bottom of the funnel. The log posterior is also high for these divergences. There is substantial probability mass in the funnel, so we do really need to sample it.\nAs an alternative plot, we can plot the divergent samples in a different color in a scatter plot of our samples. The bebi103.viz.corner() function automatically does this.\n\nbokeh.io.show(bebi103.viz.corner(samples, parameters=[\"theta\", \"v\"]))\n\n\n  \n\n\n\n\n\nThe graphical display of divergences, in particular in the colored scatter plots as above and in the parallel coordinate plot help diagnose the problem.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#conquering-the-funnel-of-hell",
    "href": "lessons/principled/mcmc_diagnostics.html#conquering-the-funnel-of-hell",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "29.6 Conquering the Funnel of Hell",
    "text": "29.6 Conquering the Funnel of Hell\nHow can we get our MCMC sampler to get deep into the funnel? The funnel is caused by the variance of the distribution of \\(\\theta\\) getting very small. This narrows the funnel and any step the sampler takes is too large such that it steps out of the funnel. We need to sample down into the funnel to get true samples out of the target distribution.\n\n29.6.1 Adjusting adapt_delta\nWe could try to take the advice of Stan’s warning messages and decrease the adapt_delta parameter to take smaller steps. The default value is 0.8, so let’s crank it up to 0.99 and see if that works.\n\nwith bebi103.stan.disable_logging():\n    samples = sm.sample(seed=3252, adapt_delta=0.99)\nsamples = az.from_cmdstanpy(samples)\n\n# Check diagnostics\nbebi103.stan.check_all_diagnostics(samples)\n\n# Add plot of samples\np.scatter(\n    samples.posterior.theta.values.flatten(),\n    samples.posterior.v.values.flatten(),\n    color=\"#8da0cb\",\n    alpha=0.3,\n    legend_label=\"small adapt_delta\",\n)\nbokeh.io.show(p)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \ntail-ESS for parameter theta is 353.4367993429466.\nESS for parameter v is 157.8386124897571.\ntail-ESS for parameter v is 333.49820097378057.\n  ESS or tail-ESS below 100 per chain indicates that expectation values\n  computed from samples are unlikely to be good approximations of the\n  true expectation values.\n\nRhat for parameter theta is 1.0183414104652264.\nRhat for parameter v is 1.0245451664600984.\n  Rank-normalized Rhat above 1.01 indicates that the chains very likely have not mixed.\n\n35 of 4000 (0.875%) iterations ended with a divergence.\n  Try running with larger adapt_delta to remove divergences.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n\n  \n\n\n\n\n\nThat helped. We have far fewer divergences. However, we are still just a bit shy of the bottom of the funnel.\n\n\n29.6.2 Noncentering\nInstead of making the sampler sample out of a distribution with tiny variance, we can make it sample out of a distribution that has a more reasonable variance, and then apply a transformation to those samples to get samples from the tiny variance distribution. To devise a strategy for doing this, we use the change of variables formula for probability distributions. Imagine we have a probability distribution of \\(\\theta\\) with probability density function \\(\\pi(\\theta)\\). If we wish to instead had a probability density function of another variable \\(\\tilde{\\theta}\\), which we can express as a function of \\(\\theta\\), \\(\\tilde{\\theta} = \\tilde{\\theta}(\\theta)\\), we need to ensure that \\(\\pi(\\tilde{\\theta})\\) is normalized,\n\\[\\begin{align}\n\\int \\mathrm{d}\\tilde{\\theta}\\,\\pi(\\tilde{\\theta}) = 1.\n\\end{align}\\]\nTo relate this integral to the integral of \\(\\pi(\\theta)\\), we need to properly change variables in the integral. This leads to the change of variables formula,\n\\[\\begin{align}\n\\pi(\\tilde{\\theta}) = \\left|\\frac{\\mathrm{d}\\theta}{\\mathrm{d}\\tilde{\\theta}}\\right|\\,\\pi(\\theta).\n\\end{align}\\]\nNow, if we choose\n\\[\\begin{align}\n\\tilde{\\theta} = \\frac{\\theta - \\mu}{\\sigma},\n\\end{align}\\]\nthen\n\\[\\begin{align}\n\\left|\\frac{\\mathrm{d}\\theta}{\\mathrm{d}\\tilde{\\theta}}\\right| = \\sigma\n\\end{align}\\]\nand\n\\[\\begin{align}\n\\pi(\\tilde{\\theta}) = \\sigma \\pi(\\theta).\n\\end{align}\\]\nIf \\(\\theta\\) is Normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we have\n\\[\\begin{align}\n\\pi(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\mathrm{e}^{-(\\theta-\\mu)^2/2\\sigma^2}.\n\\end{align}\\]\nThen, to satisfy the change of variables formula,\n\\[\\begin{align}\n\\pi(\\tilde{\\theta}) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-\\tilde{\\theta}^2/2}.\n\\end{align}\\]\nThis means that \\(\\tilde{\\theta} \\sim \\text{Norm}(0, 1)\\). Thus, we can reparametrize using the fact that \\(\\theta \\sim \\text{Norm}(\\mu, \\sigma)\\) is equivalent to\n\\[\\begin{align}\n&\\tilde{\\theta} \\sim \\text{Norm}(0, 1),\\\\[1em]\n&\\theta = \\mu + \\sigma\\,\\tilde{\\theta}.\n\\end{align}\\]\nSo, in our case, we can instead sample using \\(\\tilde{\\theta}\\) with\n\\[\\begin{align}\n&\\tilde{\\theta} \\sim \\text{Norm}(0, 1),\\\\[1em]\n&\\theta = \\mathrm{e}^{v/2}\\,\\tilde{\\theta}.\n\\end{align}\\]\nThis process is called uncentering. A non-centered parametrization has the sampler exploring away from the mean of the target distribution (hence, it is non-centered), and then a transformation ensures that the samples come from the target.\nLet’s implement the non-centered parametrization of this pathological distribution in Stan. The Stan code is\nparameters {\n  real theta_tilde;\n  real v; \n}\n\n\ntransformed parameters {\n  real theta = exp(v/2) * theta_tilde;\n}\n\n\nmodel {\n  v ~ normal(0, 3);\n  theta_tilde ~ normal(0, 1);\n}\nLet’s compile and sample. We won’t bother adjusting adapt_delta; we’ll just see what we get.\n\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='funnel_noncentered.stan')\n    samples = sm.sample(seed=3252)\n\nsamples = az.from_cmdstanpy(samples)\n\nbebi103.stan.check_all_diagnostics(samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\n\n\nExcellent! No divergences and all diagnostics check out. Let’s overlay a plot of the samples to see if we got the whole funnel.\n\np.scatter(\n    samples.posterior.theta.values.flatten(),\n    samples.posterior.v.values.flatten(),\n    color=\"#e78ac3\",\n    alpha=0.3,\n    legend_label=\"non-centered\",\n)\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nLook at that! We have managed to sample all the way down the funnel! We have conquered the Funnel of Hell.\n\n\n29.6.3 Hierarchical models feature a Funnel of Hell\nIt turns out that many hierarchical models feature a Funnel of Hell, so uncentering is often crucial. We will explore this in the future lessons.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/mcmc_diagnostics.html#computing-environment",
    "href": "lessons/principled/mcmc_diagnostics.html#computing-environment",
    "title": "29  MCMC diagnostics via a case study: Artificial funnel of hell",
    "section": "29.7 Computing environment",
    "text": "29.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,cmdstanpy,bokeh,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\ncmdstanpy : 1.2.5\nbokeh     : 3.6.2\nbebi103   : 0.1.27\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>MCMC diagnostics via a case study: Artificial funnel of hell</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc.html",
    "href": "lessons/principled/sbc.html",
    "title": "30  Principled analysis pipelines",
    "section": "",
    "text": "30.1 Building a workflow\n| Download notebook\nThis lesson is based heavily on this article by Michael Betancourt.\nWe have talked at length about Bayesian model building. The steps were roughly as follows.\nWe do have a bit missing in this work flow. There are other checks we should do with our modeling and inference procedure to ensure we will get reliable statistical results. First, we should use simulated data for which the ground truth (the actual parameter values) is known, and verify that our inference procedure can capture the ground truth. We should ensure this is the case for any conceivable data set we throw at our inference procedure. Second, we should make sure that our model is such that we can actually learn from the experiment. That is, we should ensure that the posterior is more concentrated around the ground truth than the prior.\nIn this lecture, we will discuss strategies to approach these two problems.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Principled analysis pipelines</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc.html#building-a-workflow",
    "href": "lessons/principled/sbc.html#building-a-workflow",
    "title": "30  Principled analysis pipelines",
    "section": "",
    "text": "Propose a simple generative model.\nPerform prior predictive checks.\nIterate on 1 and 2 until satisfied with prior predictive checks.\nPerform MCMC to get samples out of the posterior.\nPerform diagnostics to make sure the sampling worked ok.\nIterate on 4 and 5, possibly uncentering and/or changing sampling settings, until satisfied with the diagnostics. If not satisfied, diagnose the problems with the model and go back to (1) and proceed.\nPerform posterior predictive checks.\nIf unsatisfied, update the model with more flexibility. Make sure the updated model has the previous model as a special case or limit. Go to (2).\nIf satisfied, stop and report results.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Principled analysis pipelines</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc.html#references-and-terminology",
    "href": "lessons/principled/sbc.html#references-and-terminology",
    "title": "30  Principled analysis pipelines",
    "section": "30.2 References and terminology",
    "text": "30.2 References and terminology\nI will first describe simulation-based calibration (SBC). I encourage you to read more about how it works, starting with the Stan documentation. You should also refer to the original paper on SBC by Talts and coworkers. Finally, you should read this article by Michael Betancourt for a more complete treatment of what I present in this lesson.\nThe term “simulation-based calibration” is meant to describe the self-consistency check using rank statistics described below. I will be using the term here as an umbrella term for SBC and sensitivity analysis in computing shrinkages and z-scores, also described below. The computational effort for all of these is the same, and they all use the same samples, so I hope this abuse of terminology is forgivable.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Principled analysis pipelines</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc.html#simulation-based-calibration",
    "href": "lessons/principled/sbc.html#simulation-based-calibration",
    "title": "30  Principled analysis pipelines",
    "section": "30.3 Simulation-based calibration",
    "text": "30.3 Simulation-based calibration\nWe have performed prior predictive checks on our generative models to ensure that the models produce reasonable data sets. Accepting the model after the prior predictive check, we sample out of the posterior using Markov chain Monte Carlo. We check the samples by performing diagnostics, and then check the ability of the model to generate the actual data set we obtained by doing posterior predictive checks. Importantly, when we look at the posterior, we can compare it to the prior to make sure that the data actually were informative. If the posterior looks too much like the prior, we really have not learned anything. The process is quite powerful, but it would be nice to know if our model makes sense and if we will learn from making measurements before we perform the experiments. Specifically, we would like to know, ante experimentum,\n\nCan our model and sampling procedure capture the real parameter values of the generative process?\nCan measured data inform our knowledge of the parameters?\nCan our sampling technique handle the model for any conceivable data set we can throw at it?\n\nThe approach of simulation-based calibration addresses these questions. The procedure for generated data to use in SBC is as follows.\n\nDraw a parameter set \\(\\tilde{\\theta}\\) out of the prior.\nUse \\(\\tilde{\\theta}\\) to draw a data set \\(\\tilde{y}\\) out of the likelihood.\nPerform MCMC sampling of the posterior using \\(\\tilde{y}\\) as if it were the actual measured data set. Draw \\(L\\) MCMC samples of the parameters.\nDo steps 1-3 \\(N\\) times (hundreds to a thousand is usually a good number).\n\nIn step 3, we are using a data set for which we know the underlying parameters that generated it. Because the data were generated using \\(\\tilde{\\theta}\\) as the parameter set, \\(\\tilde{\\theta}\\) is now the ground truth parameter set. So, we can check to see if we uncover the ground truth in the posterior sampling. We can also check diagnostics for each of the trials to make sure the sampler works properly. Furthermore, we can see if the posterior is narrower than the prior, meaning that the data are informing the posterior. We can also do a check, described below, to ensure the sampler is working properly, which is the main idea in the SBC paper by Talts, et al.\n\n30.3.1 Diagnostics\nFor each of your \\(N\\) MCMC calculations, you should perform diagnostics. This will tell you if your sampler is having any obvious problems. It may be the case that some data sets will give it problems while other won’t, and the SBC procedure will help you identify that. If you do this before you analyze your real data, you will know something about what problems you might expect in your analysis.\n\n\n30.3.2 z-score\nTo check to see if the posterior encompasses the ground truth, we can compute a z-score for each parameter \\(\\theta_i\\). The z-score is a measure of how close the mean sampled parameter value is to the ground truth, relative to the posterior uncertainty in the parameter value. For each parameter \\(\\theta_i\\),\n\\[\\begin{align}\nz_i = \\frac{\\langle\\theta_i\\rangle_{\\mathrm{post}} - \\tilde{\\theta}_i}{\\sigma_{i,\\mathrm{post}}}.\n\\end{align}\\]\nHere, \\(\\langle\\theta_i\\rangle_{\\mathrm{post}}\\) is the average value of \\(\\theta_i\\) over all posterior samples, and \\(\\sigma_{i,\\mathrm{post}}\\) is the standard deviation of \\(\\theta_i\\) over all posterior samples. As a rule of thumb, the z-score should be symmetric about zero (indicating that there is no bias in under-or-overestimating the ground truth) and should have a magnitude less than four or five. If the z-score is much above that, it is a sign of overfitting, putting posterior probability mass away from the ground truth.\n\n\n30.3.3 Shrinkage\nTo have a metric for how informative the data are in the posterior, we define the shrinkage for parameter \\(\\theta_i\\) to be\n\\[\\begin{align}\ns_i = 1 - \\frac{\\sigma_{i,\\mathrm{post}}^2}{\\sigma_{i,\\mathrm{prior}}^2}.\n\\end{align}\\]\nHere, \\(\\sigma_{i,\\mathrm{prior}}^2\\) is the variance in parameter \\(\\theta_i\\) in the prior distribution. When the posterior is substantially narrower than the prior (meaning that the data have informed the model), the shrinkage tends toward unity since the width of the marginal posterior distribution is much less than that of the prior distribution, or \\(\\sigma_{i,\\mathrm{post}} \\ll \\sigma_{i,\\mathrm{prior}}\\).\n\n\n30.3.4 Shrinkage vs. z-score plot\nTo graphically assess the shrinkage and z-score, we typically make a shrinkage vs. z-score plot. Each point in the plot represents the results from one of the \\(N\\) data sets drawn from the generative model. A typical shrinkage vs. z-score plot for a model with two parameters \\(\\alpha\\) and \\(b\\) is shown below.\n\n\n\nz-score and shrinkage as a check for the posterior capturing the ground truth and being more informative than the prior.\n\n\nNote that the z-scores are symmetric about zero and do not range much above three. The shrinkage is mostly close to unity, and those that drift away do not drift too far. This model permits learning from the data.\n\n\n30.3.5 Rank statistics\nWe additionally want to check to make sure the sampler is working properly. That is, in our sampling procedure, we want to make sure we will actually sample the posterior, getting good coverage over parameter space.\nIn order to check to make sure the sampler is working properly, we can check for self-consistency for a relation that must hold for any statistical model. To derive such a relation, we consider a joint distribution \\(\\pi(\\theta, \\tilde{y}, \\tilde{\\theta})\\). Then,\n\\[\\begin{align}\ng(\\theta) &= \\int\\mathrm{d}\\tilde{y}\\,\\int\\mathrm{d}\\tilde{\\theta}\\,\\pi(\\theta, \\tilde{y}, \\tilde{\\theta})\n= \\int\\mathrm{d}\\tilde{y}\\,\\int\\mathrm{d}\\tilde{\\theta}\\,g(\\theta \\mid \\tilde{y}, \\tilde{\\theta}) \\,\\pi(\\tilde{y},\\tilde{\\theta}),\n\\end{align}\\]\nwhere we have used the definition of conditional probability. We note that\n\\[\\begin{align}\ng(\\theta \\mid \\tilde{y}, \\tilde{\\theta}) = g(\\theta \\mid \\tilde{y})\n\\end{align}\\]\nbecause \\(\\theta\\) is conditioned directly on \\(\\tilde{y}\\) and not directly on \\(\\tilde{\\theta}\\), though \\(\\tilde{y}\\) is conditioned on \\(\\tilde{\\theta}\\). Using this latter conditioning,\n\\[\\begin{align}\n\\pi(\\tilde{y},\\tilde{\\theta}) = f(\\tilde{y}\\mid \\tilde{\\theta})\\,g(\\tilde{\\theta}).\n\\end{align}\\]\nInserting these expressions into the above integral yields\n\\[\\begin{align}\ng(\\theta) = \\int\\mathrm{d}\\tilde{y}\\,\\int\\mathrm{d}\\tilde{\\theta}\\,g(\\theta \\mid \\tilde{y}) \\,f(\\tilde{y}\\mid \\tilde{\\theta})\\,g(\\tilde{\\theta}).\n\\end{align}\\]\nIn English, the right hand side is the expectation of the posterior over all possible generated data sets. We refer to the right hand side as the data averaged posterior. The left hand side is the prior. For good sampling, there should be no discrepancies between the data averaged posterior and the prior.\nThis equation says that if we average the generated data sets over the posterior distribution, we recover the prior. This means that the prior sample, \\(\\tilde{\\theta}\\), and the \\(L\\) posterior samples \\(\\theta\\), are sampled out of the same distribution. In other words, if we follow the procedure above for SBC, we should get samples out of the prior.\nTalts and coworkers derived a general theorem which says that any the rank statistic of any variable sampled over \\(\\theta\\) is Uniformly distributed. So, we can compute a rank statistic for \\(\\tilde{\\theta}\\) over the \\(L\\) \\(\\theta\\) samples we have, and this rank statistic should be uniformly distributed on the interval \\([0, L]\\). Since we did a few hundred to a thousand samples, we can check Uniformity of the rank statistics by plotting ECDFs of the rank statistic.\n\n\n30.3.6 A rank statistic ECDF plot\nWe can make an ECDF difference plot to test for uniformity. Such a plot is shown below.\n\n\n\nRank statistic plot of the ECDF difference as a test for uniformity.\n\n\nThe y-axis is the difference between the ECDF of the \\(N\\) rank statistics and that of a Uniform distribution. The surrounding envelope is the confidence interval of the ECDF difference. If the actual ECDF difference is too much outside of the envelope, then the data averaged posterior and the prior are not equal, and there is a problem with sampling. In the case above, for each of the two parameters, the sampler seems to be performing ok.\n\n\n30.3.7 Rank statistic histograms\nBy constructing rank statistic histograms, we can diagnose how the the sampling is failing to capture the true posterior. The series of graphics in Figs. 3-7 of the Talts paper provide an excellent description of how you can perform this diagnosis.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Principled analysis pipelines</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc.html#a-full-principled-pipeline",
    "href": "lessons/principled/sbc.html#a-full-principled-pipeline",
    "title": "30  Principled analysis pipelines",
    "section": "30.4 A full principled pipeline",
    "text": "30.4 A full principled pipeline\nThe SBC calculation is expensive. If you want to generate \\(N = 1000\\) data sets and perform parameter estimation on them, the calculation cost is 1000 times that of performing the parameter estimation on your real data set. However, this cost comes with major benefits. By comparing against a ground truth, you can see if your model can find it for you. By checking the shrinkage, you can see if you can learn from your data. By checking the rank statistics, you can see if the sampler will properly sample the true posterior. And by checking the diagnostics, you will know if there will be any pathological problems.\nSo, we update the analysis pipeline to include these SBC-based analyses.\n\nPropose a simple generative model.\nPerform prior predictive checks.\nIterate on 1 and 2 until satisfied with prior predictive checks.\nPerform SBC calculations.\nAnalyze (by hand and with graphical methods) diagnostics, z-scores, shrinkage, and rank statistics.\nIf you not pleased with the results of (5), adjust your model and go to (2). Otherwise, continue.\nPerform MCMC to get samples out of the posterior.\nPerform diagnostics to make sure the sampling worked ok with the real data.\nIterate on 7 and 8, possibly uncentering and/or changing sampling settings, until satisfied with the diagnostics. If not satisfied, diagnose the problems with the model and go back to (1) and proceed. Note that if you do uncentering, you may wish to run though the SBC procedure again.\nPerform posterior predictive checks.\nIf unsatisfied, update the model with more flexibility. Make sure the updated model has the previous model as a special case or limit. Go to (2).\nIf satisfied, stop and report results.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Principled analysis pipelines</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc_in_practice.html",
    "href": "lessons/principled/sbc_in_practice.html",
    "title": "31  Simulation based calibration and related checks in practice",
    "section": "",
    "text": "31.1 ECDFs of mRNA counts\n| Download notebook\nData set download\nYou should set the value of the variable cores to be the number of cores you have available on your machine. I will be using 9 cores in this notebook.\nIn Chapter 30, we laid out the a principled pipeline for constructing and testing a generative model and associated inference procedures. In this lesson, we work through the implementation of the principled pipeline on a familiar data set. For this analysis, we will consider single cell RNA transcript counts measured via single-molecule FISH from this paper from the Elowitz lab. We will specifically look at transcript count of mRNA for the rest gene. You can download the data set here.\nLet’s go ahead and load the data. In our analysis here, we will use the Rest gene.\n# Load DataFrame\ndf = pl.read_csv(os.path.join(data_path, 'singer_transcript_counts.csv'), comment_prefix='#')\n\n# Pull out data for Stan\nn = df['Rest'].to_numpy()\ndata = dict(N=len(n), n=n)\n\n# Take a look\nbokeh.io.show(iqplot.ecdf(n, x_axis_label='mRNA count'))",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation based calibration and related checks in practice</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc_in_practice.html#the-generative-model",
    "href": "lessons/principled/sbc_in_practice.html#the-generative-model",
    "title": "31  Simulation based calibration and related checks in practice",
    "section": "31.2 The generative model",
    "text": "31.2 The generative model\nAs is commonly the case, we will use a Negative Binomial likelihood (which has both a theoretical and empirical justification) for the distribution of transcript counts. In the context of mRNA counts, we usually parametrize the Negative Binomial by the burst size \\(b\\) and the burst frequency \\(\\alpha\\). We have the following generative model.\n\\[\\begin{align}\n&\\log_{10} \\alpha \\sim \\text{Norm}(0, 1),\\\\[1em]\n&\\log_{10} b \\sim \\text{Norm}(2, 1),\\\\[1em]\n&\\beta = 1/b,\\\\[1em]\n&n_i \\sim \\text{NegBinom}(\\alpha, \\beta) \\;\\forall i.\n\\end{align}\\]\nWe can code up prior predictive checks and the model in Stan. First, the prior predictive checks.\ndata {\n  int N;\n}\n\n\ngenerated quantities {\n  array[N] int n;\n\n  real log10_alpha = normal_rng(0.0, 1.0);\n  real log10_b = normal_rng(2.0, 1.0);\n  real alpha = 10^log10_alpha;\n  real b = 10^log10_b;\n  real beta_ = 1 / b;\n\n  for (i in 1:N) {\n    n[i] = neg_binomial_rng(alpha, beta_);\n  }\n}\nAnd also the model.\ndata {\n  int N;\n  array[N] int n;\n}\n\n\nparameters {\n  real&lt;lower=0&gt; log10_alpha;\n  real&lt;lower=0&gt; log10_b;\n}\n\n\ntransformed parameters {\n  real alpha = 10^log10_alpha;\n  real b = 10^log10_b;\n  real beta_ = 1.0 / b;\n}\n\n\nmodel {\n  // Priors\n  log10_alpha ~ normal(0.0, 1.0);\n  log10_b ~ normal(2.0, 1.0);\n\n  // Likelihood\n  n ~ neg_binomial(alpha, beta_);\n}\nFor now, we are not going to bother with posterior predictive checks or computing the log likelihood.\nLet’s compile the models.\n\nwith bebi103.stan.disable_logging():\n    sm_prior_pred = cmdstanpy.CmdStanModel(stan_file='prior_pred.stan')\n    sm = cmdstanpy.CmdStanModel(stan_file='model.stan')\n\nWe can now perform prior predictive checks. We will plot the resulting checks as ECDFs so we can see how the mRNA counts are distributed. For the plot, to avoid choking the browser, we will only plot 100 ECDFS.\n\nwith bebi103.stan.disable_logging():\n    samples_prior_pred = sm_prior_pred.sample(\n        data=data, fixed_param=True, chains=1, iter_sampling=1000\n    )\n\nsamples_prior_pred = az.from_cmdstanpy(\n    posterior=samples_prior_pred, prior=samples_prior_pred, prior_predictive=\"n\"\n)\n\np = None\nfor n in samples_prior_pred.prior_predictive.n.squeeze()[::10]:\n    p = iqplot.ecdf(\n        n, marker_kwargs=dict(fill_alpha=0.2, line_alpha=0.2), p=p, x_axis_type=\"log\"\n    )\n    \np.x_range = bokeh.models.Range1d(0.3, 3e5)\n\nbokeh.io.show(p)\n\n\n\n\n                                                                                \n\n\n\n  \n\n\n\n\n\nWe can also plot the mean and variance of all of the generated data sets as to further characterize the prior predictive distribution.\n\nmeans = samples_prior_pred.prior_predictive.n.squeeze().mean(axis=1).values\nvariances = samples_prior_pred.prior_predictive.n.squeeze().var(axis=1).values\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=250,\n    x_axis_label='mean of counts',\n    y_axis_label='variance of counts',\n    x_axis_type='log',\n    y_axis_type='log',\n    x_range=[1, np.nanmax(means)],\n    y_range=[1, np.nanmax(variances)],\n)\n\np.scatter(means, variances, size=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis also makes sense. We get Poissonian behavior (mean = variance) for some samples, and then a range of dispersion beyond that.\nThe prior predictive checks show a wide range of mRNA counts, and all seem reasonable. We do get some large number of counts, upwards of 10,000, considering that the typical total mRNA count in a mammalian cell is about 100,000. But this is not dominant, and we get good coverage over what we might expect, so this seems like a pretty good prior.",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation based calibration and related checks in practice</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc_in_practice.html#performing-sbc",
    "href": "lessons/principled/sbc_in_practice.html#performing-sbc",
    "title": "31  Simulation based calibration and related checks in practice",
    "section": "31.3 Performing SBC",
    "text": "31.3 Performing SBC\nPerforming SBC really only requires a few ingredients. First, we need the requisite data to be used for prior predictive checks. In this case, it is just the number of measurements we are making, \\(N\\). Second, we need a Stan model to generate the prior predictive data sets. Finally, we need a Stan model to sample out of the posterior. The bebi103.stan.sbc() function will then perform SBC and give the results back in a data frame. That is, it will draw a prior predictive data set, use that data set in a posterior sampling by MCMC calculation, and then compute the useful diagnostics and statistics (z-score, shrinkage, and rank statistic) from those samples. It does this N times (not to be confused with \\(N\\), the number of measurements in the experiment). Let’s now put it to use to perform SBC.\n\ndf_sbc = bebi103.stan.sbc(\n    prior_predictive_model=sm_prior_pred,\n    posterior_model=sm,\n    prior_predictive_model_data=data,\n    posterior_model_data=data,\n    measured_data=[\"n\"],\n    var_names=[\"alpha\", \"b\"],\n    measured_data_dtypes=dict(n=int),\n    cores=cores,\n    N=1000,\n    progress_bar=True,\n)\n\n100%|███████████████████████████████████████████████████████████████| 1000/1000 [01:35&lt;00:00, 10.44it/s]\n\n\nThe bebi103.stan.sbc() function gives a data frame with the SBC analysis results. Let’s take a look at the data frame to see what it has.\n\ndf_sbc.head()\n\n\nshape: (5, 19)\n\n\n\nground_truth\nrank_statistic\nmean\nsd\nshrinkage\nz_score\nRhat\nESS\nESS_per_iter\ntail_ESS\ntail_ESS_per_iter\nn_divergences\nn_bad_ebfmi\nn_max_treedepth\nwarning_code\nL\ntrial\nerror\nparameter\n\n\nf64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n13.5207\n1858\n13.682823\n1.392529\n0.9999\n0.116423\n1.007307\n511.837864\n0.127959\n600.47559\n0.150119\n0\n0\n0\n0\n4000\n0\n\"no error\"\n\"alpha\"\n\n\n0.270224\n0\n1.00808\n0.007843\n1.0\n94.076107\n1.001174\n1948.391645\n0.487098\n1318.471856\n0.329618\n1\n0\n0\n4\n4000\n1\n\"no error\"\n\"alpha\"\n\n\n15.0164\n562\n16.570134\n1.463665\n0.99989\n1.061536\n1.007255\n602.691759\n0.150673\n559.55433\n0.139889\n0\n0\n0\n0\n4000\n2\n\"no error\"\n\"alpha\"\n\n\n0.133584\n0\n1.004979\n0.004816\n1.0\n180.930956\n1.004775\n1376.480434\n0.34412\n1145.561864\n0.28639\n0\n0\n0\n0\n4000\n3\n\"no error\"\n\"alpha\"\n\n\n1.8491\n417\n2.06805\n0.181024\n0.999998\n1.20951\n1.002129\n866.505714\n0.216626\n948.48432\n0.237121\n0\n0\n0\n0\n4000\n4\n\"no error\"\n\"alpha\"\n\n\n\n\n\n\nFor each trial, for each parameter, we get diagnostic results, z-scores, shrinkage, rank statistic, posterior mean and standard deviations for each ground truth, as well as the ground truth used in the posterior sampling. The warning_code column gives a succinct summary of the diagnostic warnings. You can parse a warning code using the bebi103.stan.parse_warning_code() function. As an example, I’ll test it on warning code 14.\n\nbebi103.stan.parse_warning_code(14)\n\nRhat warning\ndivergence warning\ntreedepth warning\n\n\nTo visualize the results of SBC, we can first make a plot of the z-scores and of shrinkage. Ideally, the shrinkage should all be close to one, and the magnitude of the z-scores should all be less than five. Let’s take a look!\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=300,\n    x_axis_label=\"shrinkage\",\n    y_axis_label=\"z-score\",\n    tooltips=[(\"trial\", \"@trial\")],\n)\n\nfor color, ((parameter,), sub_df) in zip(\n    [\"#1f77b4\", \"orange\"], df_sbc.group_by(\"parameter\")\n):\n    p.scatter(\n        source=sub_df[[\"shrinkage\", \"z_score\", \"trial\"]].to_dict(),\n        x=\"shrinkage\",\n        y=\"z_score\",\n        size=2,\n        color=color,\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nOof! We are severely overfitting the model, as evidenced by z-scores of very large magnitude. We are missing the ground truth.\nTo diagnose why, let’s look at which samples have reasonable z-scores. We’ll make a strip plot categorizing the results of SBC by parameter and whether or not the z-score is good.\n\ndf_sbc = df_sbc.with_columns((pl.col('z_score').abs() &lt; 5).alias('good_z'))\np = iqplot.strip(\n    df_sbc,\n    cats=['parameter', 'good_z'],\n    color_column='good_z',\n    order=(('alpha', True), ('alpha', False), ('b', True), ('b', False)), \n    q='ground_truth',\n    x_axis_type='log',\n    spread='jitter',\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nMost strikingly, the z-score is poor for \\(\\alpha &lt; 1\\). Recall that for a Negative Binomial distribution, the mean is \\(\\alpha b\\). So, when \\(\\alpha\\) is small, the mean can be less than one, meaning that most of the counts generated by the model are zero. It makes sense, then, that we will miss the ground truth, since the data are almost all zero; there is nothing to properly inform the posterior.\nThis immediately identifies a possible problem with our inference pipeline. If a data set comes through with mostly zero measurements, we will not be able to make reliable inferences. SBC has thus identified a problem area look out for when doing our inference.\nHaving a typical burst size less than one is actually unphysical, since no transcripts are created. To be “on,” we would need to make at least one transcript. So, the SBC has exposed a problem in our modeling that we didn’t see before. Not only can the data fail to inform the prior for these parameter values, we have also discovered that our model can give unphysical parameter values. We will abort continued analysis of our SBC results and instead adapt our model.\n\n31.3.1 An adjusted prior\nI would expect the time between bursts to be of order minutes, since that is a typical response time to signaling of a cell. This is of the same order of magnitude of an RNA lifetime, so I might then expect \\(\\alpha\\) to be of order unity.\n\\[\\begin{align}\n\\alpha \\sim \\text{Gamma}(1.25, 0.1).\n\\end{align}\\]\nWe can make a quick plot.\n\nalpha = np.linspace(0, 50, 200)\ng = st.gamma.pdf(alpha, 1.25, loc=0, scale=1/0.1)\n\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=200,\n    x_axis_label='α',\n    y_axis_label='g(α)',\n)\n\np.line(alpha, g, line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is still pretty broad and pushes some of the prior probability mass away from zero.\nTurning now to the burst size, I would expect \\(b\\) to depend on promoter strength and/or strength of transcriptional activators. I could imagine anywhere from a few to several thousand transcripts per burst.\n\\[\\begin{align}\nb \\sim \\text{Gamma}(2, 0.002).\n\\end{align}\\]\nAgain, with a plot.\n\nb = np.linspace(0, 5000, 200)\ng = st.gamma.pdf(b, 2, loc=0, scale=1/0.002)\n\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=200,\n    x_axis_label='b',\n    y_axis_label='g(b)',\n)\n\np.line(b, g, line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis prior moves \\(b\\) off of zero, which we saw was problematic in our previous prior. The Gamma prior also decays faster than our original Log-Normal prior, which ended up getting us very large burst sizes. We then have the following model.\n\\[\\begin{align}\n&\\alpha \\sim \\text{Gamma}(1.25, 0.1), \\\\[1em]\n&b \\sim \\text{Gamma}(2, 0.002), \\\\[1em]\n&\\beta = 1/b,\\\\[1em]\n&n_i \\sim \\text{NegBinom}(\\alpha, \\beta) \\;\\forall i.\n\\end{align}\\]\nWe can code this model up and check the prior predictive checks. The Stan code is as follows.\ndata {\n  int N;\n}\n\n\ngenerated quantities {\n  array[N] int n;\n\n  real alpha = gamma_rng(1.25, 0.1);\n  real b = gamma_rng(2.0, 0.002);\n  real beta_ = 1.0 / b;\n  \n  for (i in 1:N) {\n    n[i] = neg_binomial_rng(alpha, beta_);\n  }\n}\nLet’s get some samples and look at the ECDFs of the copy numbers again.\n\nwith bebi103.stan.disable_logging():\n    sm_prior_pred_2 = cmdstanpy.CmdStanModel(stan_file='prior_pred_2.stan')\n    samples_prior_pred = sm_prior_pred_2.sample(\n        data=data, fixed_param=True, chains=1, iter_sampling=1000\n    )\n\nsamples_prior_pred = az.from_cmdstanpy(\n    posterior=samples_prior_pred, prior=samples_prior_pred, prior_predictive=\"n\"\n)\n\np = None\nfor n in samples_prior_pred.prior_predictive.n.squeeze()[::10]:\n    p = iqplot.ecdf(\n        n, marker_kwargs=dict(fill_alpha=0.2, line_alpha=0.2), p=p, x_axis_type=\"log\",\n        x_range=[0.3, 1e6]\n    )\n\nbokeh.io.show(p)\n\n\n\n\n                                                                                \n\n\n\n  \n\n\n\n\n\nMost of the data sets have reasonable ECDFs. Importantly, we see that the most number of zeros we get in any one data set is about 60% or so of the counts. These data sets again seem to match our intuition. Let’s check the mean and variance of transcript counts.\n\nmeans = samples_prior_pred.prior_predictive.n.squeeze().mean(axis=1).values\nvariances = samples_prior_pred.prior_predictive.n.squeeze().var(axis=1).values\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=250,\n    x_axis_label='mean of counts',\n    y_axis_label='variance of counts',\n    x_axis_type='log',\n    y_axis_type='log',\n    x_range=[1, np.nanmax(means)],\n    y_range=[1, np.nanmax(variances)],\n)\n\np.scatter(means, variances, size=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks good. We can now code up the Stan model and run SBC on this, hopefully improved, model. We will now include posterior predictive checks because we will ultimately use this model. The Stan code is as follows.\ndata {\n  int N;\n  array[N] int n;\n}\n\n\nparameters {\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; b;\n}\n\n\ntransformed parameters {\n  real beta_ = 1.0 / b;\n}\n\n\nmodel {\n  // Priors\n  alpha ~ gamma(1.25, 0.1);\n  b ~ gamma(2.0, 0.002);\n\n  // Likelihood\n  n ~ neg_binomial(alpha, beta_);\n}\n\n\ngenerated quantities {\n  array[N] int n_ppc;\n  for (i in 1:N) {\n    n_ppc[i] = neg_binomial_rng(alpha, beta_);\n  }\n}\nLet’s compile!\n\nwith bebi103.stan.disable_logging():\n    sm_2 = cmdstanpy.CmdStanModel(stan_file='model_2.stan')\n\nAnd now we can conduct SBC with this updated model. Because we have posterior predictive checks, we need to make sure to tell bebi103.stan.sbc() which variables are posterior predictive (or log likelihood, though we do not have that in this model).\n\ndf_sbc = bebi103.stan.sbc(\n    prior_predictive_model=sm_prior_pred_2,\n    posterior_model=sm_2,\n    prior_predictive_model_data=data,\n    posterior_model_data=data,\n    measured_data=[\"n\"],\n    var_names=[\"alpha\", \"b\"],\n    measured_data_dtypes=dict(n=int),\n    posterior_predictive_var_names=[\"n_ppc\"],\n    cores=cores,\n    N=1000,\n    progress_bar=True,\n)\n\n100%|███████████████████████████████████████████████████████████████| 1000/1000 [02:47&lt;00:00,  5.98it/s]\n\n\nThis time, let’s check the diagnostics first. We can get the count of each warning type.\n\n# Divide by two because diagnostics are listed for each parameter\ndf_sbc.group_by('warning_code').len().with_columns(pl.col('len') // 2)\n\n\nshape: (4, 2)\n\n\n\nwarning_code\nlen\n\n\ni64\nu32\n\n\n\n\n2\n85\n\n\n3\n6\n\n\n0\n905\n\n\n1\n4\n\n\n\n\n\n\nWe have two warning types, type 1 (ESS warning) and type 2 (Rhat warning). (A type 3 warning is both Rhat and ESS.) To deal with these, we can increase the number of iterations we take. Note that this is an important feature of performing these SBC calculations; we can see what kinds of difficulties we might encounter in our sampling.\n\ndf_sbc = bebi103.stan.sbc(\n    prior_predictive_model=sm_prior_pred_2,\n    posterior_model=sm_2,\n    prior_predictive_model_data=data,\n    posterior_model_data=data,\n    measured_data=[\"n\"],\n    var_names=[\"alpha\", \"b\"],\n    measured_data_dtypes=dict(n=int),\n    posterior_predictive_var_names=['n_ppc'],\n    sampling_kwargs=dict(iter_warmup=2000, iter_sampling=2000),\n    cores=cores,\n    N=1000,\n    progress_bar=True,\n)\n\n100%|███████████████████████████████████████████████████████████████| 1000/1000 [05:36&lt;00:00,  2.97it/s]\n\n\nLet’s again check the diagnostics.\n\n# Divide by two because diagnostics are listed for each parameter\ndf_sbc.group_by('warning_code').len().with_columns(pl.col('len') // 2)\n\n\nshape: (2, 2)\n\n\n\nwarning_code\nlen\n\n\ni64\nu32\n\n\n\n\n2\n2\n\n\n0\n998\n\n\n\n\n\n\nOur diagnostics are much better! Now, let’s make a plot of the z-score versus shrinkage.\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=300,\n    x_axis_label=\"shrinkage\",\n    y_axis_label=\"z-score\",\n    tooltips=[(\"trial\", \"@trial\")],\n)\n\nfor color, ((parameter,), sub_df) in zip(\n    [\"#1f77b4\", \"orange\"], df_sbc.group_by(\"parameter\")\n):\n    p.scatter(\n        source=sub_df[[\"shrinkage\", \"z_score\", \"trial\"]].to_dict(),\n        x=\"shrinkage\",\n        y=\"z_score\",\n        size=2,\n        color=color,\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe have good z-scores for all trials, and decent shrinkage. This all looks good. Let’s now do the self-consistency check with the rank statistic. Recall that the rank statistics should be Uniformly distributed. Therefore, the ECDFs of the rank statistics should fall on a diagonal line. When we plot the ECDF, we can also plot an envelope which encompasses the 99% confidence interval for the ECDF of a Uniformly distributed random variable.\n\nbokeh.io.show(bebi103.viz.sbc_rank_ecdf(df_sbc, diff=False))\n\n\n  \n\n\n\n\n\nIt looks like the rank statistic is Uniformly distributed. We can see this more clearly if we instead plot the difference of the ECDF to the theoretical ECDF of a Uniformly distributed random variable.\n\nbokeh.io.show(bebi103.viz.sbc_rank_ecdf(df_sbc))\n\n\n  \n\n\n\n\n\nIn this clearer view, we see that most of the rank statistics all live within the 99% envelope, so we are in good shape.\nWith everything checking out, we can perform our sampling with real data!",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation based calibration and related checks in practice</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc_in_practice.html#sampling-with-our-new-model",
    "href": "lessons/principled/sbc_in_practice.html#sampling-with-our-new-model",
    "title": "31  Simulation based calibration and related checks in practice",
    "section": "31.4 Sampling with our new model",
    "text": "31.4 Sampling with our new model\nWe’ll now use our model with updated priors to perform parameter estimation using our real data set, checking all diagnostics after the fact, of course.\n\nwith bebi103.stan.disable_logging():\n    samples = sm_2.sample(data=data)\n    samples = az.from_cmdstanpy(posterior=samples, posterior_predictive='n_ppc')\n\nbebi103.stan.check_all_diagnostics(samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\n\n\nLet’s take a look at the corner plot.\n\nbokeh.io.show(bebi103.viz.corner(samples, parameters=['alpha', 'b']))\n\n\n  \n\n\n\n\n\nThis result looks very much like what we achieved in Lesson 8, so the small adjustment in prior did not affect our results. Nonetheless, making that adjustment to our model improved it, since we caught a problem in the prior (it gave burst sizes that were too small). In my experience, taking a principled approach to model building often uncovers issues in your model, even in simple ones like this one, that you were not aware of before performing checks.\nFinally, let’s perform a posterior predictive check to make sure the model adequately captures our data.\n\nn_ppc = samples.posterior_predictive.n_ppc.stack(\n    {\"sample\": (\"chain\", \"draw\")}\n).transpose(\"sample\", \"n_ppc_dim_0\")\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(\n        n_ppc,\n        data=np.array(data[\"n\"]),\n        x_axis_label=\"mRNA transcript count\",\n        diff='ecdf',\n    )\n)\n\n\n  \n\n\n\n\n\nThe model completely captures the data set; excellent!",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation based calibration and related checks in practice</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc_in_practice.html#conclusions",
    "href": "lessons/principled/sbc_in_practice.html#conclusions",
    "title": "31  Simulation based calibration and related checks in practice",
    "section": "31.5 Conclusions",
    "text": "31.5 Conclusions\nThe simulation-based calibration procedure (and the associated sensitivity analysis) is effective at identifying problem areas in Bayesian modeling. After passing the checks in this procedure, you can have more confidence in your modeling and the inferences you draw.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation based calibration and related checks in practice</span>"
    ]
  },
  {
    "objectID": "lessons/principled/sbc_in_practice.html#computing-environment",
    "href": "lessons/principled/sbc_in_practice.html#computing-environment",
    "title": "31  Simulation based calibration and related checks in practice",
    "section": "31.6 Computing environment",
    "text": "31.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,cmdstanpy,arviz,bokeh,iqplot,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.30.0\ncmdstanpy : 1.2.5\narviz     : 0.21.0\nbokeh     : 3.6.2\niqplot    : 0.3.7\nbebi103   : 0.1.27\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Principled inference pipelines",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Simulation based calibration and related checks in practice</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_assessment.html",
    "href": "lessons/model_assessment/model_assessment.html",
    "title": "Model assessment",
    "section": "",
    "text": "Herein, we go beyond the graphical in model assessment.",
    "crumbs": [
      "Model assessment"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html",
    "href": "lessons/model_assessment/model_comparison.html",
    "title": "32  Model comparison",
    "section": "",
    "text": "32.1 Metrics for model assessment\nWe have spent a lot of time looking at the problem of parameter estimation. Really, we have been stepping through the process of bringing our thinking about a biological system into a concrete generative statistical model that defines a likelihood for the data and the parametrization thereof. The specification of the model defines the set of parameters \\(\\theta\\) we need to estimate. For a data set \\(y\\), we wrote down Bayes’s theorem as\n\\[\\begin{aligned}\ng(\\theta\\mid y) =\n\\frac{f(y\\mid \\theta)\\,g(\\theta)}{f(y)}.\n\\end{aligned}\\]\nImplicit in all of this is an underlying model, \\(M\\). In this lecture, we will investigate assessment of the model \\(M\\), so we will explicitly include it in the models;\n\\[\\begin{aligned}\ng(\\theta_M\\mid y, M) =\n\\frac{f(y\\mid \\theta_M, M)\\,g(\\theta_M\\mid M)}{f(y\\mid M)}.\n\\end{aligned}\\]\nNote that I have subscripted the \\(\\theta\\)’s with an \\(M\\) to denote that the parameters are connected with a specific model \\(M\\). This notation can be cumbersome (with lots of \\(M\\)’s floating around), so we can alternatively, without ambiguity, write\n\\[\\begin{aligned}\ng_M(\\theta\\mid y) =\n\\frac{f_M(y\\mid \\theta)\\,g_M(\\theta)}{f_M(y)}.\n\\label{eq:model_bayes}\n\\end{aligned}\\]\nHere, the subscript \\(M\\) denotes that we are working with model \\(M\\).\nOur goal in model assessment is to see how close our model is to the true unknown generative process. To determine a metric of this closeness, we need to make a few definitions and be a bit formal for a moment. We define \\(f_t(\\tilde{y})\\) to be the true probability density function for generating a data set \\(\\tilde{y}\\). We have observed data set \\(y\\), and we would like to see how well we can predict data set \\(\\tilde{y}\\). Assuming we know the posterior \\(g_M(\\theta\\mid y)\\) (which we can formally write down using Bayes’s theorem), we can define the posterior predictive distribution by\n\\[\\begin{aligned}\nf_M(\\tilde{y}\\mid y) = \\int\\mathrm{d}\\theta\\,\\,f_M(\\tilde{y}\\mid \\theta)\\,g_M(\\theta\\mid y).\n\\end{aligned}\\]\nTake a moment to digest what this equation says. The posterior predictive distribution describes the kind of data sets we would expect the generative model \\(M\\) to produce after we have done our statistical inference informed by the measured data \\(y\\).\nOur goal in model assessment is to find out how close \\(f_M(\\tilde{y}\\mid y)\\) is to \\(f_t(\\tilde{y})\\). That is, we ask how well our generative model can generate new data compared to the true generative process.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html#posterior-predictive-checks",
    "href": "lessons/model_assessment/model_comparison.html#posterior-predictive-checks",
    "title": "32  Model comparison",
    "section": "32.2 Posterior predictive checks",
    "text": "32.2 Posterior predictive checks\nWe considered posterior predictive checks in Chapter 28, and they are very good tools for model assessment, so I provide a quick refresher here.\nEven though we do not know what the true distribution is, you actually sampled out of it by doing the experiment! You got only one sample, \\(y\\), but it is still a sample out of the true distribution. You can also sample out of \\(f_M(\\tilde{y}\\mid y)\\) if you have done MCMC sampling out of the posterior \\(g_M(\\theta\\mid y)\\). To do so, use each sample of \\(\\theta\\) out of the posterior to condition your likelihood to draw a new data set \\(\\tilde{y}\\). So, you now have one sample from the true distribution and one from the model, and you can compare the samples. This procedure constitutes a posterior predictive check.\nWhile prior predictive checks are used to see if your generative model produces data sets within the realm of possibility (and does not produce them outside the realm of possibility), a posterior predictive check considers how reasonable it is that the observed data came from your generative model. The output of the posterior predictive check is usually a plot of the samples out of \\(f_M(\\tilde{y}\\mid y)\\) overlaid with the actual data set \\(y\\). If there is good overlap, the posterior predictive check suggests that data generated from your model is commensurate with those generated from the true generative process.\nNote that good overlap of the samples out of the posterior predictive distribution with the measured data set does not mean that your proposed model is necessarily a good one. Quite often, an model can be overly flexible such that the range of posterior predictive data sets is broad and include the measured data simply for that reason.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html#closeness-metrics",
    "href": "lessons/model_assessment/model_comparison.html#closeness-metrics",
    "title": "32  Model comparison",
    "section": "32.3 Closeness metrics",
    "text": "32.3 Closeness metrics\nWhile posterior predictive checks are very useful and powerful for model assessment, it is useful to be able to quantify how close \\(f_M(\\tilde{y}\\mid y)\\) is to \\(f_t(\\tilde{y})\\). We worked out that the Kullback-Leibler divergence is useful to quantify how close one distribution is to another in Chapter 11. In our current application, we want to use the KL divergence as a metric for how close the posterior predictive distribution \\(f(\\tilde{y}\\mid y, M)\\) is to the true distribution \\(f_t(\\tilde{y})\\), we can write1\n\\[\\begin{aligned}\nD_\\mathrm{KL}(f_t \\| f_M) = \\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln\\frac{f_t(\\tilde{y})}{f_M(\\tilde{y} \\mid y)}.\n\\end{aligned}\\]\n\n32.3.1 The expected log pointwise predictive density\nIn practice, we want to compare two or more models. In other words, we wish to know if model A is closer than model B is to the true distribution. So, we might be interested in the difference in the KL-divergences of two proposed models.\n\\[\\begin{aligned}\n\\begin{align}\nD_\\mathrm{KL}(f_t \\| f_{M_a}) - D_\\mathrm{KL}(f_t \\| f_{M_b}) &= \\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln\\frac{f_t(\\tilde{y})}{f_{M_a}(\\tilde{y}\\mid y)}\n- \\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln\\frac{f_t(\\tilde{y})}{f_{M_b}(\\tilde{y}\\mid y)} \\\\\n&=\\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln\\frac{f_{M_b}(\\tilde{y}\\mid y)}{f_{M_a}(\\tilde{y}\\mid y)} \\\\\n&=\\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln f_{M_b}(\\tilde{y}\\mid y) - \\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln f_{M_a}(\\tilde{y}\\mid y),\n\\end{align}\n\\end{aligned}\\]\nwhere we did the awkward splitting of a logarithm so it looks like we are taking logarithms of quantities with units.2 This tells us that the quantity we need to calculate for any model \\(M\\) that we wish to assess is\n\\[\\begin{aligned}\n\\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln f_M(\\tilde{y}\\mid y).\n\\end{aligned}\\]\nNow, imagine that we have \\(N\\) independent measurements of data points. That is, \\(y = (y_1, y_2, \\ldots y_N)\\), with each \\(y_i\\) being independent of the others. Thus,\n\\[\\begin{aligned}\nf_M(\\tilde{y}\\mid y) = \\prod_{i=1}^N f_M(\\tilde{y}_i\\mid y).\n\\end{aligned}\\]\nWe do not know for sure that the data points in the true model are independent, but we will assume they are, i.e., that\n\\[\\begin{aligned}\nf_t(\\tilde{y}) = \\prod_{i=1}^N f_t(\\tilde{y}_i).\n\\end{aligned}\\]\nNow, if we were to generate a new set of \\(N\\) data points, \\(\\tilde{y}\\), with the assumption of independence of the \\(\\tilde{y}_i\\), then our expression becomes\n\\[\\begin{aligned}\n\\begin{aligned}\n\\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln f_M(\\tilde{y}\\mid y)\n&= \\int \\mathrm{d}\\tilde{y}\\,\\,f_t(\\tilde{y})\\,\\ln \\left[\\prod_{i=1}^Nf_M(\\tilde{y}_i\\mid y)\\right] \\nonumber\\\\\n&= \\int \\mathrm{d}\\tilde{y}\\,f_t(\\tilde{y})\\,\\sum_{i=1}^N\\ln f_M(\\tilde{y}_i\\mid y) \\nonumber\\\\\n&=\\sum_{i=1}^N\\int\\mathrm{d}\\tilde{y}_i\\,\\,f_t(\\tilde{y}_i)\\,\\ln f_M(\\tilde{y}_i\\mid y).\n\\end{aligned}\n\\end{aligned}\\]\nThis expression is called the expected log pointwise predictive density, or elpd (sometimes elppd),\n\\[\\begin{aligned}\n\\text{elpd} = \\sum_{i=1}^N\\int\\mathrm{d}\\tilde{y}_i\\,\\,f_t(\\tilde{y}_i)\\,\\ln f_M(\\tilde{y}_i\\mid y).\n\\end{aligned}\\]\nIt took a while, but this, the elpd, is the quantity we need to determine to compare models. As a reminder, comparing the elpd of two different models gives their relative closeness (as defined by the KL divergence) to the true distribution.3 While we would like to compute the elpd, we cannot, because \\(f_t(\\tilde{y})\\) is not known. All we have is a single data set sampled from it (the one we got by doing the experiment). We therefore seek ways to approximately compute elpd.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html#the-watanabe-akaike-information-criterion",
    "href": "lessons/model_assessment/model_comparison.html#the-watanabe-akaike-information-criterion",
    "title": "32  Model comparison",
    "section": "32.4 The Watanabe-Akaike information criterion",
    "text": "32.4 The Watanabe-Akaike information criterion\nThe first approximation of the elpd we will consider is the Watanabe-Akaike information criterion, also known as the widely applicable information criterion, or WAIC. To compute the WAIC, we first approximate the elpd by the log pointwise predictive density, or lpd (sometimes called lppd). It is computed by using the plug-in estimate for \\(f_t(\\tilde{y})\\). That is,\n\\[\\begin{aligned}\nf_t(\\tilde{y}_j) \\approx \\hat{f}_t(\\tilde{y}_j) \\equiv \\frac{1}{N}\\sum_{i=1}^N \\delta(\\tilde{y}_j - y_i),\n\\end{aligned}\\]\nwhere \\(\\delta\\) denotes the Dirac delta function. Substitution of this expression into the expression for the elpd gives\n\\[\\begin{aligned}\n\\text{lpd} = \\sum_{i=1}^N \\ln f_M(y_i\\mid y).\n\\end{aligned}\\]\nThe lpd will overestimate the elpd because the averaging over the true distribution in the elpd necessarily lowers the value of the summand. To attempt to correct for this discrepancy, another term, \\(p_\\mathrm{waic}\\) is subtracted from lpd to give the WAIC estimate of elpd.\n\\[\\begin{aligned}\n\\text{elpd}_\\mathrm{waic} = \\text{lpd} - p_\\mathrm{waic}.\n\\end{aligned}\\]\nI will not go into the derivation here (see the paper by Vehtari, Gelman, and Gabry, or the arXiv version, and references therein), but \\(p_\\mathrm{waic}\\) is given by the summed variances of the log likelihood of the observations \\(y_i\\).\n\\[\\begin{aligned}\np_\\mathrm{waic} = \\sum_{i=1}^N \\text{variance}(\\ln f_M(y_i\\mid y)),\n\\end{aligned}\\]\nwhere the variance is computed over the posterior. Written out, this is\n\\[\\begin{aligned}\n\\begin{aligned}\n\\text{variance}(\\ln f_M(y_i\\mid y)) &=  \\int\\mathrm{d}\\theta\\, g_M(\\theta \\mid y)\\,(\\ln f_M(y_i\\mid y))^2 \\nonumber \\\\\n&\\;\\;\\;\\;- \\left(\\int\\mathrm{d}\\theta\\, g_M(\\theta \\mid y)\\,\\ln f_M(y_i\\mid y)\\right)^2.\n\\end{aligned}\n\\end{aligned}\\]\nThis is kind of a mess, and its form is better understood if you go through the derivation. Importantly, though, both lpd and \\(p_\\mathrm{waic}\\) can be computed using samples from the parameter estimation problem, further underscoring the incredible advantage that having samples gives. Given a set of \\(S\\) MCMC samples of the parameters \\(\\theta\\) (where \\(\\theta^{(s)}\\) is the \\(s\\)th sample), the lpd may be calculated as\n\\[\\begin{aligned}\n\\text{lpd} = \\sum_{i=1}^N\\ln \\left(\\frac{1}{S}\\sum_{s=1}^S f_M(y_i\\mid \\theta^{(s)})\\right).\n\\end{aligned}\\]\nThis is another beautiful example of how sampling converts integrals into sums. Similarly we can compute \\(p_\\mathrm{waic}\\) from samples.\n\\[\\begin{aligned}\np_\\mathrm{waic} = \\sum_{i=1}^N\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\log f_M(y_i\\mid \\theta^{(s)}) - q(y_i)\\right)^2,\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\nq(y_i) = \\frac{1}{S}\\sum_{s=1}^S\\ln f_M(y_i\\mid\\theta^{(s)}).\n\\end{aligned}\\]\nFor historical reasons, the value of the WAIC is reported as\n\\[\\begin{aligned}\n\\text{WAIC} = -2\\,\\text{elpd}_\\mathrm{waic} = -2(\\text{lpd} - p_\\mathrm{waic}).\n\\end{aligned}\\]\nThe ArviZ package offers a function az.waic() to compute the WAIC directly from MCMC samples.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html#leave-one-out-estimates-of-elpd",
    "href": "lessons/model_assessment/model_comparison.html#leave-one-out-estimates-of-elpd",
    "title": "32  Model comparison",
    "section": "32.5 Leave-one-out estimates of elpd",
    "text": "32.5 Leave-one-out estimates of elpd\nLeave-one-out cross validation (LOO) is a technique widely used in machine learning to test how well a machine can predict new data. The technique is simple; one data point is held out of a set of data, and the learning algorithm uses the remaining \\(N-1\\) data points to learn. The ability of the machine to predict the value of the omitted data point is used to assess its performance.\nThe idea behind LOO applied in Bayesian model comparison is similar. The \\(i\\)th data point is omitted from the data set, and we obtain a posterior predictive density from it. Formally, let \\(y_{-i}\\) be the data set with the \\(i\\)th data point, \\(y_i\\), removed. Then the LOO posterior predictive density is\n\\[\\begin{aligned}\nf_M(y_i\\mid y_{-i}) = \\int \\mathrm{d}\\theta\\,\\,f_M(y_i\\mid \\theta)\\,g_M(\\theta\\mid y_{-i}).\n\\end{aligned}\\]\nWe can then get the approximate elpd as\n\\[\\begin{aligned}\n\\mathrm{elpd}_\\mathrm{loo} = \\sum_{i=1}^N\\ln f_M(y_i\\mid y_{-i}).\n\\end{aligned}\\]\nThe pleasant feature of the LOO approximation of elpd is that the posterior distribution was computed from a smaller data set (smaller by one datum) and then the ability to predict is assessed against a data point that was not used in computing the posterior and was actually drawn from the true distribution (by experiment).\nIn principle, the LOO estimate for the elpd could be directly computed by performing \\(N\\) different MCMC sampling calculations, one for each omitted data point, and then summing logarithms of posterior predictive samples. For large \\(N\\), this can be very computationally expensive. Fortunately, there are good ways to estimate \\(\\text{elpd}_\\mathrm{loo}\\) directly from MCMC samples. I will not go into the details here, but importantly the methods use Pareto-smoothed importance sampling to get numerically stable estimates for the elpd. You can read about the methods in the Vehtari, Gelman, and Gabry paper. They are also implemented in the az.loo() function of the ArviZ pacakge.\nAgain for historical reasons, the LOO is not reported as the elpd estimate, but as\n\\[\\begin{aligned}\n\\text{LOO} = -2\\,\\text{elpd}_\\mathrm{loo}.\n\\end{aligned}\\]\nI have called this quantity LOO for lack of a better term and also because this is what ArviZ calls it when reporting its value. It can be shown that this quantity and the WAIC are asymptotically equal with large \\(N\\). However, the LOO estimate for the elpd tends to be better than that of the WAIC, in fact much better for smaller data sets. LOO is therefore preferred.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html#the-akaike-weights",
    "href": "lessons/model_assessment/model_comparison.html#the-akaike-weights",
    "title": "32  Model comparison",
    "section": "32.6 The Akaike weights",
    "text": "32.6 The Akaike weights\nRemember, the value of a WAIC or LOO by itself does not tell us anything. Only comparison of two or more of these criteria makes sense. Recalling that the elpd is a logarithm of a probability density, so if we exponentiate it, we get something proportional to a probability. If we have two models, \\(M_i\\) and \\(M_j\\), the Akaike weight of model \\(i\\) is\n\\[\\begin{aligned}\nw_i = \\frac{\\exp\\left[-\\frac{1}{2}\\,\\text{LOO}_i\\right]}{\\exp\\left[-\\frac{1}{2}\\,\\text{LOO}_i\\right] + \\exp\\left[-\\frac{1}{2}\\,\\text{LOO}_j\\right]},\n\\end{aligned}\\]\nwhere WAIC may be substituted for LOO as you wish. In this comparison of two models, the weight of model \\(i\\) is related to the difference of Kullback-Leibler divergences between the true distribution and the respective models.\n\\[\\begin{aligned}\nw_i \\approx \\frac{\\exp\\left[D_\\mathrm{KL}(f_t\\| f_{M_j}) - D_\\mathrm{KL}(f_t\\| f_{M_i})\\right]}{1 + \\exp\\left[D_\\mathrm{KL}(f_t\\| f_{M_j}) - D_\\mathrm{KL}(f_t\\| f_{M_i})\\right]}.\n\\end{aligned}\\]\nA common, but not agreed upon, interpretation is that the Akaike weight is an estimate of the probability that \\(M_i\\) will make the best predictions of new data.\nFinally, we can generalize the Akaike weights to multiple models.\n\\[\\begin{aligned}\nw_i = \\frac{\\exp\\left[-\\frac{1}{2}\\,\\text{LOO}_i\\right]}{\\sum_j \\exp\\left[-\\frac{1}{2}\\,\\text{LOO}_j\\right]}.\n\\end{aligned}\\]",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison.html#footnotes",
    "href": "lessons/model_assessment/model_comparison.html#footnotes",
    "title": "32  Model comparison",
    "section": "",
    "text": "I am playing a little fast and loose here converting sums to integrals. There are some subtleties involved therein, but we will not delve into those here.↩︎\nTaking logarithms of quantities with units bothers me immensely. Going forward, imagine there is an invisible “\\(1 \\text{ units-of-}y\\)” multiplying the \\(f_M(\\tilde{y}\\mid y)\\)'s.↩︎\nNote that this is not the only metric we could use to compare models, but it is the most widely used one and is intuitively convenient due to its relationship to the Kullback-Leibler divergence.↩︎",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison_in_practice.html",
    "href": "lessons/model_assessment/model_comparison_in_practice.html",
    "title": "33  Model comparison in practice",
    "section": "",
    "text": "33.1 An example model comparison\n| Download notebook\nData set download\nWhen comparing models, posterior predictive checks are a must. As discussed in the previous lesson, we can use information criteria to assess relative predictive effectiveness of models. In order to understand this lesson, you will need have carefully read and understand the previous lesson on the theory behind model comparison.\nThe key quantity we compute for more comparison is the expected log pointwise predictive density, or elpd. There were a few key ideas and assumptions in using elpd.\nWith these assumptions, we can approximately compute the elpd using the Watanabe-Akaike information criterion (WAIC) or leave-one-out cross validation (LOO). See this paper by Vehtari, Gelman, and Gabry (arXiv version) for more details about the implementation. As described in that paper, LOO, when computed using Pareto-smoothed importance sampling, is the preferred method for computing an approximate elpd.\nImportantly, the (approximate) elpd by itself is not terribly useful in assessing a model. The elpd of one prospective model needs to be compared to another. For this comparison, we can compute Akaike weights. This is the most straightforward calculation of relative weights of respective models, and perhaps easiest to understand. However, it may not be the best way to assess the predictive capabilities of a model, especially in situations where the true generative model is not known (which is often the case for us as scientists). As we think about generative models, and we are not sure which model best generates observed data, it is useful to think about model averaging if our aim is to be predictive. The idea here is that we do not know which model generates data. Instead, we try to find a combination of models that spans all of the models we are considering, that best generate the data. The respective weights of the models give their contributions to this combination of models. As a scientist, I tend to shy away from model averaging; I am rather seeking to understand how nature generates the observations I see, and nature is not trying to predict, nor average models. However, taking a model averaging approach with an eye for optimizing predictive performance leads to more robust estimates of model weights, as outlined in this paper by Yao and coworkers, which describes a technique known as stacking.\nIn this tutorial, we will demonstrate how to calculate model weights both by using Akaike weights (and variants thereof), and stacking. Conveniently, this may be done approximately directly from samples out of the posterior distributions. The ArviZ package provides much of the functionality we need.\nTo show how we can do an model comparison, we will consider a contrived data set that may come from a single Normal distribution or from a mixture of two Normals.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Model comparison in practice</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison_in_practice.html#an-example-model-comparison",
    "href": "lessons/model_assessment/model_comparison_in_practice.html#an-example-model-comparison",
    "title": "33  Model comparison in practice",
    "section": "",
    "text": "33.1.1 Computing the pointwise log likelihood\nRecalling from Chapter 32, the elpd is the the logarithm of the posterior predictive distribution, \\(f(\\tilde{y}_i\\mid y)\\), averaged over the true generative distribution \\(f_t(\\tilde{y}_i)\\).\n\\[\\begin{align}\n\\text{elpd} = \\sum_{i=1}^N\\int\\mathrm{d}\\tilde{y}_i\\,\\,f_t(\\tilde{y}_i)\\,\\ln f(\\tilde{y}_i\\mid y).\n\\end{align}\\]\nWhen generating our samples, we therefore also need to compute samples of the value of the log likelihood. To do this, for each set of parameters \\(\\theta\\) that we sample out of the posterior, we compute the pointwise log likelihood of the data set, using the parameters \\(\\theta\\). The Stan code below includes these log likelihood samples as well as posterior predictive checks for a univariate Gaussian mixture model. Be sure to read the code carefully.\ndata {\n    int&lt;lower=1&gt; N;\n    array[N] real y;\n\n    // Number of mixtures\n    int&lt;lower=1&gt; K;\n}\n\n\ntransformed data {\n    // Prior parameters\n    real mu_mu = 0;\n    real sigma_mu = 100;\n    real sigma_sigma = 100;\n}\n\n\nparameters {\n    // location and scale parameters of each component of the mixture\n    ordered[K] mu;\n    array[K] real&lt;lower=0&gt; sigma;\n\n    // Weights of components of mixture\n    simplex[K] pi_;\n}\n\n\nmodel {\n    // Priors\n    mu ~ normal(mu_mu, sigma_mu);\n    sigma ~ normal(0.0, sigma_sigma);\n\n    // Likelihood. Use logsumexp trick to compute likelihood of mixture model\n    array[K] real log_pdf_components;\n    for (i in 1:N) {\n        for (k in 1:K) {\n            log_pdf_components[k] = log(pi_[k]) + normal_lpdf(y[i] | mu[k], sigma[k]);\n        }\n        target += log_sum_exp(log_pdf_components);\n    }\n}\n\n\ngenerated quantities {\n    // Posterior predictive checks\n    array[N] real y_ppc;\n    \n    for (i in 1:N) {\n        int component = categorical_rng(pi_);\n        y_ppc[i] = normal_rng(mu[component], sigma[component]);\n    }\n\n    // Pointwise log likelihood\n    array[N] real log_lik;\n\n    for (i in 1:N) {\n        array[K] real log_pdf_components_log_lik;\n        for (k in 1:K) {\n            log_pdf_components_log_lik[k] = log(pi_[k]) + normal_lpdf(y[i] | mu[k], sigma[k]);\n        }\n        log_lik[i] = log_sum_exp(log_pdf_components_log_lik);\n    }\n\n}\nIn the array log_lik, I store the pointwise log likelihood. That is, for each measurement (in this case for each \\(y_i\\)), I compute the log likelihood for that data point using the parameters that I sampled out of the posterior. Conveniently, Stan’s distributions all have a function that ends in _lpdf that compute the log probability density function for the distribution (with _lpmf for discrete distributions that computes the log probability mass function for the distribution).\nLet’s sample out of this generative model, keeping samples of posterior predictive data sets and pointwise log likelihoods.\n\n# Load data and make data dictionary\ndf = pl.read_csv(os.path.join(data_path, \"possibly_bimodal.csv\"), comment_prefix=\"#\")\ndata = dict(N=len(df), y=df[\"y\"].to_numpy(), K=1)\n\nwith bebi103.stan.disable_logging():\n    # Compile the model\n    sm = cmdstanpy.CmdStanModel(stan_file=\"normal_mixture.stan\")\n    \n    # Perform sampling\n    samples_single = sm.sample(data=data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nWhen we convert the samples to an ArviZ object, we need to specify the posterior predictive and log likelihood.\n\n# Convert to ArviZ object\nsamples_single = az.from_cmdstanpy(\n    posterior=samples_single, posterior_predictive=\"y_ppc\", log_likelihood=\"log_lik\"\n)\n\nNow, we will check the diagnostics (as we always should) and make a corner plot.\n\n# Check diagnostics\nbebi103.stan.check_all_diagnostics(samples_single)\n\n# Make a corner plot\nbokeh.io.show(bebi103.viz.corner(samples_single, parameters=['mu[0]', 'sigma[0]']))\n\nEffective sample size looks reasonable for all parameters.\n\nRhat for parameter pi_[0] is NaN.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n\n  \n\n\n\n\n\nEverything looks good. Actually, it doesn’t. The sampling looks good, but we should do posterior predictive checks.\n\ny_ppc = samples_single.posterior_predictive.y_ppc.stack(\n    {\"sample\": (\"chain\", \"draw\")}\n).transpose(\"sample\", \"y_ppc_dim_0\")\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(\n        y_ppc,\n        name=\"y_ppc\",\n        data=df[\"y\"].to_numpy(),\n        x_axis_label=\"y\",\n    )\n)\n\n\n  \n\n\n\n\n\nWe have clearly failed the posterior predictive checks. We can stop here, but we will continue to compute the WAIC and LOO for illustrative purposes. As we do that, let’s take a quick look at the output so we can see how the log likelihood samples are organized. The log likelihood is stored in ArviZ InferenceData objects in the log_likelihood attribute.\n\nsamples_single.log_likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 13MB\nDimensions:        (chain: 4, draw: 1000, log_lik_dim_0: 394)\nCoordinates:\n  * chain          (chain) int64 32B 0 1 2 3\n  * draw           (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * log_lik_dim_0  (log_lik_dim_0) int64 3kB 0 1 2 3 4 5 ... 389 390 391 392 393\nData variables:\n    log_lik        (chain, draw, log_lik_dim_0) float64 13MB -4.635 ... -3.176\nAttributes:\n    created_at:                 2025-06-24T06:33:44.724251+00:00\n    arviz_version:              0.21.0\n    inference_library:          cmdstanpy\n    inference_library_version:  1.2.5xarray.DatasetDimensions:chain: 4draw: 1000log_lik_dim_0: 394Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])log_lik_dim_0(log_lik_dim_0)int640 1 2 3 4 5 ... 389 390 391 392 393array([  0,   1,   2, ..., 391, 392, 393])Data variables: (1)log_lik(chain, draw, log_lik_dim_0)float64-4.635 -4.726 ... -3.447 -3.176array([[[-4.63451, -4.72612, -4.24165, ..., -3.35168, -3.42874,\n         -3.18776],\n        [-4.50059, -4.58336, -4.14648, ..., -3.4009 , -3.47688,\n         -3.23026],\n        [-4.48922, -4.57438, -4.12559, ..., -3.40508, -3.48829,\n         -3.2116 ],\n        ...,\n        [-4.80964, -4.91005, -4.37755, ..., -3.29573, -3.36863,\n         -3.15706],\n        [-4.44183, -4.52264, -4.09692, ..., -3.42382, -3.50394,\n         -3.23613],\n        [-4.44584, -4.52329, -4.11461, ..., -3.4246 , -3.49651,\n         -3.26199]],\n\n       [[-4.60851, -4.70087, -4.21313, ..., -3.36109, -3.44367,\n         -3.17846],\n        [-4.68534, -4.77874, -4.28421, ..., -3.33488, -3.40888,\n         -3.18392],\n        [-4.5673 , -4.65711, -4.18313, ..., -3.37573, -3.45832,\n         -3.19   ],\n...\n        [-4.46777, -4.55685, -4.08855, ..., -3.41939, -3.51563,\n         -3.18439],\n        [-4.70743, -4.80564, -4.28624, ..., -3.32785, -3.40998,\n         -3.1538 ],\n        [-4.452  , -4.54023, -4.07657, ..., -3.42585, -3.52248,\n         -3.18844]],\n\n       [[-4.5716 , -4.65833, -4.19994, ..., -3.37434, -3.44912,\n         -3.21266],\n        [-4.64331, -4.73688, -4.24226, ..., -3.34877, -3.42916,\n         -3.17535],\n        [-4.62299, -4.71057, -4.24688, ..., -3.35777, -3.42726,\n         -3.21582],\n        ...,\n        [-4.61092, -4.70214, -4.22009, ..., -3.35987, -3.43942,\n         -3.18663],\n        [-4.67447, -4.76523, -4.28436, ..., -3.33994, -3.40947,\n         -3.20169],\n        [-4.60324, -4.69588, -4.20678, ..., -3.36329, -3.44734,\n         -3.17577]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))log_lik_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       384, 385, 386, 387, 388, 389, 390, 391, 392, 393],\n      dtype='int64', name='log_lik_dim_0', length=394))Attributes: (4)created_at :2025-06-24T06:33:44.724251+00:00arviz_version :0.21.0inference_library :cmdstanpyinference_library_version :1.2.5\n\n\nThe log likelihood is three dimensional, one dimension each for chain and draw, and then the final dimension is for the pointwise log-likelihood estimates.\nGiven that the log likelihood is stored in the ArviZ object, we can directly use the samples to compute the WAIC and LOO.\n\n\n33.1.2 Computing the WAIC and LOO\nWe will start with the WAIC. We use the scale=\"deviance\" kwarg to get the WAIC. By default, az.waic() will return the estimate for the elpd and not the traditionally used \\(-2\\mathrm{elpd}_\\mathrm{WAIC}\\), which is why we use the scale=\"deviance\" kwarg.\n\naz.waic(samples_single, scale=\"deviance\")\n\nComputed from 4000 posterior samples and 394 observations log-likelihood matrix.\n\n              Estimate       SE\ndeviance_waic  2908.20    30.09\np_waic            2.18        -\n\n\nThe output gives an estimate for the WAIC, and also the contribution of \\(p_\\mathrm{waic}\\). It also gives an estimate of the standard error in the WAIC.\nLet’s now compute the LOO.\n\nsingle_loo = az.loo(samples_single, scale=\"deviance\")\n\nsingle_loo\n\nComputed from 4000 posterior samples and 394 observations log-likelihood matrix.\n\n             Estimate       SE\ndeviance_loo  2908.21    30.10\np_loo            2.19        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      394  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\nWe see that the LOO and WAIC give almost identical results (as they should). The Pareto k diagnostic is also good, meaning that the LOO calculation does not have obvious pathologies. Remember, though, that LOO has better performance across a wider variety of models.\n\n\n33.1.3 Calculations with the mixture model\nNow, let’s do the same calculation for the mixture model. Since we wrote Stan code for a general univariate Gaussian mixture model, we can just change out inputted \\(K\\) value (the number of components in the mixture) to 2 and sample.\n\ndata['K'] = 2\n\nwith bebi103.stan.disable_logging():\n    samples_mix = sm.sample(data=data)\n\nsamples_mix = az.from_cmdstanpy(samples_mix, posterior_predictive='y_ppc', log_likelihood='log_lik')\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nWe’ll do our usual diagnostic checks and make a corner plot.\n\n# Check diagnostics\nbebi103.stan.check_all_diagnostics(samples_mix)\n\n# Make corner plot\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples_mix,\n        parameters=[\"mu[0]\", \"mu[1]\", \"sigma[0]\", \"sigma[1]\", \"pi_[0]\"],\n        xtick_label_orientation=np.pi / 4,\n    )\n)\n\nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n\n  \n\n\n\n\n\nEverything looks good! We do see a clear separation in \\(\\mu\\) values in the two components of the mixture. It also appears as though the smaller-\\(\\mu\\) component of the mixture has a mixing coefficient of about 0.45. We can do a quick posterior predictive check.\n\ny_ppc = samples_mix.posterior_predictive.y_ppc.stack(\n    {\"sample\": (\"chain\", \"draw\")}\n).transpose(\"sample\", \"y_ppc_dim_0\")\n\nbokeh.io.show(\n    bebi103.viz.predictive_ecdf(\n        y_ppc,\n        name=\"y_ppc\",\n        data=df[\"y\"].to_numpy(),\n        x_axis_label=\"y\",\n    )\n)\n\n\n  \n\n\n\n\n\nMuch nicer! The model allows for plenty of flexibility to allow for the observed bimodal behavior. Let’s proceed to compute the LOO and WAIC, starting with the WAIC.\n\naz.waic(samples_mix, scale=\"deviance\")\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n\n\nComputed from 4000 posterior samples and 394 observations log-likelihood matrix.\n\n              Estimate       SE\ndeviance_waic  2795.92    32.32\np_waic            5.12        -\n\nThere has been a warning during the calculation. Please check the results.\n\n\nAnd the LOO.\n\nmix_loo = az.loo(samples_mix, scale=\"deviance\")\n\nmix_loo\n\nComputed from 4000 posterior samples and 394 observations log-likelihood matrix.\n\n             Estimate       SE\ndeviance_loo  2795.95    32.33\np_loo            5.14        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      394  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\nAs expected, we get almost the same value for the two information criteria. Let’s take a quick look at the difference of the LOO’s between the mixture model and the single Negative Binomial model.\n\nmix_loo.elpd_loo - single_loo.elpd_loo\n\nnp.float64(-112.25895344885703)\n\n\nRemember that for historical reasons,\n\\[\\begin{align}\n\\text{WAIC} \\approx -2\\,\\text{elpd}, \\\\[1em]\n\\text{LOO} \\approx -2\\,\\text{elpd}. \\\\[1em]\n\\end{align}\\]\nThe bigger the elpd is, the smaller the Kullback-Leibler divergence is, so the better the model is. Thus, a bigger elpd means a smaller WAIC or LOO. So, the smaller the WAIC or LOO is, the closer the model is to the true generative model. This WAIC and LOO are smaller for the mixture model than for the single Normal model, so the mixture model is a better model.\n\n\n33.1.4 Computing the weights\nWe can directly compute the Akaike weights from the values of the LOO, using\n\\[\\begin{align}\nw_i = \\frac{\\exp\\left[-(\\text{LOO}_i-\\text{LOO}_j)/2\\right]}{1 + \\exp\\left[-(\\text{LOO}_i-\\text{LOO}_j)/2\\right]}.\n\\end{align}\\]\n\nd_loo = mix_loo.elpd_loo - single_loo.elpd_loo\nw_single = np.exp(d_loo/2) / (1 + np.exp(d_loo/2))\nw_mix = 1 - w_single\n\nprint('      Mixture model weight:', w_mix)\nprint('Single Normal model weight:', w_single)\n\n      Mixture model weight: 1.0\nSingle Normal model weight: 4.200277524728524e-25\n\n\nIn agreement with our posterior predictive checks, the mixture model is far more predictive than the single negative binomial model.\nAs I mentioned above, ArviZ offers more a sophisticated means of computing the weights using stacking. The results tend to be less extreme (and therefore more conservative) that directly computing the Akaike weights. We can use the az.compare() function to do the calculation. We will do it using the LOO (WAIC is default, so we use the ic kwarg). The first input is a dictionary containing the MCMC samples, where the keys of the dictionary are the names of the models.\n\naz.compare({\"single\": samples_single, \"mix\": samples_mix}, ic=\"loo\", scale=\"deviance\")\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nmix\n0\n2795.946936\n5.138599\n0.000000\n1.000000e+00\n32.327322\n0.000000\nFalse\ndeviance\n\n\nsingle\n1\n2908.205890\n2.188323\n112.258953\n3.882406e-11\n30.096514\n19.915636\nFalse\ndeviance\n\n\n\n\n\n\n\nThe mixture model is still dominant. (Again, we are not going into the details of the stacking calculation, but you can read about it in this paper by Yao and coworkers.)\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Model comparison in practice</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/model_comparison_in_practice.html#computing-environment",
    "href": "lessons/model_assessment/model_comparison_in_practice.html#computing-environment",
    "title": "33  Model comparison in practice",
    "section": "33.2 Computing environment",
    "text": "33.2 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,cmdstanpy,arviz,bokeh,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.30.0\ncmdstanpy : 1.2.5\narviz     : 0.21.0\nbokeh     : 3.6.2\nbebi103   : 0.1.27\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Model comparison in practice</span>"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/intro_variate_covariate.html",
    "href": "lessons/variate_covariate/intro_variate_covariate.html",
    "title": "Variate-covariate models",
    "section": "",
    "text": "Consider a titration experiment to determine the dissociation constant of binding partners A and B. The total concentration of B, \\(c_\\mathrm{B}^0\\) is held constant while the total concentration of A, \\(c_\\mathrm{A}^0\\), is varied. The equilibrium concentration of the duplex AB, \\(c_\\mathrm{AB}\\), is measured for each value of \\(c_\\mathrm{A}^0\\). For the reaction AB ⇌ A + B with dissociation constant \\(K_d\\), we can solve for the concentration of the AB duplex using the equation\n\\[\\begin{align}\nK_d = \\frac{c_\\mathrm{A}\\,c_\\mathrm{B}}{c_\\mathrm{AB}} = \\frac{(c_\\mathrm{A}^0 - c_\\mathrm{AB})\\,(c_\\mathrm{A}^0 - c_\\mathrm{AB})}{c_\\mathrm{AB}},\n\\end{align}\n\\]\nwhich is solved to give\n\\[\\begin{align}\nc_\\mathrm{AB} = \\frac{1}{2}\\left(K_d + c_\\mathrm{A}^0 + c_\\mathrm{B}^0 - \\sqrt{\\left(K_d + c_\\mathrm{A}^0 + c_\\mathrm{B}^0\\right)^2 - 4c_\\mathrm{A}^0\\,c_\\mathrm{B}^0}\\right).\n\\end{align}\n\\]\nSo, we have an equation for our measured quantity \\(c_\\mathrm{AB}\\) in terms of our manipulated quantity \\(c_\\mathrm{A}^0\\) (with \\(c_\\mathrm{B}^0\\) held constant). This is an example of a variate-covariate model, where the measured quantity is called the variate and manipulated quantities are called covariates. This constitutes an important class of models that are routinely encountered in science.\nNote that analysis of variate-covariate models is often referred to as “curve fitting.” I eschew this term because I prefer to focus on the generative model, as opposed to whatever technique we may use to obtain estimates of its parameters.",
    "crumbs": [
      "Variate-covariate models"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/model_building.html",
    "href": "lessons/variate_covariate/model_building.html",
    "title": "34  Model building",
    "section": "",
    "text": "34.1 End of theory?\nIn this lesson, we will learn how to construct a variate-covariate model and discuss some useful theoretical results related to the maximum likelihood estimates for the parameters of these models. As we usually do, we will have a concrete model in mind as an example. I will take this opportunity to illustrate the process of carefully building a model (this time in the context of variate-covariate modeling) and discuss why modeling is important for scientists.\nAs machine learning methods grow in power and prominence, and as data acquisition becomes more and more facile, we see more and more methods where a machine “learns” directly from data. Over a decade ago, Chris Anderson wrote an article entitled The End of Theory: The Data Deluge Makes the Scientific Method Obsolete in Wired Magazine. Anderson claimed that because we have access to large data sets, we no longer need the scientific method of testable hypotheses. Specifically, he says we do not need models, we can just use lots and lots of data to make predictions. This is absurd because if we just try to learn from data, we do not really learn anything fundamental about how nature works. If you are working for Netflix and trying to figure out what movies people want to watch, learning from data is fine. But if you’re a scientist and want to increase knowledge, you need models.",
    "crumbs": [
      "Variate-covariate models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model building</span>"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/model_building.html#an-example-exercise-in-modeling",
    "href": "lessons/variate_covariate/model_building.html#an-example-exercise-in-modeling",
    "title": "34  Model building",
    "section": "34.2 An example exercise in modeling",
    "text": "34.2 An example exercise in modeling\nWe will introduce two competing models for how the size of mitotic spindles are set. Matt Good and coworkers (*Science*, 2013) developed a microfluidic device where they could create droplets of cytoplasm extracted from Xenopus eggs and embryos, as shown the figure below (scale bar 20 µm; image taken from the paper).\n\n\n\nDroplets encapsulating mitotic spindles from Good and coworkers.\n\n\nA remarkable property about Xenopus extract is that mitotic spindles spontaneously form; the extracted cytoplasm has all the ingredients to form them. This makes it an excellent model system for studying spindles. With their device, Good and his colleagues were able to study how the size of the cell affects the dimensions of the mitotic spindle; a simple, yet beautiful, question. The experiment is conceptually simple; they made the droplets and then measured their dimensions and the dimensions of the spindles using microscope images.\nLet’s take a quick look at the result.\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now propose two models for how the droplet diameter affects the spindle length.\n\nThe spindles have an inherent length, independent of droplet diameter.\nThe length of spindles is determined by the total amount of tubulin available to make them.\n\n\n34.2.1 Model 1: Spindle size is independent of droplet size\nAs a first model, we propose that the size of a mitotic spindle is inherent to the spindle itself. This means that the size of the spindle is independent of the size of the droplet or cell in which it resides. This would be the case, for example, if construction of the spindle involves length-sensing molecules, such as depolymerizing motor proteins. We define that set length as \\(\\phi\\).\n\n34.2.1.1 The likelihood\nNot all spindles will be measured to be exactly \\(\\phi\\) µm in length. Rather, there may be some variation about \\(\\phi\\) due to natural variation and measurement error. So, we would expect measured length of spindle i to be\n\\[\\begin{align}\nl_i = \\phi + e_i,\n\\end{align}\\]\nwhere \\(e_i\\) is the noise component of the ith datum.\nSo, we have a theoretical model for spindle length, \\(l = \\phi\\), and to get a fully generative model, we need to model the errors \\(e_i\\). A reasonable model assumes\n\nEach measured spindle’s length is independent of all others.\nThe variability in measured spindle length is Normally distributed.\n\nWith these assumptions, we can write the probability density function for \\(l_i\\) as\n\\[\\begin{align}\nf(l_i ; \\phi, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\exp\\left[-\\frac{(l_i - \\phi)^2}{2\\sigma^2}\\right].\n\\end{align}\\]\nSince each measurement is independent, we can write likelihood, the joint probability density function of the entire data set, which we will define as \\(\\mathbf{l} = \\{l_1, l_2,\\ldots\\}\\), consisting of \\(n\\) total measurements.\n\\[\\begin{align}\nf(\\mathbf{l} ; \\phi, \\sigma) = \\prod_{i} f(l_i ; \\phi, \\sigma) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}}\\,\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i}(l_i - \\phi)^2\\right].\n\\end{align}\\]\nWe can write the likelihood more succinctly, and perhaps more intuitively, as\n\\[\\begin{align}\nl_i \\sim \\text{Norm}(\\phi, \\sigma) \\;\\;\\forall i.\n\\end{align}\\]\nWe will generally write our models in this format, which is easier to parse and understand. Note that in writing this generative model, we have necessarily introduced another parameter, \\(\\sigma\\), the standard deviation parametrizing the Normal distribution. So, we have two parameters in our model, \\(\\phi\\) and \\(\\sigma\\).\n\n\n\n34.2.2 Model 2: Spindle length is set by total amount of tubulin\n\n34.2.2.1 The cartoon model\nThe three key principles of this “cartoon” model are:\n\nThe total amount of tubulin in the droplet or cell is conserved.\nThe total length of polymerized microtubules is a function of the total tubulin concentration after assembly of the spindle. This results from the balance of microtubule polymerization rate with catastrophe frequencies.\nThe density of tubulin in the spindle is independent of droplet or cell volume.\n\n\n\n34.2.2.2 The mathematical model\nFrom these principles, we need to derive a mathematical model that will provide us with testable predictions. The derivation follows below (following the derivation presented in the paper), and you may read it if you are interested. Since our main focus here is building a statistical model, you can skip ahead to to the final equation, where we define a mathematical expression relating the spindle length, \\(l\\) to the droplet diameter, \\(d\\), which depends on two parameters, \\(\\gamma\\) and \\(\\phi\\). Nonetheless, it is important to see how a models such as this one is derived.\nPrinciple 1 above (conservation of tubulin) implies\n\\[\\begin{align}\nT_0 V_0 = T_1(V_0 - V_\\mathrm{s}) + T_\\mathrm{s}V_\\mathrm{s},\n\\end{align}\\]\nwhere \\(V_0\\) is the volume of the droplet or cell, \\(V_\\mathrm{s}\\) is the volume of the spindle, \\(T_0\\) is the total tubulin concentration (polymerized or not), \\(T_1\\) is the tubulin concentration in the cytoplasm after the the spindle has formed, and \\(T_\\mathrm{s}\\) is the concentration of tubulin in the spindle. If we assume the spindle does not take up much of the total volume of the droplet or cell (\\(V_0 \\gg V_\\mathrm{s}\\), which is the case as we will see when we look at the data), we have\n\\[\\begin{align}\nT_1 \\approx T_0 - \\frac{V_\\mathrm{s}}{V_0}\\,T_\\mathrm{s}.\n\\end{align}\\]\nThe amount of tubulin in the spindle can we written in terms of the total length of polymerized microtubules, \\(L_\\mathrm{MT}\\) as\n\\[\\begin{align}\nT_s V_\\mathrm{s} = \\alpha L_\\mathrm{MT},\n\\end{align}\\]\nwhere \\(\\alpha\\) is the tubulin concentration per unit microtubule length. (We will see that it is unimportant, but from the known geometry of microtubules, \\(\\alpha \\approx 2.7\\) nmol/µm.)\nWe now formalize assumption 2 into a mathematical expression. Microtubule length should grow with increasing \\(T_1\\). There should also be a minimal threshold \\(T_\\mathrm{min}\\) where polymerization stops. We therefore approximate the total microtubule length as a linear function,\n\\[\\begin{aligned}\n\\begin{align}\nL_\\mathrm{MT} \\approx \\left\\{\\begin{array}{ccl}\n0 & &T_1 \\le T_\\mathrm{min} \\\\\n\\beta(T_1 - T_\\mathrm{min}) & & T_1 &gt; T_\\mathrm{min}.\n\\end{array}\\right.\n\\end{align}\n\\end{aligned}\\]\nBecause spindles form in Xenopus extract, \\(T_0 &gt; T_\\mathrm{min}\\), so there exists a \\(T_1\\) with \\(T_\\mathrm{min} &lt; T_1 &lt; T_0\\). Thus, going forward, we are assured that \\(T_1 &gt; T_\\mathrm{min}\\). So, we have\n\\[\\begin{align}\nV_\\mathrm{s} \\approx \\alpha\\beta\\,\\frac{T_1 - T_\\mathrm{min}}{T_\\mathrm{s}}.\n\\end{align}\\]\nWith insertion of our expression for \\(T_1\\), this becomes\n\\[\\begin{align}\nV_{\\mathrm{s}} \\approx \\alpha \\beta\\left(\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}} - \\frac{V_\\mathrm{s}}{V_0}\\right).\n\\end{align}\\]\nSolving for \\(V_\\mathrm{s}\\), we have\n\\[\\begin{align}\nV_\\mathrm{s} \\approx \\frac{\\alpha\\beta}{1 + \\alpha\\beta/V_0}\\,\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\n=\\frac{V_0}{1 + V_0/\\alpha\\beta}\\,\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}.\n\\end{align}\\]\nWe approximate the shape of the spindle as a prolate spheroid with major axis length \\(l\\) and minor axis length \\(w\\), giving\n\\[\\begin{align}\nV_\\mathrm{s} = \\frac{\\pi}{6}\\,l w^2 = \\frac{\\pi}{6}\\,k^2 l^3,\n\\end{align}\\]\nwhere \\(k \\equiv w/l\\) is the aspect ratio of the spindle. We can now write an expression for the spindle length as\n\\[\\begin{align}\nl \\approx \\left(\\frac{6}{\\pi k^2}\\,\n\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\\,\n\\frac{V_0}{1+V_0/\\alpha\\beta}\\right)^{\\frac{1}{3}}.\n\\end{align}\\]\nFor small droplets, with \\(V_0\\ll \\alpha \\beta\\), this becomes\n\\[\\begin{align}\nl \\approx \\left(\\frac{6}{\\pi k^2}\\,\n\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\\,\nV_0\\right)^{\\frac{1}{3}}\n= \\left(\\frac{T_0 - T_\\mathrm{min}}{k^2T_\\mathrm{s}}\\right)^{\\frac{1}{3}}\\,d,\n\\end{align}\\]\nwhere \\(d\\) is the diameter of the spherical droplet or cell. So, we expect the spindle size to increase linearly with the droplet diameter for small droplets.\nFor large \\(V_0\\), the spindle size becomes independent of droplet size;\n\\[\\begin{align}\nl \\approx \\left(\\frac{6 \\alpha \\beta}{\\pi k^2}\\,\n\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\\right)^{\\frac{1}{3}}.\n\\end{align}\\]\n\n\n34.2.2.3 Indentifiability of parameters\nWe measure the microtubule length \\(l\\) and droplet diameter \\(d\\) directly from the images. We can also measure the spindle aspect ratio \\(k\\) directly from the images. Thus, we have four unknown parameters, since we already know α ≈ 2.7 nmol/µm. The unknown parameters are:\n\n\n\nrameter me\naning\n\n\n\n\n\\(\\beta\\)\nrate constant for MT growth\n\n\n\\(T_0\\)\ntotal tubulin concentration\n\n\n\\(T_\\mathrm{min}\\)\ncritical tubulin concentration for polymerization\n\n\n\\(T_s\\)\ntubulin concentration in the spindle\n\n\n\nWe would like to determine all of these parameters. We could measure them all either in this experiment or in other experiments. We could measure the total tubulin concentration \\(T_0\\) by doing spectroscopic or other quantitative methods on the Xenopus extract. We can \\(T_\\mathrm{min}\\) and \\(T_s\\) might be assessed by other in vitro assays, though these parameters may by strongly dependent on the conditions of the extract.\nImportantly, though, the parameters only appear in combinations with each other in our theoretical model. Specifically, we can define two parameters,\n\\[\\begin{aligned}\n\\begin{align}\n\\gamma &= \\left(\\frac{T_0-T_\\mathrm{min}}{k^2T_\\mathrm{s}}\\right)^\\frac{1}{3} \\\\\n\\phi &= \\gamma\\left(\\frac{6\\alpha\\beta}{\\pi}\\right)^{\\frac{1}{3}}.\n\\end{align}\n\\end{aligned}\\]\nWe can then rewrite the general model expression in terms of these parameters as\n\\[\\begin{align}\nl(d) \\approx \\frac{\\gamma d}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}}.\n\\end{align}\\]\nIf we tried to determine all four parameters from this experiment only, we would be in trouble. This experiment alone cannot distinguish all of the parameters. Rather, we can only distinguish two combinations of them, which we have defined as \\(\\gamma\\) and \\(\\phi\\). This is an issue of identifiability. We may not be able to distinguish all parameters in a given model, and it is important to think carefully before the analysis about which ones we can identify. Even with our work so far, we are not quite done with characterizing identifiability.\nThis model is a variate-covariate model in which the spindle length is the variate and the droplet diameter is the covariate. The parameters \\(\\gamma\\) and \\(\\phi\\) parametrize the function \\(l(d)\\), which describes how the variate \\(l\\) depends on the covariate \\(d\\).\nNote that variate-covariate modeling and the associated parameter estimation is often referred to as regression. I avoid the use of this term because it is a historical artifact and describes a subset of variate-covariate models. You can read more about the terminology in this short historical description by Michael Betancourt.\nFinding MLEs of parameters for variate-covariate models is also often referred to as curve fitting. I also avoid that term because I prefer the more descriptive term “maximum likelihood estimation,” which further emphasizes that it is the same procedure we have been doing for other models beyond variate-covariate models.\n\n\n34.2.2.4 Visualizing the mathematical model\nLet’s take a quick look at the mathematical model so we can see how the curve looks. It is best to nondimensionalize the diameter by \\(\\phi\\), giving\n\\[\\begin{align}\n\\frac{l(d)}{\\phi} \\approx \\frac{\\gamma d/\\phi}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}}.\n\\end{align}\\]\nSo, we will plot \\(l(d/\\phi)/\\phi\\) versus \\(d/\\phi\\), which means we choose units of length to be \\(\\phi\\).\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe curve grows from zero to a plateau at \\(l = \\phi\\), more rapidly for larger \\(\\gamma\\). We can more carefully characterize the limiting behavior.\n\n\n34.2.2.5 Limiting behavior\nFor large droplets, with \\(d \\gg \\phi/\\gamma\\), the spindle size becomes independent of \\(d\\), with\n\\[\\begin{align}\nl \\approx \\phi.\n\\end{align}\\]\nConversely, for \\(d \\ll \\phi/\\gamma\\), the spindle length varies approximately linearly with diameter.\n\\[\\begin{align}\nl(d) \\approx \\gamma\\,d.\n\\end{align}\\]\nNote that the expression for the linear regime gives bounds for \\(\\gamma\\). Obviously, \\(\\gamma &gt; 0\\), lest we get negative spindle lengths. Because \\(l \\le d\\), lest the spindle not fit in the droplet, we also have \\(\\gamma \\le 1\\).\nImportantly, if the experiment is done in the regime where \\(d\\) is large (and we do not really know a priori how large that is since we do not know the parameters \\(\\phi\\) and \\(\\gamma\\)), we cannot tell the difference between the two models, since they are equivalent in that regime. Further, if the experiment is in this regime the model is unidentifiable because we cannot resolve \\(\\gamma\\).\nThis sounds kind of dire, but this is actually a convenient fact. The second model is more complex, but it has the simpler model, model 1, as a limit. Thus, the two models are in fact commensurate with each other. Knowledge of how these limits work also enhances the experimental design. We should strive for small droplets. And perhaps most importantly, if we didn’t consider the second model, we might automatically assume that droplet size has nothing to do with spindle length if we simply did the experiment in larger droplets.\n\n\n34.2.2.6 Likelihood for the generative model\nWe have a theoretical model relating the droplet diameter to the spindle length. Let us now build a generative model. To do so, we need to specify how the measured spindle lengths are distributed.\nWe start by assuming that the measured droplet length-diameter pairs are independent and identically distributed (i.i.d.). This is often a reasonable assumption in variate-covariate models, and certainly seems so in this one. The i.i.d. assumption may break down, for example in some models in which the variate is time and subsequent data points are depended upon each other. Nonetheless, even for many time series, the i.i.d. assumption is reasonable, especially because many processes are memoryless.\nTaking the i.i.d. assumption as given, we now note that the measured spindle length for a given droplet diameter will be close to the theoretical curve, but will not be exactly. The difference between the theoretical curve and the measured value is called a residual. More generally, a residual is the difference between the theoretical value of a covariate and the measured value of a covariate for a given value of the variate(s). So, to complete our generative model, we need to specify how the residuals are distributed.\nFor spindle, droplet pair i, we assume\n\\[\\begin{align}\nl_i = \\frac{\\gamma d_i}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}} + e_i,\n\\end{align}\\]\nwhere \\(e_i\\) is the residual for point i. We could model the residual in many ways. Here are some examples.\n\nHomoscedastic, Normally distributed residuals: Each data point varies from the theoretical curve according to a Normal distribution with the same variance \\(\\sigma^2\\), such that \\(e_i \\sim \\text{Norm}(0, \\sigma)\\;\\forall i\\).\nHeteroscedastic, Normally distributed residuals: Each data point varies from the theoretical curve according to a Normal distribution with the a variance that is unique to each data point, \\(\\sigma_i^2\\), such that \\(e_i \\sim \\text{Norm}(0, \\sigma_i)\\). This requires an expression for \\(\\sigma_i\\) for each datum. Commonly, the residuals may be proportional to magnitude of the covariate such that \\(\\sigma_i = \\sigma_0\\left|\\mu_i\\right|\\), where \\(\\mu_i\\) is the value of the theoretical curve for data point i, and \\(\\sigma_0\\) is a parameter describing how the values of \\(\\sigma_i\\) scale.\nHomoscedastic, Student-t distributed residuals: Each data point varies from the theoretical curve according to a Student-t distribution with the same scale parameter \\(\\sigma\\) and shape parameter \\(\\nu\\), such that \\(e_i \\sim \\text{Student-t}(\\nu, 0, \\sigma)\\;\\forall i\\). This is commonly used when you suspect there may be strong variation from the theoretical curve, and you need a heavier-tailed distribution to describe the residuals.\n\nHomoscedastic, Normally distributed residuals is the most commonly used model, and we will adopt that here.\n\\[\\begin{aligned}\n\\begin{align}\n&\\mu_i =  \\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\\\[1em]\n&e_i \\sim \\text{Norm}(0, \\sigma);\\forall i,\\\\[1em]\n&l_i = \\mu_i + e_i \\;\\forall i,\n\\end{align}\n\\end{aligned}\\]\nwhich can be equivalently stated as\n\\[\\begin{aligned}\n\\begin{align}\n&\\mu_i =  \\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\\\[1em]\n&l_i \\sim \\text{Norm}(\\mu_i, \\sigma) \\;\\forall i,\n\\end{align}\n\\end{aligned}\\]\nor even more compactly as\n\\[\\begin{align}\nl_i \\sim \\text{Norm}\\left(\\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\sigma\\right) \\;\\forall i.\n\\end{align}\\]\nImportantly, note that this model builds upon our first model. Generally, when doing modeling, it is a good idea to build more complex models on your initial baseline model such that the models are related to each other by limiting behavior. This gives you a continuum of model and a sound basis for making comparisons among models.\nNote that we are assuming the droplet diameters are known. When we generate data sets for prior predictive checks, we will randomly generate them from about 20 µm to 200 µm, since this is the range achievable with the microfluidic device.\n\n\n34.2.2.7 Checking model assumptions\nIn deriving the mathematical model, we made a series of assumptions. It is generally a good idea to check to see if assumptions in the mathematical modeling are realized in the experiment. If they are not, you may need to relax the assumptions and have a potentially more complicated model (which may suffer from identifiability issues). This underscores the interconnection between modeling and experimental design. You can allow for modeling assumptions and identifiability if you design your experimental parameters to meet the assumptions (e.g., choosing the appropriate range of droplet sizes).\n\n34.2.2.7.1 Is \\(V_\\mathrm{s} / V_0 \\ll 1\\)?\nLet’s do a quick verification that the droplet volume is indeed much larger than the spindle volume. Remember, the spindle volume for a prolate spheroid of length \\(l\\) and width \\(w\\) is \\(V_\\mathrm{s} = \\pi l w^2 / 6\\).\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that for pretty much all spindles that were measured, \\(V_\\mathrm{s} / V_0\\) is small, so this is a sound assumption.\n\n\n34.2.2.7.2 Do all spindles have the same aspect ratio \\(k\\)?\nIn setting up our model, we assumed that all spindles had the same aspect ratio. We can check this assumption because we have the data to do so available to us.\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe median aspect ratio is about 0.4, and we see spindle lengths about \\(\\pm 25\\%\\) of that. This could be significant variation. We may wish to update the model to account for nonconstant \\(k\\). Going forward, we will assume \\(k\\) is constant, but you may wish to perform the analysis that follows with nonconstant \\(k\\) as an exercise.\nImportantly, these checks of the model highlight the importance of checking your assumptions against your data. Always a good idea!",
    "crumbs": [
      "Variate-covariate models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model building</span>"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/variate_covariate_with_stan.html",
    "href": "lessons/variate_covariate/variate_covariate_with_stan.html",
    "title": "35  Variate-covariate models with MCMC",
    "section": "",
    "text": "35.1 Full generative model\n| Download notebook\nData set download\nIn this lesson, we will perform parameter estimation using a variate-covariate model. We will use the data set from Good, et al., which investigates how the length of mitotic spindles varies with the size of the encapsulating droplet.\nWe start by loading in the data set and making a quick plot to remind us of the data set.\nWe have previously worked out a likelihood for the conserved tubulin model. In this model, the spindle length \\(l\\) is a function of droplet diameter \\(d\\) according to\n\\[\\begin{align}\nl(d;\\gamma, \\phi) = \\frac{\\gamma d}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}}.\n\\end{align}\\]\nWe will model the variation off of this curve as Normally distributed (assuming homoscedasticity), such that our likelihood is defined as\n\\[\\begin{align}\nl_i \\mid d_i, \\gamma, \\phi, \\sigma \\sim \\text{Norm}(l(d_i;\\gamma, \\phi), \\sigma) \\;\\forall i.\n\\end{align}\\]\nWe are now left to choose priors to complete the model. We will build the prior for the three parameters \\(\\phi\\), \\(\\gamma\\), and \\(\\sigma\\), taking each to be independent of the others. Let’s start with \\(\\sigma\\). It is possible that the spindle size is very carefully controlled. It is also possible that it could be highly variable. So, we can choose a Half Normal prior for \\(\\sigma\\) with a large scale parameter of 10 µm; \\(\\sigma \\sim \\text{HalfNorm}(10)\\). For \\(\\gamma\\), we know the physically is must be between zero and one (as we have worked out in previous lessons), so we will assume a prior on is close to uniform that domain, but with probability density dropping right at zero and one; \\(\\gamma \\sim \\text{Beta}(1.1, 1.1)\\). For \\(\\phi\\), the typical largest spindle size, we assume a smallest spindle of about a micron (right where I would start to feel uncomfortable betting the farm) and the largest spindle to be about a millimeter (the size of a very large cell, like a Xenopus egg). We therefore take \\(\\log_{10}\\phi \\sim \\text{Norm}(1.5, 0.75)\\), where \\(\\phi\\) is in units of microns.\nWe thus have our complete generative model.\n\\[\\begin{align}\n&\\log_{10} \\phi \\sim \\text{Norm}(1.5, 0.75),\\\\[1em]\n&\\gamma \\sim \\text{Beta}(1.1, 1.1), \\\\[1em]\n&\\sigma \\sim \\text{HalfNorm}(10),\\\\[1em]\n&\\mu_i =  \\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\\\[1em]\n&l_i \\mid d_i, \\gamma, \\phi, \\sigma \\sim \\text{Norm}(\\mu_i, \\sigma) \\;\\forall i.\n\\end{align}\\]",
    "crumbs": [
      "Variate-covariate models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Variate-covariate models with MCMC</span>"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/variate_covariate_with_stan.html#using-stan-to-sample",
    "href": "lessons/variate_covariate/variate_covariate_with_stan.html#using-stan-to-sample",
    "title": "35  Variate-covariate models with MCMC",
    "section": "35.2 Using Stan to sample",
    "text": "35.2 Using Stan to sample\nThe Stan code we will use is below.\nfunctions {\n  real ell_theor(real d, real phi, real gamma_) {\n    real denom_ratio = (gamma_ * d / phi)^3;\n    return gamma_ * d / cbrt(1 + denom_ratio);\n  }\n}\n\n\ndata {\n  int N;\n  int N_ppc;\n  array[N] real d;\n  array[N_ppc] real d_ppc;\n  array[N] real ell;\n}\n\n\nparameters {\n  real log10_phi;\n  real&lt;lower=0, upper=1&gt; gamma_;\n  real&lt;lower=0&gt; sigma;\n}\n\n\ntransformed parameters {\n  real phi = 10 ^ log10_phi;\n\n  array[N] real mu;\n\n  for (i in 1:N) {\n    mu[i] = ell_theor(d[i], phi, gamma_);\n  }\n}\n\n\nmodel {\n  // Priors\n  log10_phi ~ normal(1.5, 0.75);\n  gamma_ ~ beta(1.1, 1.1);\n  sigma ~ normal(0.0, 10.0);\n\n  // Likelihood\n  ell ~ normal(mu, sigma);\n}\n\n\ngenerated quantities {\n  array[N_ppc] real ell_ppc;\n\nfor (i in 1:N_ppc) {\n    real mu_ppc = ell_theor(d_ppc[i], phi, gamma_);\n    ell_ppc[i] = normal_rng(mu_ppc, sigma);\n  }\n}\nI pause to point out a few syntactical elements.\n\nNote how I used gamma_ as the variable name for gamma. This is because gamma is used as a distribution name in Stan.\nI have used the functions block in this Stan program. The definition of the function is real ell_theor(real d, real phi, real gamma_) { }, where the code to be executed in the function is between the braces. This is how you declare a function in Stan. The first word, real specifies what data type is expected to be returned. Then comes the name of the function, followed by its arguments in parentheses preceded by their data type.\nWithin the function, I had to declare denom_ratio to be real. Remember, Stan is statically typed, so every variable needs a declaration of its type.\nThe return statement is like Python; return followed by a space and then an expression for the value to be returned.\nThere is no Half-Normal distribution in Stan. It is implemented by putting a bound on the variable (in this case real&lt;lower=0&gt; sigma;) and then model sigma as being Normally distributed with location parameter zero.\nWe computed an array of mu values in the transformed parameters block, which enabled us to use the simpler sampling statement of ell ~ normal(mu, sigma).\nWe are also specifying for what values of the covariate d we want to have posterior predictive values for the spindly length. We do not need to have the covariate be exactly the same as what was used for the experimental observations. We simply need to have the covariate vary over the range of the experimental observations.\n\nAll right! Let’s do some sampling!\n\nN_ppc = 200\nd_ppc = np.linspace(0.1, 250, N_ppc)\ndata = {\n    \"N\": len(ell),\n    \"d\": d,\n    \"ell\": ell,\n    \"N_ppc\": N_ppc,\n    \"d_ppc\": d_ppc,\n}\n\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='cons_tubulin_model.stan')\n    \n    samples = sm.sample(\n        data=data,\n        chains=4,\n        iter_sampling=1000\n    )\n\nsamples = az.from_cmdstanpy(samples, posterior_predictive='ell_ppc')\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nWith samples in hand, let’s make a plot of the posterior samples.\n\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples,\n        parameters=[(\"phi\", \"ϕ (µm)\"), (\"gamma_\", \"γ\"), (\"sigma\", \"σ (µm)\")],\n        xtick_label_orientation=np.pi/4,\n    )\n)\n\n\n  \n\n\n\n\n\nThe corner plot provides a complete picture of the posterior and allows for direct assessment of confidence regions. Apparently, we have that \\(\\phi \\approx 38.25\\) µm, \\(\\gamma \\approx 0.86\\), and \\(\\sigma \\approx 3.75\\) µm.",
    "crumbs": [
      "Variate-covariate models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Variate-covariate models with MCMC</span>"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/variate_covariate_with_stan.html#posterior-predictive-checks",
    "href": "lessons/variate_covariate/variate_covariate_with_stan.html#posterior-predictive-checks",
    "title": "35  Variate-covariate models with MCMC",
    "section": "35.3 Posterior predictive checks",
    "text": "35.3 Posterior predictive checks\nLet’s make a plot for a graphical posterior predictive check. Because we have a covariate, we want to make a plot of what values we might get for the variate (spindle length) for various values of the covariate (droplet diameter). We can make sure a plot using the bebi103.viz.predictive_regression() function.\n\nell_ppc = (\n    samples.posterior_predictive[\"ell_ppc\"]\n    .stack({\"sample\": (\"chain\", \"draw\")})\n    .transpose(\"sample\", \"ell_ppc_dim_0\")\n)\n\nbokeh.io.show(\n    bebi103.viz.predictive_regression(\n        ell_ppc,\n        samples_x=d_ppc,\n        percentiles=[30, 50, 70, 99],\n        data=np.vstack((d, ell)).transpose(),\n        x_axis_label=\"droplet diameter [µm]\",\n        y_axis_label=\"spindle length [µm]\",\n        x_range=[0, 250],\n    )\n)\n\n\n  \n\n\n\n\n\nIndeed, the observed measurements are conistent with the model.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Variate-covariate models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Variate-covariate models with MCMC</span>"
    ]
  },
  {
    "objectID": "lessons/variate_covariate/variate_covariate_with_stan.html#computing-environment",
    "href": "lessons/variate_covariate/variate_covariate_with_stan.html#computing-environment",
    "title": "35  Variate-covariate models with MCMC",
    "section": "35.4 Computing environment",
    "text": "35.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,cmdstanpy,arviz,bokeh,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\ncmdstanpy : 1.2.5\narviz     : 0.22.0\nbokeh     : 3.7.3\nbebi103   : 0.1.28\njupyterlab: 4.4.5\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Variate-covariate models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Variate-covariate models with MCMC</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/optimization.html",
    "href": "lessons/optimization/optimization.html",
    "title": "Summarizing posterior distributions with maxima",
    "section": "",
    "text": "We have seen that writing a mathematical expression for a posterior distribution is relatively easy. The name of the inference game is making sense of this expression. Sampling via Markov chain Monte Carlo is a powerful way to do that, but can have prohibitive computational cost.\nTo get a much cruder picture of a posterior distribution, we can find the parameter values that maximize the posterior PDF (or, rarely, PMF). We can then locally approximate the posterior as Normal. Such a summary is far inferior to what we can obtain with MCMC, but is useful when practical considerations become important. In fact, optimization-based techniques, where we attempt to summarize a posterior distribution with its maximum, are by far the most widely used approaches.",
    "crumbs": [
      "Summarizing posterior distributions with maxima"
    ]
  },
  {
    "objectID": "lessons/optimization/optimization_basics.html",
    "href": "lessons/optimization/optimization_basics.html",
    "title": "36  Bayesian approach to parameter estimation by optimization",
    "section": "",
    "text": "36.1 Summarizing the posterior near its maximum\n| Download notebook\nWe have learned in previous lessons that the posterior distribution for a set of parameters \\(\\theta\\) given a data set \\(y\\) is given by the likelihood \\(f(y\\mid \\theta)\\) and prior \\(g(\\theta)\\) according to\n\\[\n\\begin{align}\ng(\\theta\\mid y) \\propto f(y\\mid \\theta)\\,g(\\theta).\n\\end{align}\n\\tag{36.1}\\]\nThe central goal of Bayesian inference is making sense of this posterior distribution; writing it down is the easy part. We have learned how to make sense of the posterior using sampling. Now, we will consider an alternative, generally far inferior but often nevertheless necessary, approach: optimization.\nFor many posteriors, there exists a set of parameters for which the posterior is maximal. This set of parameters, denoted \\(\\theta^*\\), is referred to as the maximal a posteriori parameter set, abbreviated MAP. Given that \\(\\theta^*\\) maximizes the posterior, it is of some importance (but certainly not paramount importance), and may be useful in summarizing a posterior.\nFinding the MAP is an optimization problem. In some simple, rare cases, the optimization problem may be solved analytically, though we will mostly focus on numerical methods. Specifically, we will discuss numerical methods for finding \\(\\theta^*\\) for continuous \\(\\theta\\) as we proceed through this lesson. For now, we will proceed assuming we have found \\(\\theta^*\\) and seek to approximate the posterior distribution near the MAP.\nNear the MAP, we can approximately express the log posterior as a Taylor expansion truncated to second order,\n\\[\\begin{align}\n\\ln g(\\theta\\mid y) \\approx \\ln g(\\theta^*\\mid y) + \\frac{1}{2}\\left(\\theta - \\theta^*\\right)^\\mathsf{T} \\cdot \\mathsf{B} \\cdot \\left(\\theta - \\theta^*\\right),\n\\end{align}\n\\]\nwhere \\(\\mathsf{B}\\) is the symmetric Hessian matrix of second derivatives, with entries\n\\[\\begin{align}\nB_{ij} = \\left.\\frac{\\partial^2\\,\\ln g(\\theta\\mid y)}{\\partial \\theta_i\\,\\partial \\theta_j}\\right|_{\\theta = \\theta^*}.\n\\end{align}\n\\]\nIn one dimension, this is\n\\[\\begin{align}\n\\ln g(\\theta\\mid y) \\approx \\ln g(\\theta^*\\mid y) + \\frac{1}{2}\\left(\\left.\\frac{\\partial^2\\,\\ln g(\\theta\\mid y)}{\\partial \\theta^2}\\right|_{\\theta = \\theta^*}\\right) \\left(\\theta - \\theta^*\\right)^2 = \\ln g(\\theta^*\\mid y) - \\frac{\\left(\\theta - \\theta^*\\right)^2}{2\\sigma^2},\n\\end{align}\n\\]\nwhere we have defined\n\\[\\begin{align}\n\\sigma^2 = -\\left(\\left.\\frac{\\partial^2\\,\\ln g(\\theta\\mid y)}{\\partial \\theta^2}\\right|_{\\theta = \\theta^*}\\right)^{-1}.\n\\end{align}\n\\]\nWe note that the \\(-(\\theta-\\theta^*)^2/2\\sigma^2\\) term is the \\(\\theta\\)-dependence of the logarithm of the PDF of a Normal distribution! We can therefore locally approximate the posterior distribution as Normal near the MAP;\n\\[\\begin{align}\ng(\\theta\\mid y) \\approx \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\exp\\left[-\\frac{\\left(\\theta - \\theta^*\\right)^2}{2\\sigma^2}\\right].\n\\end{align}\n\\]\nThis generalizes to multiple dimensions, where the covariance matrix \\(\\mathsf{\\Sigma}\\) of the approximate multivariate Normal distribution is related to the Hessian matrix by\n\\[\\begin{align}\n\\mathsf{\\Sigma} = - \\mathsf{B}^{-1}.\n\\end{align}\n\\]\nNote that the covariance matrix is not the inverse of every element of the Hessian; it is the inverse of the Hessian matrix. Note also that optimization theory tells us that because the Hessian is evaluated at \\(\\theta^*\\), which maximizes the posterior, the Hessian must be negative definite. Therefore, the covariance matrix must be positive definite, which we already know must by the case for a multivariate Normal distribution.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian approach to parameter estimation by optimization</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/optimization_basics.html#summarizing-the-posterior-near-its-maximum",
    "href": "lessons/optimization/optimization_basics.html#summarizing-the-posterior-near-its-maximum",
    "title": "36  Bayesian approach to parameter estimation by optimization",
    "section": "",
    "text": "36.1.1 Demonstration of the Normal approximation\nActually, the probability density function of any continuous, peaked distribution can be approximated as Normal near a peak. We can demonstrate this fact by approximating a Gamma distribution locally as Normal. Recall that the PDF for a Gamma distribution is\n\\[\\begin{align}\nf(x ; \\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}\\,\\frac{(\\beta x)^\\alpha}{x}\\,\\mathrm{e}^{-\\beta x}.\n\\end{align}\\]\nWe can analytically find the mode by setting\n\\[\\begin{align}\n\\frac{\\mathrm{d}\\,\\ln f}{\\mathrm{d}x} = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left((\\alpha-1)\\ln x - \\beta x\\right) = \\frac{\\alpha-1}{x} - \\beta = 0,\n\\end{align}\\]\nand solving to get \\(x^* = (\\alpha - 1) / \\beta\\). We can also compute the second derivative as\n\\[\\begin{align}\n\\frac{\\mathrm{d}^2\\,\\ln f}{\\mathrm{d}x^2} = -\\frac{\\alpha - 1}{x^2},\n\\end{align}\\]\nwhich evaluated at \\(x^*\\) is \\(-\\beta^2 / (\\alpha - 1)\\). Therefore, the scale parameter for the approximate Normal distribution is \\(\\sigma = \\sqrt{\\alpha - 1} / \\beta\\). We can plot the distribution and its Normal approximation as a visualization. We do this for \\(\\alpha = 50\\) and \\(\\beta = 20\\).\n\nx = np.linspace(1, 5, 400)\nalpha = 50\nbeta = 20\ny_gamma = st.gamma.pdf(x, alpha, loc=0, scale=1 / beta)\ny_norm = st.norm.pdf(x, (alpha - 1) / beta, np.sqrt(alpha - 1) / beta)\n\np = bokeh.plotting.figure(\n    frame_width=400,\n    frame_height=200,\n    x_range=[1, 5],\n    x_axis_label=\"x\",\n    y_axis_label=\"PDF\",\n)\n\np.line(x, y_gamma, line_width=2, legend_label=\"Gamma\")\np.line(x, y_norm, line_width=2, color=\"tomato\", legend_label=\"Normal\")\n\nbokeh.io.show(p)",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian approach to parameter estimation by optimization</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/optimization_basics.html#credible-intervals",
    "href": "lessons/optimization/optimization_basics.html#credible-intervals",
    "title": "36  Bayesian approach to parameter estimation by optimization",
    "section": "36.2 Credible intervals",
    "text": "36.2 Credible intervals\nA \\(\\alpha\\)-percent Bayesian credible interval for a parameter is an an interval that contains \\(\\alpha\\) percent of the posterior probability density. (This is different form a frequentist confidence interval, though a confidence interval is often erroneously interpreted as a credible interval.) For example, if \\(g(\\theta \\mid y)\\) is the marginalized posterior probability density function for a single parameter \\(\\theta\\) and \\(G(\\theta\\mid y)\\) is the cumulative distribution function, then a 95% credible interval is \\([G^{-1}(0.025),\\;G^{-1}(0.975)]\\).\nIf we approximate the posterior as a multivariate Normal distribution, we have analytical results for credible intervals for each parameter. This is the main benefit of using optimization; it affords us an approximate (Normal) expression for the posterior that has convenient, known analytical results. This enables us to make sense of the posterior.\nSpecifically, if \\(\\Sigma_{ii}\\) is a diagonal element in the covariance matrix \\(\\mathsf{\\Sigma}\\), then, e.g., the 95% credible interval for parameter \\(\\theta_i\\) is centered on the MAP estimate, \\(\\theta_i^*\\) and is \\([\\theta_i^* - 1.96\\sqrt{\\Sigma_{ii}},\\;\\theta_i^* + 1.96\\sqrt{\\Sigma_{ii}}]\\). Because the credible intervals are symmetric under this approximation, we can report the estimates as the \\(\\theta_i^* \\pm 1.96 \\sqrt{\\Sigma_{ii}}\\).\n\n36.2.1 Credible intervals may be skewed\nWe are approximating the PDF of a distribution close to its mode as Normal. However, for an asymmetric distribution, which posterior distributions can certainly be, the mode may not be all that relevant as far as credible regions go. As an extreme example, consider an Exponential distribution. It’s mode is zero, which is also the minimal allowed value. If we were to compute a central 95% credible region for a posterior that happened to be Exponential, the mode would not be included. In cases like these, though the Normal distribution approximately describes the PDF of the posterior near the MAP, the Normal approximation can lead to rather flawed credible regions.\nTo demonstrate this, let us again return to the Gamma example, this time with \\(\\alpha = 5\\) and \\(\\beta = 2\\). The mode of this Gamma distribution is 2, but the median is about 2.34. If we look graphically, we can see right away that the Normal approximation is not as good as the \\(\\alpha = 50\\), \\(\\beta = 20\\) case.\n\nx = np.linspace(0, 8, 400)\nalpha = 5\nbeta = 2\ny_gamma = st.gamma.pdf(x, alpha, loc=0, scale=1 / beta)\ny_norm = st.norm.pdf(x, (alpha - 1) / beta, np.sqrt(alpha - 1) / beta)\n\np = bokeh.plotting.figure(\n    frame_width=400,\n    frame_height=200,\n    x_range=[0, 8],\n    x_axis_label=\"x\",\n    y_axis_label=\"PDF\",\n)\n\np.line(x, y_gamma, line_width=2, legend_label=\"Gamma\")\np.line(x, y_norm, line_width=2, color=\"tomato\", legend_label=\"Normal\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nLet’s look at how this will affect the credible intervals. We will compute the lower and upper bound to a credible interval directly from the Gamma distribution and from its Normal approximation. We can compute credible intervals using the ppf() method of distributions in scipy.stats, which returns the inverse of the CDF. We can then plot the 1%, …, 99% credible intervals for the Gamma distribution and for its Normal approximation.\n\nperc_cred_int = np.linspace(1, 99, 200)\nmu = (alpha - 1) / beta\nsigma = np.sqrt(alpha - 1) / beta\n\ngamma_low = st.gamma.ppf((50 - perc_cred_int / 2) / 100, alpha, loc=0, scale=1 / beta)\ngamma_high = st.gamma.ppf((50 + perc_cred_int / 2) / 100, alpha, loc=0, scale=1 / beta)\nnorm_low = st.norm.ppf((50 - perc_cred_int / 2) / 100, mu, sigma)\nnorm_high = st.norm.ppf((50 + perc_cred_int / 2) / 100, mu, sigma)\n\np = bebi103.viz.fill_between(\n    gamma_low,\n    perc_cred_int,\n    gamma_high,\n    perc_cred_int,\n    patch_kwargs=dict(alpha=0.5),\n    x_axis_label=\"x\",\n    y_axis_label=\"percent credible interval\",\n    y_range=[0, 100],\n)\np = bebi103.viz.fill_between(\n    norm_low,\n    perc_cred_int,\n    norm_high,\n    perc_cred_int,\n    patch_kwargs=dict(alpha=0.5, color=\"tomato\"),\n    line_kwargs=dict(color=\"tomato\"),\n    p=p,\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nFor tiny credible intervals, the shift is clear. The median for a Gamma distribution with these parameters is shifted rightward from its mode, whereas the mode and median for a Normal distribution are equal. As we get broader credible intervals, the Normal approximation notably has the 99% credible interval containing negative values (not allowed for a Gamma distribution) and missing important values at the high end.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian approach to parameter estimation by optimization</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/optimization_basics.html#why-use-the-map-for-parameter-estimation",
    "href": "lessons/optimization/optimization_basics.html#why-use-the-map-for-parameter-estimation",
    "title": "36  Bayesian approach to parameter estimation by optimization",
    "section": "36.3 Why use the MAP for parameter estimation?",
    "text": "36.3 Why use the MAP for parameter estimation?\nDespite these drawbacks, using the MAP/Normal approximation for parameter estimation is still useful in Bayesian inference, at least for simple, well-behaved problems. Why? The most important reason is speed. For problems with simple models, optimization is much faster than other methods for characterizing the posterior, such as directly plotting it or using Markov chain Monte Carlo. If you have high-throughput data, you may not have the computational resources to do a full Bayesian treatment with other methods. Provided your model that is tractable by optimization method, finding the MAP and summarizing with an approximate Normal distribution is an attractive method.\nOptimization methods suffer from the curse of dimensionality. In general, high-dimensional optimization problems, which is the case for a large number of parameters, are very difficult to solve. However, a common practice is to choose models for which the optimization problems are more tractable, even with large data sets and/or large numbers of parameters. Note that this is a purely pragmatic approach! The generative model that is convenient is often not a well-reasoned generative model for the actual observations. Nonetheless, sometimes practicality is necessary, and MCMC and its full characterization of the posterior is out of reach, and you may need to turn to optimization methods. You have to take some water with your wine.\nIn many cases, the Normal approximation can also be a good approximation. But this is definitely not guaranteed! In real inference problems, it is very difficult to identify pathologies that would lead to breakdown of the Normal approximation, since comparing to the real posterior would involve computationally intensive methods to compute it.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian approach to parameter estimation by optimization</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/optimization_basics.html#computing-environment",
    "href": "lessons/optimization/optimization_basics.html#computing-environment",
    "title": "36  Bayesian approach to parameter estimation by optimization",
    "section": "36.4 Computing environment",
    "text": "36.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nbokeh     : 3.6.2\nbebi103   : 0.1.27\njupyterlab: 4.4.2",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian approach to parameter estimation by optimization</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/gamma_optimization.html",
    "href": "lessons/optimization/gamma_optimization.html",
    "title": "37  Parameter estimation by optimization case study: Gamma likelihood",
    "section": "",
    "text": "37.1 Exploratory data analysis\n| Download notebook\nData set download\nIn ?exr-gamma-spike, you considered a model for spiking in which interspike intervals are Gamma distributed. We will use that model here to demonstrate optimization procedures for MAP funding.\nWe’ll again make a quick ECDF of the data set as a remidner.\n# Load in as dataframe\ndf = pl.read_csv(os.path.join(data_path, 'rgc_spike_times_1.csv'))\n\n# Spike times in milliseconds for convenience\nspike_times = df['spike time (ms)'].to_numpy()\n\n# Interspike intervals\ny = np.concatenate(((spike_times[0],), np.diff(spike_times)))\n\n# Make ECDF\nbokeh.io.show(\n    iqplot.ecdf(y, 'interspike interval (ms)')\n)",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Parameter estimation by optimization case study: Gamma likelihood</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/gamma_optimization.html#the-model",
    "href": "lessons/optimization/gamma_optimization.html#the-model",
    "title": "37  Parameter estimation by optimization case study: Gamma likelihood",
    "section": "37.2 The model",
    "text": "37.2 The model\nWe will model the interspike intervals as being Gamma distributed with broad priors for the Gamma distribution parameters. Our model is as follows.\n\\[\\begin{align}\n&\\alpha \\sim \\text{HalfNorm(0, 100)} ,\\\\[1em]\n&\\beta \\sim \\text{HalfNorm(0, 100)} ,\\\\[1em]\n&y_i \\sim \\text{Gamma}(\\alpha, \\beta)\\;\\forall i.\n\\end{align}\n\\]\nWe will be doing optimization, but to have a gold standard for comparison, we can code this model up in Stan as follows, modifying the Stan code we wrote in Chapter 26 (which is essentially what you did in ?exr-gamma-spike).\ndata {\n    int&lt;lower=2&gt; n;\n    array[n] real spike_times;\n}\n\n\ntransformed data {\n    // Sorted spike times\n    array[n] real t = sort_asc(spike_times);\n\n    // Interspike intervals\n    array[n] real y;\n    y[1] = t[1];\n    for (i in 2:n) {\n        y[i] = t[i] - t[i-1];\n    }\n}\n\n\nparameters {\n    real&lt;lower=0&gt; alpha;\n    real&lt;lower=0&gt; beta_;\n}\n\n\nmodel {\n    // Priors\n    alpha ~ normal(0, 100);\n    beta_ ~ normal(0, 100);\n\n    // Likelihood\n    y ~ gamma(alpha, beta_);\n}\nLet’s compile, sample, and make a corner plot to get out standard of comparison.\n\ndata = dict(n=len(y), spike_times=spike_times)\n\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='gamma_spike.stan')\n    samples = az.from_cmdstanpy(sm.sample(data=data))\n\ncorner = bebi103.viz.corner(samples)\nbokeh.io.show(corner)",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Parameter estimation by optimization case study: Gamma likelihood</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/gamma_optimization.html#obtaining-map-parameters-by-optimization",
    "href": "lessons/optimization/gamma_optimization.html#obtaining-map-parameters-by-optimization",
    "title": "37  Parameter estimation by optimization case study: Gamma likelihood",
    "section": "37.3 Obtaining MAP parameters by optimization",
    "text": "37.3 Obtaining MAP parameters by optimization\nFinding the MAP amounts to an optimization problem in which we find the arguments (parameters) for which a function (the posterior distribution) is maximal. It is almost always much easier to find the maximum for the logarithm of the posterior. In fact, in the vast majority of Bayesian inference problems, we work with log posteriors instead of the posteriors themselves. Since the logarithm function is monotonic, a maximum of the log posterior is also a maximum of the posterior. So, our first step is code up a function that returns the log posterior.\nIn considering a log posterior, it is useful to know that\n\\[\\begin{align}\n\\ln g(\\theta \\mid y) = \\text{constant} + \\ln f(y \\mid \\theta) + \\ln g(\\theta).\n\\end{align}\\]\nFurthermore, if the priors for each parameter are independent, such that\n\\[\\begin{align}\ng(\\theta) = g(\\theta_1)\\, g(\\theta_2)\\cdots,\n\\end{align}\\]\nand the measurements are independent and identically distributed (i.i.d.), such that\n\\[\\begin{align}\nf(y\\mid \\theta) = f(y_1 \\mid \\theta)\\,f(y_2 \\mid \\theta)\\,f(y_3 \\mid \\theta)\\cdots,\n\\end{align}\\]\nthen\n\\[\\begin{align}\n\\ln g(\\theta \\mid y) = \\text{constant} + \\sum_i \\ln f(y_i \\mid \\theta) + \\sum_j \\ln g(\\theta_j).\n\\end{align}\\]\nThis means that we can construct the log posterior by summing up all of the terms in the log likelihood and log prior.\nFortunately, we do not have to hand-code any of the mathematical expressions for the log PDFs; we can use the built-in scipy.stats functions.\n\ndef log_prior(alpha, beta):\n    # If unphysical, return -infinity\n    if alpha &lt;= 0 or beta &lt;= 0:\n        return -np.inf\n    \n    lp = st.halfnorm.logpdf(alpha, 0.0, 100.0)\n    lp += st.halfnorm.logpdf(beta, 0.0, 100.0)\n\n    return lp\n\n\ndef log_likelihood(alpha, beta, y):\n    return np.sum(st.gamma.logpdf(y, alpha, loc=0, scale=1/beta))\n\n\ndef log_posterior(params, y):\n    # Unpack parameters\n    alpha, beta = params\n\n    lp = log_prior(alpha, beta)\n    if lp == -np.inf:\n        return lp\n\n    return lp + log_likelihood(alpha, beta, y)\n\nNow that we have the log posterior, we can perform numerical optimization. The function scipy.optimize.minimize() offers many options for algorithms, which you can read about in the documentation. I find that Powell’s method tends to work well. It does not rely on derivative information, so discontinuities don’t hurt, nor do the \\(-\\infty\\) values we get in the log posterior for parameter values that are out of bounds. That is particularly important for the present example because we have hard bounds on \\(\\alpha\\) and \\(\\beta\\); they must be positive. We could use constrained optimizers such as L-BFGS-B or COBYLA, but Powell’s method generally suffices. It is sometimes slow, though.\nAs an alternative, we could instead choose transformed parameters for \\(\\alpha\\) and \\(\\beta\\) that are unbounded. For example, we could let \\(a = \\log \\alpha\\) and \\(b = \\log \\beta\\) and find the optimal values of \\(a\\) and \\(b\\), which are unbounded. We could then use a more efficient unbounded optimizer like BFGS. For simplicity, we will just use Powell’s method, which is slower, but handles constraints without the need for transformations.\nThe optimizers offered by scipy.optimize.minimize() find minimizers, so we need to define our objective function as the negative log posterior. Since this is the objective function that will be passed into scipy.optimize.minimize(), we need to make sure it has the proper call signature. Specifically, the parameters that are being optimized, in this case \\(\\alpha\\) and \\(\\beta\\), need to be passed in as an array. All other arguments to the negative log posterior are passed in as separate arguments. In our case, that is the ISIs, y. (Note that we also built the log_posterior() function in this way, which is useful for when we compute the Hessian later.)\n\ndef neg_log_posterior(params, y):\n    return -log_posterior(params, y)\n\nNext, we have to provide an initial guess for parameter values. The optimization routine only finds a local maximum of the posterior and is not in general guaranteed to converge. Therefore, the initial guess can be very important. Looking at the data, we would expect \\(\\alpha\\) to be somewhere around 1 and for \\(\\beta\\) to be around 40 Hz.\n\n# Initial guess of alpha and beta\nparams_0 = np.array([1, 0.04])\n\nWe are now ready to use scipy.optimize.minimze() to compute the MAP. We use the args kwarg to pass in the other arguments to the neg_log_posterior() function. In our case, these arguments are the data points. The scipy.optimzie.minimize() function returns an object that has several attributes about the resulting optimization. Most important is x, the optimal parameter values.\n\n# Specify extra arguments into posterior function\nargs = (y,)\n\n# Compute the MAP\nres = scipy.optimize.minimize(\n    neg_log_posterior, params_0, args=args, method=\"powell\"\n)\n\n# Extract optimal parameters\npopt = res.x\n\n# For convenience...\nalpha_MAP, beta_MAP = popt\n\n# Print results\nprint(\n    \"\"\"Most probable parameters\n------------------------\nα = {0:.2f}\nβ = {1:.4f} kHz\"\"\".format(\n        alpha_MAP, beta_MAP\n    )\n)\n\nMost probable parameters\n------------------------\nα = 1.41\nβ = 0.0337 kHz\n\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/scipy/optimize/_optimize.py:2502: RuntimeWarning: invalid value encountered in scalar multiply\n  tmp2 = (x - v) * (fx - fw)\n\n\nWe have successfully found the MAP which provides just that; the maximally probable parameter values. This tells us only one piece of information about the posterior; where it is maximal. So we know where all of the first derivatives of the posterior are zero. We want to learn more about it, at least near the MAP. To that end, we can compute all of the second derivatives of the log posterior near the MAP. Using the second derivatives (the Hessian), we can start to understand the posterior, at least locally, by approximating it as Normal.\n\n37.3.1 Normal approximation of the posterior\nTo compute the Normal approximation of the posterior, our task is two-fold. First, find the parameter values that give the maximal posterior probability, which we have already done. Second, compute the Hessian of the log posterior at the MAP and invert it to get your covariance matrix. Note that the covariance matrix is not the inverse of every element of the Hessian; it is the inverse of the Hessian matrix. The credible interval for each parameter can then be calculated from the diagonal elements of the covariance matrix.\nTo compute the covariance matrix, we need to compute the Hessian of the log of the posterior at the MAP. We will do this numerically using the statsmodels.tools.numdiff module, which we imported as smnd. We use the function smnd.approx_hess() to compute the Hessian at the optimal parameter values. Note that since we already have the log posterior coded up and the MAP parameter values, we can just directly shove these into the numerical Hessian calculator.\n\nhes = smnd.approx_hess(popt, log_posterior, args=args)\n\nNow that we have the Hessian, we take its negative inverse to get the covariance matrix.\n\n# Compute the covariance matrix\ncov = -np.linalg.inv(hes)\n\n# Look at it\ncov\n\narray([[2.29386616e-03, 5.46728407e-05],\n       [5.46728407e-05, 1.86448377e-06]])\n\n\nThe diagonal terms give the approximate variance in the parameters in the posterior. The off-diagonal terms give the covariance, which describes how the parameters relate to each other. Nonzero covariance indicates that they are not completely independent.\n\n\n37.3.2 Credible intervals\nWe can compute the approximate 95% credible intervals for the parameters by multiplying the diagonal terms of the posterior covariance matrix \\(\\mathsf{\\Sigma}\\) by 1.96.\n\n# Report results\nprint(\"\"\"\nCredible intervals (≈ 95% of total probability density)\n-------------------------------------------------------\nα = {0:.2f} ± {1:.2f}\nβ = {2:.4f} ± {3:.4f} kHz\n\"\"\".format(alpha_MAP, 1.96*np.sqrt(cov[0,0]), beta_MAP, 1.96*np.sqrt(cov[1,1])))\n\n\nCredible intervals (≈ 95% of total probability density)\n-------------------------------------------------------\nα = 1.41 ± 0.09\nβ = 0.0337 ± 0.0027 kHz\n\n\n\n\n\n37.3.3 How good is the approximation?\nTo assess how close the Normal approximation is to the posterior, let’s plot the Normal approximation on top of the corner plot.\n\n# Extract plots\nalpha_p = corner.children[1].children[0].children[0]\nbeta_p = corner.children[1].children[1].children[1]\nsample_p = corner.children[1].children[1].children[0]\n\n# Set up ranges for plots; should be near MAP\nalpha = np.linspace(1.3, 1.55, 200)\nbeta = np.linspace(0.03, 0.038, 200)\n\n# Compute approximate normal posterior\npost_norm = np.empty((len(alpha), len(beta)))\nfor i in range(len(alpha)):\n    for j in range(len(beta)):\n        post_norm[i, j] = st.multivariate_normal.pdf(\n            (alpha[i], beta[j]), popt, cov\n        )\n\n# Approximate marginal posteriors\npost_alpha_norm = st.norm.pdf(alpha, popt[0], np.sqrt(cov[0, 0]))\npost_beta_norm = st.norm.pdf(beta, popt[1], np.sqrt(cov[1, 1]))\n\n# Add plots\nsample_p = bebi103.viz.contour(\n    alpha, \n    beta, post_norm, line_kwargs=dict(line_color=\"orange\"), p=sample_p\n)\nalpha_p.line(alpha, post_alpha_norm, line_color='orange', line_width=2)\nbeta_p.line(beta, post_beta_norm, line_color='orange', line_width=2)\n\n# Look!\nbokeh.io.show(corner)\n\n\n  \n\n\n\n\n\nand the exact marginalized posterior on the same contour plot. We can use scipy.stats.multivariate_normal.pdf() function to generate the PDF for the Normal approximation.\nThe calculation will take a while, since we have to compute the whole posterior in the neighborhood of the MAP to plot it. The whole reason we used optimization in the first place was to avoid this calculation! But we are doing it here to visualize the Normal approximation.\nThe Normal approximation, shown in orange, is not that far off from the posterior, especially at the peak. This is not always the case! This is one of the main dangers of using optimization, you are using very local information to summarize a posterior that may have relevant features away from the MAP. Despite the marginal distributions being close, we do see that the full posterior is slightly rotated; we do not completely correctly capture the covariance present in the posterior.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Parameter estimation by optimization case study: Gamma likelihood</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/gamma_optimization.html#sec-stan-opt",
    "href": "lessons/optimization/gamma_optimization.html#sec-stan-opt",
    "title": "37  Parameter estimation by optimization case study: Gamma likelihood",
    "section": "37.4 Optimization with Stan",
    "text": "37.4 Optimization with Stan\nStan does have functionality for finding the MAP (docs). To find a MAP, use the sm.optimize() function. By default, Stan with transform the parameters appropriately and use the BFGS algorithm. The optimized parameters can then be accesses as a dictionary using the optimized_params_dict attribute of the output.\n\nsm_opt = sm.optimize(data=data)\nsm_opt.optimized_params_dict\n\n01:09:52 - cmdstanpy - INFO - Chain [1] start processing\n01:09:52 - cmdstanpy - INFO - Chain [1] done processing\n\n\nOrderedDict([('lp__', np.float64(-6725.93)),\n             ('alpha', np.float64(1.41317)),\n             ('beta_', np.float64(0.0336819))])\n\n\nThe lp__ entry in the dictionary is not an optimum parameter value; it is the value of the log posterior at the MAP. We see that Stan gave us the same MAP as we got using SciPy. Note, though, that Stan does not compute the Hessian for us. To do that, which enables us to get credible regions, we need to code up the log posterior and numerically differentiate it. Since we need to code that up anyway, I usually use SciPy for MAP finding. SciPy also have more optimizers and settings therefor available.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Parameter estimation by optimization case study: Gamma likelihood</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/gamma_optimization.html#computing-environment",
    "href": "lessons/optimization/gamma_optimization.html#computing-environment",
    "title": "37  Parameter estimation by optimization case study: Gamma likelihood",
    "section": "37.5 Computing environment",
    "text": "37.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,cmdstanpy,arviz,statsmodels,iqplot,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy      : 2.1.3\npolars     : 1.30.0\nscipy      : 1.15.3\ncmdstanpy  : 1.2.5\narviz      : 0.21.0\nstatsmodels: 0.14.4\niqplot     : 0.3.7\nbokeh      : 3.6.2\nbebi103    : 0.1.27\njupyterlab : 4.3.7",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Parameter estimation by optimization case study: Gamma likelihood</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/mm_algorithms.html",
    "href": "lessons/optimization/mm_algorithms.html",
    "title": "38  Minorize-maximize algorithms",
    "section": "",
    "text": "38.1 Description of the algorithm\nWe have been taking the (suboptimal, but in some cases necessary) approach of making sense of posterior distributions by finding the parameter values for which the posterior probability density function is maximal, the so-called MAP. We have been employing various optimization methods to find the MAP, and different methods can be effective in different contexts. Specifically for the context of inferring latent variables, the expectation-maximization algorithm, or EM algorithm is particularly powerful.\nThe EM algorithm is a special case of a minorize-maximize algorithm, or MM algorithm, also know as a bound optimization algorithm. Here, we introduce the main ideas behind MM algorithms with an eye toward applications of EM algorithms.\nConsider an optimization problem in which we seek parameters \\(\\theta\\) that maximize some objective function \\(h(\\theta)\\). This is the same general optimization problem we have been considering thus far.\nThe basic idea behind an MM algorithm is to define a surrogate function, \\(Q(\\theta, \\phi)\\), that is a tight lower bound of \\(h(\\theta)\\), or minorizes \\(h(\\theta)\\). That is to say, \\(Q(\\theta, \\phi)\\) is everywhere less than \\(h(\\theta)\\), except at \\(Q(\\phi, \\phi)\\), which is equal to \\(h(\\phi)\\). Once we have defined the surrogate function \\(Q(\\theta, \\phi)\\), we find the value of \\(\\theta\\) that maximizes \\(Q(\\theta, \\phi)\\). Because \\(Q(\\theta, \\phi)\\) minorizes \\(h(\\theta)\\), if we evaluate \\(h(\\theta)\\) at the \\(\\theta\\) that maximized \\(Q(\\theta, \\phi)\\), the result will be above \\(Q(\\theta, \\phi)\\). The only time that \\(h(\\theta)\\) will not be above \\(Q(\\theta, \\phi)\\) is when the maximum of \\(Q(\\theta, \\phi)\\) is also the maximum of \\(h(\\theta)\\), which means we have found the maximizer of \\(h(\\theta)\\).\nSo, we start with some guess for which \\(\\theta\\) maximizes \\(h(\\theta)\\); call it \\(\\phi\\). We then construct the surrogate function \\(Q(\\theta, \\phi)\\). Next, we find the value of \\(\\theta\\) that maximizes the surrogate function; call that \\(\\theta^*\\).\n\\[\\begin{align}\n\\theta^* = \\underset{\\theta}{\\text{arg max}} \\,Q(\\theta, \\phi).\n\\end{align} \\tag{38.1}\\]\nGiven that \\(Q(\\theta,\\phi)\\) minorizes \\(h(\\theta)\\), we have\n\\[\\begin{align}\nh(\\theta^*) \\ge Q(\\theta^*,\\phi) \\ge Q(\\phi, \\phi) = h(\\phi).\n\\end{align} \\tag{38.2}\\]\nThis means that by updating \\(\\phi\\) to \\(\\theta^*\\), we have moved uphill on \\(h(\\theta)\\). We continue in this way until we stop climbing uphill. An MM algorithm follows thusly.\nThe alternating minorization of a function and then maximization of the minorized function is where the MM algorithm gets its name.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Minorize-maximize algorithms</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/mm_algorithms.html#description-of-the-algorithm",
    "href": "lessons/optimization/mm_algorithms.html#description-of-the-algorithm",
    "title": "38  Minorize-maximize algorithms",
    "section": "",
    "text": "Guess a maximizer of \\(h(\\theta)\\) and define it as \\(\\phi\\).\nDefine \\(Q(\\theta, \\phi)\\) which minorizes \\(h(\\theta)\\).\nCompute \\(\\theta^* = \\text{arg max}_\\theta, Q(\\theta, \\phi)\\).\nIf the difference between \\(h(\\theta^*)\\) and \\(h(\\phi)\\) is smaller than a predefined threshold, STOP and record \\(\\theta^*\\) as the maximizer of \\(h(\\theta)\\). Otherwise, go to 5.\nSet \\(\\phi = \\theta^*\\).\nGo to 2.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Minorize-maximize algorithms</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/mm_algorithms.html#this-is-clever-but-what-is-the-rub",
    "href": "lessons/optimization/mm_algorithms.html#this-is-clever-but-what-is-the-rub",
    "title": "38  Minorize-maximize algorithms",
    "section": "38.2 This is clever, but what is the rub?",
    "text": "38.2 This is clever, but what is the rub?\nThe MM algorithm is appealing. We sequentially bound the objective function from below with a surrogate function, maximize the surrogate function, and then move the surrogate function up, and repeat. But there are two challenges.\nIn the maximization step, we still need to do a maximization! Why not just maximize the objective function in the first place? The idea is that we can pick a surrogate function that is much easier to maximize, maybe even a surrogate function that we can maximize analytically. It is much easier to solve many easy maximization problems than one hard one.\nThe other issue is coming up with the surrogate function. We have already alluded to the fact that it is not worth using an MM algorithm if the surrogate function is no easier to optimize than the objective function. So, we have to pick a surrogate function the is easily optimizable. Beyond that, we need to make sure it is indeed a minorizer; the surrogate function must be a tight lower bound of the objective function!\nSo, ultimately, effective use of an MM algorithm comes down to effectively choosing easily optimizable minorizers of the objective function. For a general optimization, this is nontrivial. In the next lesson, we will show that for a specific class of posterior distributions, we can conveniently pick a surrogate function that allows for effective MM approaches.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Minorize-maximize algorithms</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_algorithm.html",
    "href": "lessons/optimization/em_algorithm.html",
    "title": "39  The expectation-maximization (EM) algorithm",
    "section": "",
    "text": "39.1 The optimization problem solved by EM\nConsider the case where I have a generative model where I observe data \\(y\\), but I have unobserved data \\(z\\). Unobserved data, often referred to as latent variables are parameters in the Bayesian sense, but we will treat them differently than the rest of the parameters, \\(\\theta\\), in our generative model. So, if our goal is to get a MAP estimate for \\(\\theta\\), we will need to marginalize away the latent variables \\(z\\).\nFor this case, the joint distribution is \\(\\pi(y, z, \\theta)\\), and Bayes’s theorem reads\n\\[\\begin{align}\ng(z, \\theta \\mid y) = \\frac{\\pi(y, z, \\theta)}{g(y)} = \\frac{\\pi(y, z\\mid \\theta)\\,g(\\theta)}{g(y)} =  \\frac{f(y\\mid z, \\theta)\\,g(z\\mid \\theta)\\,g(\\theta)}{g(y)}\n\\end{align}\n\\tag{39.1}\\]\nThe marginal posterior of interest is\n\\[\\begin{align}\ng(\\theta \\mid y) &= \\frac{1}{g(y)}\\,\\sum_z \\pi(y, z, \\theta) \\nonumber \\\\[1em]\n&=  \\frac{1}{g(y)}\\,\\sum_z  \\frac{\\pi(y, z\\mid \\theta)\\,g(\\theta)}{g(y)} \\nonumber \\\\[1em]\n&= \\frac{f(y\\mid \\theta)\\,g(\\theta)}{g(y)},\n\\end{align}\n\\tag{39.2}\\]\nwhere, as usual, the sum over \\(z\\) is replaced with an integral for continuous \\(z\\).\nSo, if we are trying to find the MAP, we wish to maximize \\(g(\\theta\\mid y)\\), or equivalently to maximize \\(\\ln g(\\theta \\mid y)\\). As usual, the denominator in Bayes theorem does not have any \\(z\\) or \\(\\theta\\) dependence, so finding the MAP is equivalent to finding the value of \\(\\theta\\) that maximizes\n\\[\\begin{align}\n\\pi(y, \\theta) = \\sum_z \\pi(y, z, \\theta) = f(y\\mid \\theta)\\,g(\\theta).\n\\end{align}\n\\tag{39.3}\\]\nOur goal, then is to compute\n\\[\\begin{align}\n\\underset{\\theta}{\\text{arg max}} h(\\theta),\n\\end{align}\n\\tag{39.4}\\]\nwhere the objective function is \\[\\begin{align}\nh(\\theta) = \\ln \\pi(y,\\theta)\n\\end{align}\n\\tag{39.5}\\]\nNote that defining the objective function requires evaluation of the sum over \\(z\\) to marginalize the joint distribution. This sum is in general very difficult to evaluate. However, we will see that we can get away without ever evaluating it by using the EM algorithm! This is where its real magic comes in.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>The expectation-maximization (EM) algorithm</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_algorithm.html#the-e-step-finding-the-minorizing-surrogate-function",
    "href": "lessons/optimization/em_algorithm.html#the-e-step-finding-the-minorizing-surrogate-function",
    "title": "39  The expectation-maximization (EM) algorithm",
    "section": "39.2 The E-step: Finding the minorizing surrogate function",
    "text": "39.2 The E-step: Finding the minorizing surrogate function\nWe will take a somewhat convoluted path to constructing an appropriate surrogate function \\(Q(\\theta, \\phi)\\), but upon seeing the end result, it will make sense. We start by using conditional probability to write\n\\[\\begin{align}\n\\pi(y, z, \\theta) = \\pi(y, z\\mid \\theta)\\,g(\\theta) = g(z\\mid y, \\theta)\\,f(y\\mid \\theta)\\,g(\\theta) = g(z\\mid y, \\theta)\\,\\pi(y, \\theta),\n\\end{align}\n\\]\nwhich we can rearrange to give\n\\[\\begin{align}\n\\pi(y, \\theta) = \\frac{\\pi(y, z\\mid \\theta)\\,g(\\theta)}{g(z\\mid y, \\theta)}.\n\\end{align}\n\\]\nTherefore, our objective function can be written as\n\\[\\begin{align}\nh(\\theta) = \\ln \\pi(y, \\theta) = \\ln \\left[\\frac{\\pi(y, z \\mid \\theta)\\,g(\\theta)}{g(z\\mid y, \\theta)}\\right] = \\ln \\pi(y, z \\mid \\theta) + \\ln g(\\theta) - \\ln g(z \\mid y, \\theta).\n\\end{align}\n\\]\nWe now define a probability mass/density function \\(q(z\\mid \\phi)\\), which for now is arbitrary. We multiply both sides of the above equation by \\(q(z\\mid \\phi)\\) to get\n\\[\\begin{align}\nq(z\\mid \\phi)\\,h(\\theta) = q(z\\mid \\phi)\\,\\ln \\left[\\pi(y, z \\mid \\theta)\\,g(\\theta)\\right] - q(z\\mid \\phi)\\,\\ln g(z \\mid \\theta, y).\n\\end{align}\n\\]\nWe next use one of the best tricks in mathematics; we add zero to the right hand side. In this case, the “zero” we add is \\(q(z\\mid \\phi) \\ln q(z\\mid \\phi) - q(z\\mid \\phi) \\ln q(z\\mid \\phi)\\).\n\\[\\begin{align}\nq(z\\mid \\phi)\\,h(\\theta) &= q(z\\mid \\phi)\\,\\ln \\left[\\pi(y, z \\mid \\theta)\\,g(\\theta)\\right] - q(z\\mid \\phi)\\,\\ln g(z, \\theta \\mid y) + q(z\\mid \\phi) \\ln q(z\\mid \\phi) - q(z\\mid \\phi) \\ln q(z\\mid \\phi) \\nonumber \\\\[1em]\n&= q(z\\mid \\phi)\\,\\ln \\left[\\frac{\\pi(y, z \\mid \\theta)\\,g(\\theta)}{q(z\\mid \\phi)}\\right] - q(z\\mid \\phi)\\,\\ln\\left[\\frac{g(z \\mid y, \\theta)}{q(z\\mid \\phi)}\\right].\n\\end{align}\n\\]\nWe now sum both sides of the equation over \\(z\\). The left hand side simply evaluates to \\(h(\\theta)\\), since \\(q(z\\mid \\phi)\\) is normalized. We thus obtain\n\\[\\begin{align}\nh(\\theta) =  \\sum_z q(z\\mid \\phi)\\,\\ln \\left[\\frac{\\pi(y, z\\mid \\theta)\\,g(\\theta)}{q(z\\mid \\phi)}\\right] - \\sum_z q(z\\mid \\phi)\\,\\ln\\left[\\frac{g(z \\mid y, \\theta)}{q(z\\mid \\phi)}\\right]\n\\end{align}\n\\]\nWe recognize the second term as the negative Kullback-Leibler divergence between the \\(q(z\\mid \\phi)\\) and \\(g(z \\mid y, \\theta)\\);\n\\[\\begin{align}\nD_\\mathrm{KL}(q(z\\mid \\phi) \\, \\| \\, g(z \\mid y, \\theta)) \\sum_z q(z\\mid \\phi)\\,\\ln\\left[\\frac{q(z\\mid \\phi)}{g(z \\mid y, \\theta)}\\right],\n\\end{align}\n\\]\nso that\n\\[\\begin{align}\nh(\\theta) =  \\sum_z q(z\\mid \\phi)\\,\\ln \\left[\\frac{\\pi(y, z\\mid \\theta)\\,g(\\theta)}{q(z\\mid \\phi)}\\right] + D_\\mathrm{KL}(q(z\\mid \\phi)\\,\\|\\, g(z \\mid y, \\theta)).\n\\end{align}\n\\]\nThe Gibbs inequality tells us that the Kullback-Leibler divergence is positive, except when \\(q(z\\mid \\phi) = g(z, \\theta \\mid y)\\), where the Kullback-Leibler divergence is zero. Therefore, the first term in the above equation is indeed a tight lower bound (a minorizer) of \\(h(\\theta)\\) if \\(q(z\\mid \\phi) = g(z \\mid y, \\phi)\\). The minorizer of \\(h(\\theta)\\) is then\nSo our surrogate function that minorizes \\(h(\\theta)\\) is\n\\[\\begin{align}\nQ(\\theta, \\phi) &= \\sum_z g(z \\mid y, \\phi)\\,\\ln \\left[\\frac{\\pi(y, z \\mid \\theta)\\,g(\\theta)}{g(z \\mid y, \\phi)}\\right].\n\\end{align}\n\\]\nBecause the surrogate function is an expectation of \\(\\ln \\left[\\pi(y, z \\mid \\theta)\\,g(\\theta)/g(z \\mid y, \\phi)\\right]\\) over the distribution \\(g(z \\mid y, \\phi)\\), this minorizing step is referred to as the “expectation step,” or the “E step.”",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>The expectation-maximization (EM) algorithm</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_algorithm.html#the-m-step-finding-the-theta-that-maximizes-the-surrogate-function",
    "href": "lessons/optimization/em_algorithm.html#the-m-step-finding-the-theta-that-maximizes-the-surrogate-function",
    "title": "39  The expectation-maximization (EM) algorithm",
    "section": "39.3 The M-step: Finding the \\(\\theta\\) that maximizes the surrogate function",
    "text": "39.3 The M-step: Finding the \\(\\theta\\) that maximizes the surrogate function\nNow that we have defined the surrogate function that minorizes the marginal log posterior, we need to find the value of \\(\\theta\\) that maximizes \\(Q(\\theta, \\phi)\\). We note that we can write \\(Q(\\theta, \\pi)\\) as\n\\[\\begin{align}\nQ(\\theta, \\phi) = \\sum_z g(z \\mid y, \\phi)\\,\\ln \\left[\\pi(y, z \\mid \\theta)\\,g(\\theta)\\right] - \\sum_z g(z \\mid y, \\phi)\\,\\ln g(z \\mid y, \\phi).\n\\end{align}\n\\]\nThe last term is the entropy of the \\(g(z\\mid y, \\phi)\\) distribution, and is \\(\\theta\\)-independent. So, the maximization problem involves finding the value of \\(\\theta\\) that maximizes the quantity\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) = \\sum_z g(z \\mid y, \\phi)\\,\\ln \\left[\\pi(y, z \\mid \\theta)\\,g(\\theta)\\right],\n\\end{align}\n\\]\nwhere \\(\\mathcal{Q} = Q(\\theta,\\phi) - H[g(z \\mid y, \\phi)]\\). We can further simplify \\(\\mathcal{Q}(\\theta, \\phi)\\) by evaluating the logarithm to give\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) = \\ln g(\\theta) + \\sum_z g(z \\mid y, \\phi)\\,\\ln \\pi(y, z \\mid \\theta).\n\\end{align}\n\\tag{39.6}\\]",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>The expectation-maximization (EM) algorithm</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_algorithm.html#sec-em-summary",
    "href": "lessons/optimization/em_algorithm.html#sec-em-summary",
    "title": "39  The expectation-maximization (EM) algorithm",
    "section": "39.4 Summary of the EM algorithm",
    "text": "39.4 Summary of the EM algorithm\nWe seek to find the value of the parameter \\(\\theta\\) that maximizes the marginal log posterior,\n\\[\\begin{align}\ng(\\theta \\mid y) = \\frac{1}{f(y)}\\sum_z \\pi(y, z \\mid \\theta) g(\\theta),\n\\end{align}\n\\]\nwhere \\(\\pi(y,z\\mid \\theta) = f(y\\mid z, \\theta)\\,g(z\\mid \\theta)\\) and \\(g(\\theta)\\) are known. This is equivalent to maximizing\n\\[\\begin{align}\nh(\\theta) = \\ln \\pi(y, \\theta) = \\ln[f(y\\mid\\theta)\\,g(\\theta)].\n\\end{align}\n\\]\nThe EM algorithm goes as follows.\n\nGuess a maximizer of the marginal log posterior and define it as \\(\\phi\\).\nEvaluate \\(g(z\\mid y, \\theta)\\). (E step)\nUse the result of step (2) to define \\(\\mathcal{Q}(\\theta, \\phi) = \\sum_z g(z \\mid y, \\phi)\\,\\ln \\left[\\pi(y, z \\mid \\theta)\\,g(\\theta)\\right]\\).\nCompute \\(\\theta^* = \\text{arg max}_\\theta, \\mathcal{Q}(\\theta, \\phi)\\). (M step)\nIf the difference between \\(h(\\theta^*)\\) and \\(h(\\phi)\\) is smaller than a predefined threshold, STOP and record \\(\\theta^*\\) as the maximizer of \\(h(\\theta)\\). Otherwise, go to 6.\nSet \\(\\phi = \\theta^*\\).\nGo to 2.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>The expectation-maximization (EM) algorithm</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm.html",
    "href": "lessons/optimization/em_gmm.html",
    "title": "40  EM applied to a Gaussian mixture model",
    "section": "",
    "text": "40.1 Gaussian mixture models (GMMs)\nIn preparing this section, I benefitted from discussions with and reading a notebook by Kayla Jackson.\nA Gaussian mixture model, or GMM, is a model in which each of \\(N\\) i.i.d. data points may be drawn from one of \\(K\\) \\(n\\)-variate Normal (a.k.a. Gaussian) distributions. We index the data points with \\(i\\) and the Normal distributions with \\(k\\). The probably that an arbitrary data point in the data set is drawn from Normal distribution \\(k\\) is \\(\\pi_k\\), a quantity referred to as a mixing coefficient. We denote the set of \\(K\\) mixing coefficients as \\(\\boldsymbol{\\pi}\\) and \\(\\sum_k \\pi_k = 1\\). We define \\(z_{ik}\\) to be one if data point \\(i\\) derives from a Normal distribution with parameters \\((\\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\) and zero otherwise. The prior probability of \\(z_{ik}\\) being one is \\(\\pi_k\\), since we do not know the value of datum \\(i\\) a priori. Thus, \\(\\boldsymbol{z}_i\\) represents a \\(K\\)-vector for data point \\(i\\) in which \\(K-1\\) entries are zero and one entry is one.\nFor notational convenience, when referring to the set of all parameters, we will drop the subscripts. That is, \\(\\mathbf{z}\\) denotes the set of all \\(\\mathbf{z}_i\\)’s, \\(\\boldsymbol{\\mu}\\) denotes the set of all \\(\\boldsymbol{\\mu}_k\\)’s, and \\(\\mathsf{\\Sigma}\\) denotes the set of all \\(\\mathsf{\\Sigma}_k\\)’s. In general, a Gaussian mixture model where we assume the parameters have independent priors is\n\\[\\begin{align}\n&\\boldsymbol{\\pi} \\sim \\text{prior for mixing coefficients}, \\\\[1em]\n&\\boldsymbol{\\mu} \\sim \\text{prior for location parameters}, \\\\[1em]\n&\\mathsf{\\Sigma} \\sim \\text{prior for scale parameters}, \\\\[1em]\n&\\mathbf{z}_i \\mid \\boldsymbol{\\pi} \\sim \\text{Categorical}(\\boldsymbol{\\pi}) \\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\sim \\prod_{k=1}^K (\\mathrm{Norm}(\\boldsymbol{\\mu}_k,\\mathsf{\\Sigma}_k))^{z_{ik}} \\;\\forall i.\n\\end{align}\n\\tag{40.1}\\]\nTo make all conditioning clear, we can explicitly write out Bayes’s theorem.\n\\[\\begin{align}\ng(\\mathbf{z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\mid \\mathbf{y}) = \\frac{1}{f(\\mathbf{y})}\\, \\left(\\prod_{i=1}^N f(\\mathbf{y}_i\\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\mathsf{\\Sigma})\\right)\\left(\\prod_{i=1}^N g(\\mathbf{z}_i \\mid \\boldsymbol{\\pi})\\right)\\,g(\\boldsymbol{\\pi})\\,g(\\boldsymbol{\\mu})\\,g(\\mathsf{\\Sigma}).\n\\end{align}\n\\]\nFor further clarity, we write out the Categorical prior for the mixing coefficients,\n\\[\\begin{align}\ng(\\mathbf{z}_i \\mid \\boldsymbol{\\pi}) = \\prod_{k=1}^K \\pi_k^{z_{ik}},\n\\end{align}\n\\]\nand the likelihood,\n\\[\\begin{align}\nf(\\mathbf{y} \\mid \\mathbf{z}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma}) &= \\prod_{i=1}^N f(\\mathbf{y}_i \\mid \\mathbf{z}_i,  \\boldsymbol{\\mu}, \\mathsf{\\Sigma}) \\nonumber \\\\[1em]\n&= \\prod_{i=1}^N\\underbrace{\\prod_{k=1}^K \\left(\\frac{1}{\\sqrt{(2\\pi)^N\\det\\mathsf{\\Sigma}_k}}\\,\\exp\\left[-\\frac{1}{2}(\\mathbf{y}_i-\\boldsymbol{\\mu}_k)^\\mathsf{T}\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot(\\mathbf{y}_i-\\boldsymbol{\\mu}_k)\\right]\\right)^{z_{ik}}}_{f(\\mathbf{y}_i \\mid \\mathbf{z}_i,  \\boldsymbol{\\mu}, \\mathsf{\\Sigma})},\n\\end{align}\n\\]\nwhere the fact that \\(z_{ik}\\) is one for only one value of \\(k\\) means that a single Normal distribution is assigned to datum \\(i\\) for a given value of \\(\\mathbf{z}_i\\). To prevent the equations for becoming unwieldy, we define, as is commonly done,\n\\[\\begin{align}\n\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k) \\equiv \\frac{1}{\\sqrt{(2\\pi)^N\\det\\mathsf{\\Sigma}_k}}\\,\\exp\\left[-\\frac{1}{2}(\\mathbf{y}_i-\\boldsymbol{\\mu}_k)^\\mathsf{T}\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot(\\mathbf{y}_i-\\boldsymbol{\\mu}_k)\\right],\n\\end{align}\n\\tag{40.2}\\]\nsuch that the likelihood may more compactly be written as\n\\[\\begin{align}\nf(\\mathbf{y} \\mid \\mathbf{z}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma}) = \\prod_{i=1}^N \\prod_{k=1}^K \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)^{z_{ik}}.\n\\end{align}\n\\]",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>EM applied to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm.html#gaussian-mixture-models-gmms",
    "href": "lessons/optimization/em_gmm.html#gaussian-mixture-models-gmms",
    "title": "40  EM applied to a Gaussian mixture model",
    "section": "",
    "text": "40.1.1 The marginalized GMM posterior\nFor many applications, such as for sampling with Markov chain Monte Carlo, it is useful to marginalize over the \\(z_{ik}\\)’s. This may be done analytically.\n\\[\\begin{align}\ng(\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\mid \\mathbf{y}) &= \\sum_\\mathbf{z} g(\\mathbf{z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\mid \\mathbf{y})\n=\\frac{g(\\boldsymbol{\\pi})\\,g(\\boldsymbol{\\mu})\\,g(\\mathsf{\\Sigma})}{f(\\mathbf{y})}\\, \\sum_\\mathbf{z}\\left(\\prod_{i=1}^N f(\\mathbf{y}_i\\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\mathsf{\\Sigma})\\right)\\left(\\prod_{i=1}^N \\prod_{k=1}^K \\pi_k^{z_{ik}}\\right),\n\\end{align}\n\\]\nSo, the sum we need to evaluate is\n\\[\\begin{align}\n\\sum_\\mathbf{z}\\left(\\prod_{i=1}^N f(\\mathbf{y}_i\\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\mathsf{\\Sigma})\\right)\\left(\\prod_{i=1}^N \\prod_{k=1}^K \\pi_k^{z_{ik}}\\right) &= \\sum_\\mathbf{z} \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{z_{ik}} \\nonumber \\\\[1em]\n&= \\sum_{j=1}^K \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{\\delta_{jk}} \\nonumber \\\\\n&= \\sum_{j=1}^K\\prod_{i=1}^N \\pi_j\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_j, \\mathsf{\\Sigma}_j),\n\\end{align}\n\\]\nwhere \\(\\delta_{jk}\\) is the Kronecker delta function, equal to one for \\(j = k\\) and zero otherwise. The above result means that we can write the likelihood for the marginalized posterior as\n\\[\\begin{align}\n\\mathbf{y}_i \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\sim \\sum_{k=1}^K \\pi_k\\, \\text{Norm}(\\boldsymbol{\\mu}_k,\\mathsf{\\Sigma}_k)\\;\\forall i,\n\\end{align}\n\\]\nand thereby write the model with the \\(z\\)-variables marginalized out as\n\\[\\begin{align}\n&\\boldsymbol{\\pi} \\sim \\text{prior for mixing coefficients}, \\\\[1em]\n&\\boldsymbol{\\mu} \\sim \\text{prior for location parameters}, \\\\[1em]\n&\\mathsf{\\Sigma} \\sim \\text{prior for scale parameters}, \\\\[1em]\n&\\mathbf{y}_i \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\sim \\sum_{k=1}^K \\pi_k\\, \\text{Norm}(\\boldsymbol{\\mu}_k,\\mathsf{\\Sigma}_k) \\;\\forall i.\n\\end{align}\n\\]\nThus, the log posterior is\n\\[\\begin{align}\n\\ln g(\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\mid \\mathbf{y}) = \\text{constant} + \\ln g(\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma}) + \\sum_{i=1}^N\\ln\\left[\\sum_{k=1}^K \\pi_k\\, \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k,\\mathsf{\\Sigma}_k)\\right].\n\\end{align}\n\\tag{40.3}\\]",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>EM applied to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm.html#sec-em-algorithm-for-gmm",
    "href": "lessons/optimization/em_gmm.html#sec-em-algorithm-for-gmm",
    "title": "40  EM applied to a Gaussian mixture model",
    "section": "40.2 The EM algorithm for a GMM",
    "text": "40.2 The EM algorithm for a GMM\nNow that we have a firm understanding of what a GMM is, we can proceed to develop an EM algorithm for GMMs.\n\n40.2.1 The surrogate function for a GMM\nRecall that we need to specify the parameter-dependent part of the surrogate function, which for an EM algorithm is given by Equation 39.6,\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) = \\ln g(\\theta) + \\sum_z g(z \\mid y, \\phi)\\,\\ln \\pi(y, z \\mid \\theta).\n\\end{align}\n\\]\nFor a Gaussian mixture model, \\(z\\) in the above equation is \\(\\mathbf{z}\\), \\(y\\) is the set of all the data, and \\(\\phi\\) and \\(\\theta\\) are sets of parameters that we will respectively call \\((\\boldsymbol{\\pi}, \\boldsymbol{\\mu},\\mathsf{\\Sigma})\\) and \\((\\boldsymbol{\\pi}^\\theta, \\boldsymbol{\\mu}^\\theta, \\mathsf{\\Sigma}^\\theta)\\) parameters. Since we will eventually employ EM, it will be useful to get expressions for \\(g(z \\mid y, \\phi)\\) and \\(\\pi(y, z \\mid \\theta)\\). We start with the latter.\n\\[\\begin{align}\n\\pi(y, z \\mid \\theta) &= f(y \\mid z, \\theta)\\,g(z \\mid \\theta) = \\left(\\prod_{i=1}^N f(\\mathbf{y}_i\\mid \\mathbf{z}_i, \\boldsymbol{\\mu}^\\theta, \\mathsf{\\Sigma}^\\theta)\\right)\\left(\\prod_{i=1}^N \\prod_{k=1}^K (\\pi_k^\\theta)^{z_{ik}}\\right) \\nonumber \\\\[1em]\n&= \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k^\\theta\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k^\\theta, \\mathsf{\\Sigma}_k^\\theta)\\right]^{z_{ik}}.\n\\end{align}\n\\]\nTo compute \\(g(z \\mid y, \\phi)\\), we use Bayes’s theorem.\n\\[\\begin{align}\ng(z \\mid y, \\phi) &= \\frac{f(y\\mid z, \\phi)\\,g(z\\mid\\phi)}{f(y\\mid\\phi)}\n\\propto f(y\\mid z, \\phi)\\,g(z\\mid\\phi)\n= \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{z_{ik}}.\n\\end{align}\n\\]\nEvaluating the normalization constant for \\(g(z\\mid y, \\phi)\\), we get\n\\[\\begin{align}\ng(z \\mid y, \\phi) &= \\frac{\\displaystyle{\\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{z_{ik}}}}{\\displaystyle{\\sum_\\mathbf{z} \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{z_{ik}}}}.\n\\end{align}\n\\tag{40.4}\\]\nWe can plug these expressions in and write the parameter-dependent part of the surrogate function as\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) &= \\ln g(\\theta) + \\sum_\\mathbf{z} g(z\\mid y, \\phi)\\,\\sum_{i=1}^N\\sum_{k=1}^k z_{ik}\\left(\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right) \\nonumber \\\\[1em]\n&= \\ln g(\\theta) + \\sum_{i=1}^N\\sum_{k=1}^k\\left[\\sum_{z_{ik}} g(z\\mid y, \\phi)\\,z_{ik}\\right] \\left(\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right).\n\\end{align}\n\\]\nWe recognize the bracketed sum as the expectation of \\(z_{ik}\\) over the posterior distribution. That is, the bracketed term is probability that \\(z_{ik}\\) is unity for mixture component \\(k\\). This quantity is called the responsibility of component \\(k\\) generating datum \\(i\\), and is typically denoted as \\(\\gamma(z_{ik})\\). We can directly evaluate it.\n\\[\\begin{align}\n\\gamma(z_{ik}) &= \\sum_{z_{ik}} g(z\\mid y, \\phi)\\,z_{ik} \\nonumber \\\\[1em]\n&= \\sum_{z_{ik}} z_{ik}\\,\\frac{\\displaystyle{\\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{z_{ik}}}}{\\displaystyle{\\sum_\\mathbf{z} \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)\\right]^{z_{ik}}}} \\nonumber \\\\[1em]\n&= \\displaystyle{\\frac{\\pi_k \\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)}{\\sum_{j=1}^K\\pi_j\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_j, \\mathsf{\\Sigma}_j)}},\n\\end{align}\n\\tag{40.5}\\]\nwhere we have used the fact that \\(z_{ik}\\) takes a value of one for exactly one \\(k\\) and a value of zero otherwise.\nThe responsibilities are of use even if we do not directly use them directly in the computational tasks of inference (we will in the EM algorithm, but not, for example, when we do MCMC). The have the direct interpretation as the posterior probability that a given datum comes from a given component of the mixture model. Equation 40.5 gives a formula for computing the responsibilities from estimates (or samples of) the model parameters.\nProceeding with the EM specification, now that we have the responsibilities, we can write the parameter-dependent part of the surrogate function as\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) = \\ln g(\\theta) + \\sum_{i=1}^N\\sum_{k=1}^k\\gamma(z_{ik}) \\left(\\ln \\pi_k^\\theta + \\ln \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k^\\theta, \\mathsf{\\Sigma}_k^\\theta)\\right),\n\\end{align}\n\\tag{40.6}\\]\nwhere \\(g(\\theta) = g(\\boldsymbol{\\pi}^\\theta)\\,g(\\boldsymbol{\\mu}^\\theta)\\,g(\\mathsf{\\Sigma}^\\theta)\\).\n\n\n40.2.2 Summary of the EM algorithm for a GMM\nWe can write the EM algorithm for this specific case (see Section 39.4) as follows.\n\nGuess a maximizer of the marginal log posterior. Define this initial guess as \\((\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma})\\).\nEvaluate \\(g(\\mathbf{z}\\mid \\mathbf{y}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma})\\) using Equation 40.4. (E step)\nUse the result of step (2) to define \\(\\mathcal{Q}((\\boldsymbol{\\pi}^\\theta, \\boldsymbol{\\mu}^\\theta, \\mathsf{\\Sigma}^\\theta), (\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma}))\\). This involves computing the responsibilities \\(\\gamma(z_{ik})\\) using the result from the E step according to Equation 40.5 and then using Equation 40.6.\nCompute \\((\\boldsymbol{\\pi}^*, \\boldsymbol{\\mu}^*, \\mathsf{\\Sigma}^*) = \\text{arg max}_{(\\boldsymbol{\\pi}^\\theta, \\boldsymbol{\\mu}^\\theta, \\mathsf{\\Sigma}^\\theta)}\\, \\mathcal{Q}((\\boldsymbol{\\pi}^\\theta, \\boldsymbol{\\mu}^\\theta, \\mathsf{\\Sigma}^\\theta), (\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma}))\\). (M step)\nIf the difference between the log posteriors (given by Equation 40.3) evaluated at \\((\\boldsymbol{\\pi}^*, \\boldsymbol{\\mu}^*, \\mathsf{\\Sigma}^*)\\) and \\((\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma})\\) is smaller than a predefined threshold, STOP and record \\((\\boldsymbol{\\pi}^*, \\boldsymbol{\\mu}^*, \\mathsf{\\Sigma}^*)\\) as the MAP estimate for the parameters. Otherwise, go to 6.\nSet \\((\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\mathsf{\\Sigma}) = (\\boldsymbol{\\pi}^*, \\boldsymbol{\\mu}^*, \\mathsf{\\Sigma}^*)\\).\nGo to 2.\n\nThe intermediate step to compute the parameter-dependent part of the surrogate function, namely computing the responsibilities, is traditionally included in the E step.\nWhile the E step is achieved analytically, the M step in general cannot be. However, for some specific priors, the M step may be solved analytically. The most widely used example is when all priors are uniform, and we describe that case below.\n\n\n40.2.3 MAP estimation with uniform priors using EM\nIf we use uniform priors, \\(g(\\theta) = g(\\boldsymbol{\\pi}^\\theta)\\,g(\\boldsymbol{\\mu}^\\theta)\\,g(\\mathsf{\\Sigma}^\\theta) = \\text{constant}\\), we can perform the M step analytically. First, differentiating the surrogate function with respect to \\(\\boldsymbol{\\mu}_k^\\theta\\), we have at the MAP,\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k}\n&= \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k}\\,\\sum_{i=1}^N \\gamma(z_{ik})\\,(\\mathbf{y}_i - \\boldsymbol{\\mu}_k)^\\mathsf{T}\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot (\\mathbf{y}_i - \\boldsymbol{\\mu}_k) \\nonumber \\\\[1em]\n&= \\sum_{i=1}^N \\gamma(z_{ik})\\,\\mathsf{\\Sigma}_k^{-1}\\cdot (\\mathbf{y}_i - \\boldsymbol{\\mu}_k).\n\\end{align}\n\\]\nIn the above, we have dropped the \\(\\theta\\) superscripts for notational convenience. Setting the derivative to zero and multiplying by \\(\\mathsf{\\Sigma}_k\\) gives\n\\[\\begin{align}\n\\sum_{i=1}^N \\gamma(z_{ik})\\,(\\mathbf{y}_i - \\boldsymbol{\\mu}_k) = \\mathbf{0}.\n\\end{align}\n\\]\nSolving gives the optimal \\(\\boldsymbol{\\mu}_k\\), perhaps unsurprisingly, as the responsibility-weighted average of the data,\n\\[\\begin{align}\n\\boldsymbol{\\mu}^*_k = \\frac{1}{\\gamma_k}\\sum_{i=1}^N\\gamma(z_{ik})\\,\\mathbf{y}_i,\n\\end{align}\n\\]\nwhere\n\\[\\begin{align}\n\\gamma_k \\equiv \\sum_{i=1}^N\\gamma(z_{ik}).\n\\end{align}\n\\]\nSimilarly, we can differentiate the surrogate function with respect to \\(\\mathsf{\\Sigma}_k^\\theta\\), where we will again drop the \\(\\theta\\) superscripts for notational convenience. First, it helps to know a couple identities from matrix calculus,\n\\[\\begin{align}\n&\\frac{\\partial}{\\partial \\mathsf{A}}\\,\\mathbf{x}^\\mathsf{T}\\cdot \\mathsf{A}\\cdot\\mathbf{x} = \\mathbf{x}\\cdot\\mathbf{x}^\\mathsf{T},\\\\[1em]\n&\\frac{\\partial \\mathsf{A}^{-1}}{\\partial \\mathsf{A}} = -\\mathsf{A}^{-1}\\otimes\\mathsf{A}^{-1},\\\\[1em]\n&\\frac{\\partial}{\\partial \\mathsf{A}}\\,\\mathbf{x}^\\mathsf{T}\\cdot \\mathsf{A}\\cdot\\mathbf{x} = -\\mathsf{A}^{-1}\\cdot\\mathbf{x}\\cdot\\mathbf{x}^\\mathsf{T}\\cdot\\mathsf{A}^{-1},\\\\[1em]\n&\\frac{\\partial}{\\partial \\mathsf{A}}\\,\\ln \\det\\mathsf{A} = \\mathsf{A}^{-1},\n\\end{align}\n\\]\nwhere the \\(\\otimes\\) symbol denotes the outer product and \\(\\mathbf{x}\\cdot\\mathbf{x}^\\mathsf{T} = \\mathbf{x}\\otimes\\mathbf{x}\\). It follows from the first two identities and the chain rule that\n\\[\\begin{align}\n\\frac{\\partial}{\\partial \\mathsf{A}}\\,\\mathbf{x}^\\mathsf{T}\\cdot \\mathsf{A}^{-1}\\cdot\\mathbf{x}\n= \\frac{\\partial \\mathsf{A}^{-1}}{\\partial \\mathsf{A}}\\frac{\\partial}{\\partial \\mathsf{A}^{-1}}\\,\\mathbf{x}^\\mathsf{T}\\cdot \\mathsf{A}^{-1}\\cdot\\mathbf{x} = -\\mathsf{A}^{-1}\\cdot\\mathbf{x}\\cdot\\mathbf{x}^\\mathsf{T}\\cdot\\mathsf{A}^{-1}\n\\end{align}\n\\]\nNow, computing the derivative,\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{Q}}{\\partial \\mathsf{\\Sigma}_k}\n&= \\frac{\\partial}{\\partial \\mathsf{\\Sigma}_k}\\,\\sum_{i=1}^N \\gamma(z_{ik})\\,\\left(-\\frac{1}{2}\\,\\ln \\det \\mathsf{\\Sigma}_k - \\frac{1}{2} (\\mathbf{y}_i - \\boldsymbol{\\mu}_k)^\\mathsf{T}\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot (\\mathbf{y}_i - \\boldsymbol{\\mu}_k)\\right)\\nonumber \\\\[1em]\n&= -\\frac{1}{2}\\sum_{i=1}^N \\gamma(z_{ik})\\left(\\mathsf{\\Sigma}_k^{-1}  - \\mathsf{\\Sigma}_k^{-1}\\cdot(\\mathbf{y}_i - \\boldsymbol{\\mu}_k)\\cdot (\\mathbf{y}_i - \\boldsymbol{\\mu}_k)^\\mathsf{T}\\cdot\\mathsf{\\Sigma}_k^{-1}\\right).\n\\end{align}\n\\]\nSetting the derivative to zero and left multiplying and then right multiplying by \\(\\mathsf{\\Sigma}\\) gives\n\\[\\begin{align}\n\\mathsf{0} &= -\\frac{1}{2}\\sum_{i=1}^N \\gamma(z_{ik})\\left(\\mathsf{\\Sigma}_k\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot\\mathsf{\\Sigma}_k  - \\mathsf{\\Sigma}_k\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot(\\mathbf{y}_i - \\boldsymbol{\\mu}_k)\\cdot (\\mathbf{y}_i - \\boldsymbol{\\mu}_k)^\\mathsf{T}\\cdot\\mathsf{\\Sigma}_k^{-1}\\cdot\\mathsf{\\Sigma}_k\\right) \\nonumber \\\\\n&= -\\frac{1}{2}\\sum_{i=1}^N \\gamma(z_{ik})\\left(\\mathsf{\\Sigma}_k  - (\\mathbf{y}_i - \\boldsymbol{\\mu}_k)\\cdot (\\mathbf{y}_i - \\boldsymbol{\\mu}_k)^\\mathsf{T}\\right) ,\n\\end{align}\n\\]\nwhich is readily solved to give\n\\[\\begin{align}\n\\mathsf{\\Sigma}_k^* = \\frac{1}{\\gamma_k}\\sum_{i=1}^N \\gamma(z_{ik})\\,(\\mathbf{y}_i - \\boldsymbol{\\mu}_k^*)\\otimes(\\mathbf{y}_i - \\boldsymbol{\\mu}_k^*).\n\\end{align}\n\\]\nTo find the value of \\(\\boldsymbol{\\pi}^\\theta\\) that maximizes the surrogate function, we need to enforce the constraint that \\(\\sum_k\\pi_k^\\theta = 1\\), which we can do using a Lagrange multiplier.\n\\[\\begin{align}\n\\frac{\\partial}{\\partial \\boldsymbol{\\pi}_k} \\left(\\mathcal{Q} + \\lambda\\left(1 - \\sum_{k=1}^K \\pi_k\\right)\\right)\n&= \\sum_{i=1}^N\\frac{\\mathcal{N}(\\mathbf{y}_i\\mid\\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)}{\\sum_{j=1}^K\\pi_j\\,\\mathcal{N}(\\mathbf{y}_i\\mid\\boldsymbol{\\mu}_j, \\mathsf{\\Sigma}_j)} - \\lambda.\n\\end{align}\n\\]\nSetting this derivative to zero and multiplying both sides by \\(\\pi_k\\) gives\n\\[\\begin{align}\n0 &= \\sum_{i=1}^N\\frac{\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i\\mid\\boldsymbol{\\mu}_k, \\mathsf{\\Sigma}_k)}{\\sum_{j=1}^K\\pi_j\\,\\mathcal{N}(\\mathbf{y}_i\\mid\\boldsymbol{\\mu}_j, \\mathsf{\\Sigma}_j)} - \\lambda\\,\\pi_k\n= \\sum_{i=1}^N\\gamma(z_{ik}) - \\lambda\\,\\pi_k\n= \\gamma_k - \\lambda\\,\\pi_k.\n\\end{align}\n\\]\nSumming both sides over \\(k\\) and solving for \\(\\lambda\\) gives \\(\\lambda = N\\). Then, solving of \\(\\pi_k\\), we have\n\\[\\begin{align}\n\\pi_k^* = \\frac{\\gamma_k}{N}.\n\\end{align}\n\\]\nIn summary, then, the M step involves updating the values of the parameters as follows:\n\\[\\begin{align}\n&\\pi_k^* = \\frac{\\gamma_k}{N}, \\\\[1em]\n&\\boldsymbol{\\mu}^*_k = \\frac{1}{\\gamma_k}\\sum_{i=1}^N\\gamma(z_{ik})\\,\\mathbf{y}_i, \\\\[1em]\n&\\mathsf{\\Sigma}_k^* = \\frac{1}{\\gamma_k}\\sum_{i=1}^N \\gamma(z_{ik})\\,(\\mathbf{y}_i - \\boldsymbol{\\mu}_k^*)\\otimes(\\mathbf{y}_i - \\boldsymbol{\\mu}_k^*).\n\\end{align}\n\\]",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>EM applied to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm.html#gmm-priors",
    "href": "lessons/optimization/em_gmm.html#gmm-priors",
    "title": "40  EM applied to a Gaussian mixture model",
    "section": "40.3 GMM priors",
    "text": "40.3 GMM priors\nUntil now, except for the above treatment with uniform priors, we have left the choice of priors unspecified. There are a few challenges for choosing priors for Gaussian mixture models that arise from constraints on the parameters, and we address them one by one below.\n\n40.3.1 Prior for the mixing coefficients\nWe must have \\(0 \\le \\pi_k \\le 1\\) for all \\(\\pi_k\\) with \\(\\sum_{k=1}^K \\pi_k = 1\\). The mixing coefficients thus comprise a simplex. The Dirichlet distribution is convenient for defining priors for simplices. It is parametrized by a \\(K\\)-array \\(\\boldsymbol{\\alpha}\\) of positive numbers that push probability mass into corresponding mixing coefficients. An uninformative prior on a simplex is Dirichlet with \\(\\boldsymbol{\\alpha}\\) being an array of ones.\n\n\n40.3.2 Prior for the location parameters\nMixture models suffer from a label-switching nonidentifiability. To understand this issue, recall that we have a set of location parameters \\(\\{\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots \\boldsymbol{\\mu}_K\\}\\). We can permute the subscripts of these parameters and we still get the same model because the subscript labels are arbitrary. In practice, that means that if we, say, swapped the values of \\(\\boldsymbol{\\mu}_1\\) and \\(\\boldsymbol{\\mu}_2\\), \\(\\pi_1\\) and \\(\\pi_2\\), and \\(\\mathsf{\\Sigma}_1\\) and \\(\\mathsf{\\Sigma}_2\\) we get exactly the same posterior probability density.\nThis is a tricky problem in general. One approach is to enforce that \\(\\mu_{11} &lt; \\mu_{21} &lt; \\cdots &lt; \\mu_{k1}\\). That is, the entries of each \\(\\boldsymbol{\\mu}_k\\) for the first dimension of the multivariate Gaussian are ordered.\n\n\n40.3.3 Prior for the covariance matrix\nAny covariance matrix \\(\\Sigma\\) must be real symmetric positive definite. It can therefore be tricky to come up with a prior distribution, and further to ensure positive definiteness.\nIt is useful to note that any real symmetric positive definite matrix has a unique Cholesky decomposition \\(\\mathsf{L}\\) such that \\(\\mathsf{\\Sigma} = \\mathsf{L}\\cdot \\mathsf{L}^\\mathsf{T}\\), where \\(\\mathsf{L}\\) is a lower triangular matrix with real entries with the diagonal entries being positive. It is therefore often easier to consider the Cholesky decomposition of a covariance matrix when performing optimization (or any other kind of inference) because you can in practice leave the entries of the Cholesky decomposition unconstrained and the covariance matrix will retain positive definiteness.\nIt is further useful to note that any real symmetric positive definite matrix can be written as \\(\\mathsf{\\Sigma} = \\mathsf{S}\\cdot \\mathsf{C} \\cdot \\mathsf{S}\\), where \\(\\mathsf{S}\\) is a diagonal matrix with the diagonal being the square root of the diagonal entries of \\(\\mathsf{\\Sigma}\\), and \\(\\mathsf{C}\\) is a symmetric matrix whose with entry \\(i,j\\) being \\(C_{ij} = \\Sigma_{ij} / \\sqrt{\\Sigma_{ii} \\Sigma_{jj}}\\). Note that the diagonal entries of \\(\\mathsf{C}\\) are all one. It follows that \\(\\mathsf{\\Sigma}\\) is positive definite if and only if \\(\\mathsf{C}\\) is positive definite. The matrix \\(\\mathsf{C}\\) is referred to as a correlation matrix.\nGiven these two facts, a useful (and typical) approach to priors for covariance matrices are to specify priors for the standard deviations of each dimension (the square roots of the diagonal entries of \\(\\mathsf{\\Sigma}\\) and then to specify a prior for the correlation matrix and ensure positive definiteness. We could just specify a prior for the Cholesky decomposition of the correlation matrix so that whatever values we get for the Cholesky decomposition will result in a positive definite covariance matrix. (The diagonal of the Cholesky decomposition of a correlation matrix is necessarily comprised of ones, so the only the off-diagonal entries in the lower triangle need be inferred.)\nThe Lewandowski-Kurowicka-Joe (LKJ) distribution provides convenient priors for correlation matrices, and is implemented in Stan. It has a single positive scalar parameter, \\(\\eta\\), which tunes the strength of the correlations. If \\(\\eta = 1\\), the density is uniform over all correlation matrices. If \\(\\eta &gt; 1\\), matrices with a stronger diagonal (and therefore smaller correlations) are favored. If \\(\\eta &lt; 1\\), the diagonal is weak and correlations are favored.\nIn the bivariate case more fine-grained control of the prior can be easily achieved. A covariance matrix may be decomposed as follows.\n\\[\\begin{align}\n\\mathsf{\\Sigma} = \\begin{pmatrix}\n\\sigma_1^2 & \\sigma_{12} \\\\[0.5em]\n\\sigma_{12} & \\sigma_2^2\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sigma_1 & 0 \\\\[0.5em]\n0 & \\sigma_2\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n1 & \\rho \\\\[0.5em]\n\\rho & 1\n\\end{pmatrix}\n\\cdot \\begin{pmatrix}\n\\sigma_1 & 0 \\\\[0.5em]\n0 & \\sigma_2\n\\end{pmatrix},\n\\end{align}\n\\]\nwhere \\(\\rho\\) is the bivariate correlation, also known as the Pearson correlation, given by\n\\[\\begin{align}\n\\rho = \\frac{\\sigma_{12}}{\\sigma_1\\,\\sigma_2}.\n\\end{align}\n\\]\nPositive definiteness of \\(\\mathsf{\\Sigma}\\) is achieved when \\(-1 &lt; \\rho &lt; 1\\). Therefore, we can provide priors for \\(\\sigma_{1}\\), \\(\\sigma_{2}\\), and \\(\\rho\\) for a covariance matrix we wish to parametrize. Given that \\(\\rho\\) must be between \\(-1\\) and \\(1\\), a common prior is \\(\\rho \\sim 2\\,\\mathrm{Beta}(\\alpha, \\beta) - 1\\). An \\(\\mathrm{LKJ}(\\eta)\\) prior is equivalent to \\(\\rho \\sim 2\\,\\mathrm{Beta}(\\eta, \\eta) - 1\\).",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>EM applied to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm_example.html",
    "href": "lessons/optimization/em_gmm_example.html",
    "title": "41  An example application of the EM algorithm to a Gaussian mixture model",
    "section": "",
    "text": "41.0.1 Full MCMC of the generative model\n| Download notebook\nSome of the material in this notebook is based on a similar notebook written by Kayla Jackson.\nAs an example of an application of the EM algorithm to a Gaussian mixture model, we can use a now-classic data set featuring the waiting time and durations of eruptions of the Old Faithful geyser in Yellowstone National Park. The data set consists of a set of measurements of eruptions in 1985 and was published by Azzalini and Bowman in 1990. Let us first take a look at the data.\nTo build our model, we assume there are two bivariate Gaussians in our mixture. We will assume uniform priors for the mixing coefficients and for the correlation matrix. We choose reasonable priors for the remaining parameters. Our model is\n\\[\\begin{align}\n&\\pi_1 \\sim \\mathrm{Beta}(1, 1)\\;\\text{(equivalent to }\\boldsymbol{\\pi} \\sim \\text{Dirichlet}(1, 1)\\text{)}, \\\\[1em]\n&\\pi_2 = 1- \\pi_1, \\\\[1em]\n&\\mu_{d,1}, \\mu_{d,2} \\sim \\mathrm{Norm}(5, 2), \\\\[1em]\n&\\mu_{w,1}, \\mu_{w,2} \\sim \\mathrm{Norm}(60, 10), \\\\[1em]\n&\\boldsymbol{\\mu}_1 = (\\mu_{d,1}, \\mu_{w,1})^\\mathsf{T},\\\\[1em]\n&\\boldsymbol{\\mu}_2 = (\\mu_{d,2}, \\mu_{w,2})^\\mathsf{T},\\\\[1em]\n&\\sigma_{d,1}, \\sigma_{d,2} \\sim \\mathrm{HalfNorm}(5), \\\\[1em]\n&\\sigma_{w,1}, \\sigma_{w,2} \\sim \\mathrm{HalfNorm}(20), \\\\[1em]\n&\\rho_{1}, \\rho_{2} \\sim 2\\,\\mathrm{Beta}(1, 1) - 1\\;\\text{(equivalent to }\\mathsf{C}_1, \\mathsf{C}_2 \\sim \\text{LKJ}(1)\\text{)}, \\\\[1em]\n&\\mathsf{\\Sigma}_1 = \\begin{pmatrix}\n\\sigma_{d,1} & 0 \\\\[0.5em]\n0 & \\sigma_{w,1}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n1 & \\rho_1 \\\\[0.5em]\n\\rho_1 & 1\n\\end{pmatrix}\n\\cdot \\begin{pmatrix}\n\\sigma_{d,1} & 0 \\\\[0.5em]\n0 & \\sigma_{w,1}\n\\end{pmatrix}, \\\\[1em]\n&\\mathsf{\\Sigma}_2 = \\begin{pmatrix}\n\\sigma_{d,2} & 0 \\\\[0.5em]\n0 & \\sigma_{w,2}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n1 & \\rho_2 \\\\[0.5em]\n\\rho_2 & 1\n\\end{pmatrix}\n\\cdot \\begin{pmatrix}\n\\sigma_{d,2} & 0 \\\\[0.5em]\n0 & \\sigma_{w,2}\n\\end{pmatrix}, \\\\[1em]\n&z_{i1} \\mid \\pi_1 \\sim \\text{Bernoulli}(\\pi_1) \\;\\forall i, \\\\[1em]\n&z_{i2} = 1 - z_{i,1} \\; \\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\sim \\mathrm{Norm}(\\boldsymbol{\\mu}_1,\\mathsf{\\Sigma}_1)^{z_{i1}} \\cdot \\mathrm{Norm}(\\boldsymbol{\\mu}_2,\\mathsf{\\Sigma}_2)^{z_{i2}}\\;\\forall i.\n\\end{align}\n\\]\nTo avoid a label switching nonidentifiability, we stipulate that \\(\\mu_{d,1} &lt; \\mu_{d,2}\\).\nWe start by applying the gold standard: performing full MCMC to sample out of the posterior distribution. Because HMC (Stan’s algorithm) can only sample continuous variables, we must sample out of the marginal posterior where we have marginalized out the \\(\\mathbf{z}_i\\)’s. This is not problematic, as we can always compute the responsibilities for each sample. The Stan code below is more general for Gaussian mixture models, but we can use it in this case.\nLet’s put this to use!\n# Extract data set from data frame\ny = df.select(pl.col('duration-minutes', 'waiting-minutes')).to_numpy()\n\n# Prepare data for input, including \n# number of components in mixture and number of dimensions.\ndata = dict(\n    K=2,\n    d=2,\n    N=len(df),\n    y=y,\n)\n\n# Compile and sample!\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='old_faithful_gmm.stan')\n    samples = sm.sample(data=data)\n\n# Get a convenient ArviZ object\nsamples = az.from_cmdstanpy(samples)\n\n# As always, check diagnostics\nbebi103.stan.check_all_diagnostics(samples, parameters=['mu', 'L_Sigma', 'pi_'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\nLooks great! We can check a corner plot to see where the centers of the two Gaussians of the mixture model are.\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples, \n        parameters=[\n            ('mu[0,0]', r'$$\\mu_{\\mathrm{d},1}$$ (min)'), \n            ('mu[0,1]', r'$$\\mu_{\\mathrm{d},2}$$ (min)'), \n            ('mu[1,0]', r'$$\\mu_{\\mathrm{w},1}$$ (min)'), \n            ('mu[1,1]', r'$$\\mu_{\\mathrm{w},2}$$ (min)')\n        ],\n        xtick_label_orientation=np.pi/4,\n    )\n)\nThis looks about right from what we would expect from our exploration of the plot. Conveniently, the corner plot displays credible intervals for the parameters describing the centers of the bivariate Gaussians in the mixture. We can explicitly calculate these from the samples.\n# Credible intervals on all mu's\nfor component in [0, 1]:\n    for dim, descriptor in zip([0, 1], ['duration (min)', 'waiting (min)']):\n        print(f\"95% cred int for mean {descriptor} for component {component}:  \", end='')\n        print(np.percentile(samples.posterior.mu.sel(dict(mu_dim_0=component, mu_dim_1=dim)).values, [2.5, 97.5]))\n\n95% cred int for mean duration (min) for component 0:  [1.98143675 2.09175075]\n95% cred int for mean waiting (min) for component 0:  [53.302945 55.70961 ]\n95% cred int for mean duration (min) for component 1:  [4.2240095  4.35365475]\n95% cred int for mean waiting (min) for component 1:  [78.9693875 80.8362875]\nWe can further explore the results by making a plot of the data overlaid with a contour plot of the mixture of Gaussians given by the expectation of the parameter values. We can then color the data points according to the responsibilities. We start by computing expectations of the responsibilities.\nresponsibilities = (\n    samples.posterior.responsibilities\n    .sel(dict(responsibilities_dim_1=0))\n    .mean(dim=['chain', 'draw'])\n)\nTo color the data points according to which component of the mixture model they came from, we can convert the quantitative responsibilities to colors using the convenient bebi103.viz.q_to_color() function that converts quantitative information to colors.\ncolors = bebi103.viz.q_to_color(responsibilities, colorcet.CET_D11, low=0, high=1)\nNext, we can compute expectations of the \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\mathsf{\\Sigma}\\) parameters.\npi_1, pi_2 = samples.posterior.pi_.median(dim=['chain', 'draw']).values\nmu_1, mu_2 = samples.posterior.mu.median(dim=['chain', 'draw']).values\nSigma_1, Sigma_2 = samples.posterior.Sigma.median(dim=['chain', 'draw']).values\nNow we make the data from which the contours will be calculated.\nduration = np.linspace(1.5, 5.5, 200)\nwaiting = np.linspace(40, 100, 200)\nX, Y = np.meshgrid(duration, waiting)\nZ = np.exp(\n    np.logaddexp(\n        np.log(pi_1) + st.multivariate_normal.logpdf(np.dstack((X, Y)), mean=mu_1, cov=Sigma_1),\n        np.log(pi_2) + st.multivariate_normal.logpdf(np.dstack((X, Y)), mean=mu_2, cov=Sigma_2)\n    )\n)\nFinally, we can make the plot, first plotting the contours and then overlaying the appropriately shaded data.\np = bebi103.viz.contour(\n    X, \n    Y, \n    Z,     \n    frame_width=300,\n    frame_height=300,\n    x_axis_label='eruption duration (min)',\n    y_axis_label='eruption waiting time (min)'\n)\n\n# Add data\np.scatter(x=df['duration-minutes'], y=df['waiting-minutes'], color=colors)\n\nbokeh.io.show(p)\nThis is a reasonably effective visualization that is commonly employed for data set of this type with a Gaussian mixture model. I would not use this visualization alone; corner plots and/or credible intervals for the parameters are important as well.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>An example application of the EM algorithm to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm_example.html#gmms-with-scikit-learn",
    "href": "lessons/optimization/em_gmm_example.html#gmms-with-scikit-learn",
    "title": "41  An example application of the EM algorithm to a Gaussian mixture model",
    "section": "41.1 GMMs with scikit-learn",
    "text": "41.1 GMMs with scikit-learn\nThe scikit-learn package has built-in functionality to perform parameter estimation for Gaussian mixture models. However, it only works with uniform priors. That is, scikit-learn does maximum likelihood estimation, and not a full Bayesian parameter estimation. Nonetheless, the functionality is useful, and we demonstrate its use here.\nWe start by instantiating a GMM with the appropriate number \\(K\\) of components in the mixture. For the present data set, \\(K = 2\\).\n\ngmm = sklearn.mixture.GaussianMixture(2)\n\nNext, we “fit” the model using our data set y.\n\ngmm.fit(y)\n\nGaussianMixture(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixture?Documentation for GaussianMixtureiFittedGaussianMixture(n_components=2) \n\n\nWe can pull the MAP (again, it’s a MAP for a model with uniform priors) using the appropriate attributes. For example, our array mu above, in which the first row is \\(\\boldsymbol{\\mu}\\) for the first component and the second row is \\(\\boldsymbol{\\mu}\\) for the second component, is accessed using the means_ attribute of the GaussianMixture instance.\n\ngmm.means_\n\narray([[ 4.28977944, 79.96953298],\n       [ 2.03652149, 54.47986018]])\n\n\nThis is quite similar to the result we got with the full prior, which is not surprising given we had plenty of data and the model seems to be a good descriptor of the generative process. Note, though, that the order of the components may be different, since scikit-learn does not handle the label-switching nonidenfiability.\nSimilarly, we can get the values of \\(\\mathsf{\\Sigma}\\) using the covariances_ attribute.\n\ngmm.covariances_\n\narray([[[ 0.16982046,  0.93871793],\n        [ 0.93871793, 36.02497019]],\n\n       [[ 0.06927449,  0.43627723],\n        [ 0.43627723, 33.70493352]]])\n\n\nAnd the mixing coefficients using the weights_ attribute.\n\ngmm.weights_\n\narray([0.64407255, 0.35592745])\n\n\nFinally, we can access the responsibilities using the predict_proba() method.\n\n# Show first five for brevity\ngmm.predict_proba(y)[:5]\n\narray([[9.99999997e-01, 2.68469170e-09],\n       [1.86358243e-09, 9.99999998e-01],\n       [9.99991353e-01, 8.64671381e-06],\n       [1.05070205e-05, 9.99989493e-01],\n       [1.00000000e+00, 1.08537069e-21]])\n\n\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>An example application of the EM algorithm to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/em_gmm_example.html#computing-environment",
    "href": "lessons/optimization/em_gmm_example.html#computing-environment",
    "title": "41  An example application of the EM algorithm to a Gaussian mixture model",
    "section": "41.2 Computing environment",
    "text": "41.2 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,statsmodels,sklearn,arviz,cmdstanpy,bokeh,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy      : 2.1.3\npolars     : 1.27.1\nscipy      : 1.15.2\nstatsmodels: 0.14.4\nsklearn    : 1.6.1\narviz      : 0.21.0\ncmdstanpy  : 1.2.5\nbokeh      : 3.6.2\nbebi103    : 0.1.27\njupyterlab : 4.4.2\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>An example application of the EM algorithm to a Gaussian mixture model</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/kmeans.html",
    "href": "lessons/optimization/kmeans.html",
    "title": "42  K-means clustering",
    "section": "",
    "text": "42.1 K-means with scikit-learn\n| Download notebook\nWe has seen the EM algorithm applied to a Gaussian mixture model. In computing the responsibilities, we are able to probabilistically assign each data point to one of the components of the mixture model. In this way, inference of the parameters of a GMM may be viewed as a form of clustering, in which each data point is assigned to a group, or cluster.\nOne of the simplest, and also fairly widely used, methods is K-means clustering (usually written with a lowercase k, but we use a capital K here to connect it to GMMs that have \\(K\\) components). This method is typically described in term of its algorithm. The algorithm proceeds as follows for a set of \\(d\\)-dimensional data.\nWhile the K-means algorithm is easy to implement, we do not have to do it, as it is implemented in scikit-learn. Let us apply it to the Old Faithful data set we used in ?sec-gmm-em-example.\n# Load in data\ndf = pl.read_csv(os.path.join(data_path, 'old_faithful.csv'))\n\n# Pull data out as a Numpy array\ny  = df.select(pl.col('duration-minutes', 'waiting-minutes')).to_numpy()\n\n# Instantiate KMeans instance with two clusters\nkmeans = sklearn.cluster.KMeans(2)\n\n# Perform optimziation to get means and cluster assignments\nkmeans.fit(y)\n\nKMeans(n_clusters=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2)\nWe can access the means (cluster centers) using the cluster_centers_ attribute and the cluster assignments using the labels_ attribute. We will use those to make a plot of the cluster assignments of the data in the Old Faithful data set.\n# Color according to labels\ncolors = ['#00b6ff' if label==1 else '#f6906c' for label in kmeans.labels_]\n\n# Plot data with label by color\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label='eruption duration (min)',\n    y_axis_label='eruption waiting time (min)'\n)\np.scatter(y[:,0], y[:,1], color=colors)\n\n# Put in means\np.scatter(\n    kmeans.cluster_centers_[:, 0], \n    kmeans.cluster_centers_[:, 1], \n    color='black', \n    marker='+',\n    size=15\n)\n\n\nbokeh.io.show(p)\nThe result is similar to what we say in using EM (see Chapter 41), but the data points that apparently lie between the two clusters differ in their assignment.\nFor K-means clustering, it is wise to first center-and-scale the data. That is, perform a linear transformation on the observations such that the data points have a mean of zero and a variance of one.\n# Center and scale y\nscaler = sklearn.preprocessing.StandardScaler().fit(y)\ny_scaled = scaler.transform(y)\n\n# Do K-means on centered-and-scaled\nkmeans = sklearn.cluster.KMeans(2).fit(y_scaled)\n\n# Rescale the cluster centers\ncluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n\n# Plot data\ncolors = ['#00b6ff' if label==1 else '#f6906c' for label in kmeans.labels_]\n\n# Plot data with label by color\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label='eruption duration (min)',\n    y_axis_label='eruption waiting time (min)'\n)\np.scatter(y[:,0], y[:,1], color=colors)\n\n# Put in means\np.scatter(\n    cluster_centers[:, 0], \n    cluster_centers[:, 1], \n    color='black', \n    marker='+',\n    size=15\n)\n\nbokeh.io.show(p)\nThese results are closer to what we got with a GMM.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/kmeans.html#k-means-clustering-as-a-gmm",
    "href": "lessons/optimization/kmeans.html#k-means-clustering-as-a-gmm",
    "title": "42  K-means clustering",
    "section": "42.2 K-means clustering as a GMM",
    "text": "42.2 K-means clustering as a GMM\nAs if often the case, heuristic/algorithmic methods have a Bayesian generative model underlying them. Indeed, the above algorithm for K-means clustering screams EM!\nRecall from Equation 42.1 that the generative model for a GMM is\n\\[\\begin{align}\n&\\boldsymbol{\\pi} \\sim \\text{prior for mixing coefficients}, \\\\[1em]\n&\\boldsymbol{\\mu} \\sim \\text{prior for location parameters}, \\\\[1em]\n&\\mathsf{\\Sigma} \\sim \\text{prior for scale parameters}, \\\\[1em]\n&\\mathbf{z}_i \\mid \\boldsymbol{\\pi} \\sim \\text{Categorical}(\\boldsymbol{\\pi}) \\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\mathsf{\\Sigma} \\sim \\prod_{k=1}^K (\\mathrm{Norm}(\\boldsymbol{\\mu}_k,\\mathsf{\\Sigma}_k))^{z_{ik}} \\;\\forall i.\n\\end{align}\n\\tag{42.1}\\]\nWhen doing K-means, the priors are all uniform and all covariance matrices are diagonal with the same fixed very small entry \\(\\sigma^2\\) on the diagonal. The model is thus\n\\[\\begin{align}\n&\\boldsymbol{\\pi} \\sim \\text{Dirichlet}(1/K, 1/K, \\ldots, 1/K), \\\\[1em]\n&\\mathbf{z}_i \\mid \\boldsymbol{\\pi} \\sim \\text{Categorical}(\\boldsymbol{\\pi}) \\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid \\mathbf{z}_i, \\boldsymbol{\\mu}, \\sigma \\sim \\prod_{k=1}^K (\\mathrm{Norm}(\\boldsymbol{\\mu}_k, \\sigma))^{z_{ik}} \\;\\forall i.\n\\end{align}\n\\tag{42.2}\\]\nApproaching this model with an EM algorithm (see Section 40.2), the E step involves evaluating \\(g(z\\mid y, \\phi)\\), which in this case is\n\\[\\begin{align}\ng(z \\mid y, \\phi) = \\prod_{i=1}^N\\prod_{k=1}^K \\left[\\pi_k\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\sigma)\\right]^{z_{ik}},\n\\end{align}\n\\]\nresulting in a parameter-dependent part of the surrogate function of\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) = \\sum_{i=1}^N\\sum_{k=1}^k \\gamma(z_{ik}) \\left(\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\sigma)\\right),\n\\end{align}\n\\]\nwhere the responsibilities are\n\\[\\begin{align}\n\\gamma(z_{ik}) = \\displaystyle{\\frac{\\pi_k \\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_k, \\sigma)}{\\sum_{j=1}^K\\pi_j\\,\\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_j, \\sigma)}}\n= \\frac{\\pi_k\\,\\mathrm{e}^{-\\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_k \\rVert^2/2\\sigma^2}}{\\sum_{j=1}^K\\pi_j\\,\\mathrm{e}^{-\\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_j\\rVert^2/2\\sigma^2}}.\n\\end{align}\n\\]\nAs \\(\\sigma\\) gets really small, \\(\\mathrm{exp}[-\\lVert \\mathbf{y}_i - \\mathbf{\\mu}_k \\rVert^2/2\\sigma^2]\\) is largest for the \\(\\boldsymbol{\\mu}_k\\) that is closest to \\(\\mathbf{y}_i\\) and relatively tiny for every other term. Therefore, the responsibility is approximately one for the component \\(k\\) whose mean is closest to \\(\\mathbf{y}_i\\) and zero for the other components. Since the responsibilities are all one or zero, each data point is assigned to the cluster whose mean is closest.\nTherefore, the parameter-dependent part of the surrogate function is\n\\[\\begin{align}\n\\mathcal{Q}(\\theta, \\phi) = \\sum_{i=1}^N \\ln \\mathcal{N}(\\mathbf{y}_i \\mid \\boldsymbol{\\mu}_{ik}, \\sigma)\n= -\\frac{1}{2\\sigma}\\sum_{i=1}^N \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_{ik} \\rVert^2,\n\\end{align}\n\\]\nwhere we have define \\(\\mu_{ik}\\) to be the mean of the cluster to which data point \\(i\\) is assigned. So, the goal in the M step is to find the value of \\(\\boldsymbol{\\mu}_{ik}\\) that maximizes the surrogate function. We differentiate with respect to \\(\\boldsymbol{\\mu}_k\\). The terms of the sum that survive are only those assigned to cluster \\(k\\), such that\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k} = -\\frac{1}{\\sigma}\\sum_{i\\text{ in cluster }k} (\\mathbf{y}_i - \\boldsymbol{\\mu}_{k}).\n\\end{align}\n\\]\nSetting the derivative to zero gives\n\\[\\begin{align}\n\\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{i\\text{ in cluster }k} \\mathbf{y}_i,\n\\end{align}\n\\]\nwhich says that \\(\\mu_k\\) is updated to be the mean of all of the data points assigned to it. So, the EM algorithm applied to the mixture model described by Equation 42.2 is indeed the algorithm for K-means clustering, thereby making clear what the generative model underlying it is. It’s a rather restrictive model with all uniform priors and a very rigid ascribed covariance matrix for the likelihood.",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "lessons/optimization/kmeans.html#computing-environment",
    "href": "lessons/optimization/kmeans.html#computing-environment",
    "title": "42  K-means clustering",
    "section": "42.3 Computing environment",
    "text": "42.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p polars,sklearn,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\npolars    : 1.27.1\nsklearn   : 1.6.1\nbokeh     : 3.6.2\njupyterlab: 4.4.2",
    "crumbs": [
      "Summarizing posterior distributions with maxima",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical.html",
    "href": "lessons/hierarchical/hierarchical.html",
    "title": "Hierarchical models",
    "section": "",
    "text": "In this section, we discuss an important class of models in which parameters are conditioned on other parameters. This results in a hierarchical structure of conditioning. These models commonly arise, but are difficult to treat computationally.",
    "crumbs": [
      "Hierarchical models"
    ]
  },
  {
    "objectID": "lessons/hierarchical/modeling_repeated_experiments.html",
    "href": "lessons/hierarchical/modeling_repeated_experiments.html",
    "title": "43  Modeling repeated experiments",
    "section": "",
    "text": "43.1 A model for reversals\n| Download notebook\nIn this lesson, we will investigate hierarchical models, in which some model parameters are dependent on others in specific ways. As is often the case, this is perhaps best learned by example.\nIn ?exr-boolean-sampling, we studied reversals under exposure to blue light in C. elegans with Channelrhodopsin in two different neurons. Let’s consider one of the strains which contains a Channelrhodopsin in the ASH sensory neuron. We considered data done in three different years by the students of Bi 1x. In 2015, we found that 9 out of 35 worms reversed under exposure to blue light. In 2016, 12 out of 35 reversed. In 2017, 18 out of 54 reversed.\nAs we have worked out, a Binomial likelihood makes sense for this experiment; the number \\(n\\) out of \\(N\\) Bernoulli trials that are successful is Binomially distributed. The parameter we are trying to estimate, the probability of reversal \\(\\theta\\), needs a prior. A Beta distribution makes sense for this, and we will choose a Beta distribution that weakly disfavors a zero or one probability of reversal.\n\\[\\begin{align}\n&\\theta \\sim \\text{Beta}(1.1, 1.1),\\\\[1em]\n&n \\sim \\text{Binomial}(N, \\theta).\n\\end{align}\\]\nThe problem is that this is a generative model for a single experiment. We did the experiment in 2015, getting \\(n/N = 9/35\\), and again in 2016, getting \\(n/N = 12/35\\), and in 2017 with \\(n/N = 18/54\\). Actually, we could imagine doing the experiment over and over again, say \\(k\\) times, each time getting a value of \\(n\\) and \\(N\\). Conditions may change from experiment to experiment. For example, we may have different lighting set-ups, slight differences in the strain of worms we’re using, etc. We are left with some choices on how to model the data.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modeling repeated experiments</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/modeling_repeated_experiments.html#a-model-for-reversals",
    "href": "lessons/hierarchical/modeling_repeated_experiments.html#a-model-for-reversals",
    "title": "43  Modeling repeated experiments",
    "section": "",
    "text": "43.1.1 Pooled data: identical parameters\nWe could pool all of the data together. In other words, let’s say we measure \\(n_1\\) out of \\(N_1\\) reversals in the first set of experiments, \\(n_2\\) out of \\(N_2\\) reversals in the second set, etc., up to \\(k\\) total experiments. We could pool all of the data together to get\n\\[\\begin{align}\n&n = \\sum_{i=1}^k n_i \\text{ out of } N = \\sum_{i=1}^k N_i \\text{ reversals}.\n\\end{align}\\]\nWe then compute our posterior as in the model above. Here, the modeling assumption is that the result in each experiment are governed by identical parameters. That is to say that we assume \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_k = \\theta\\). This is the approach we took in Homework 4.2.\n\n\n43.1.2 Independent parameters\nAs an alternative model, we could instead say that the parameters in each experiment are totally independent of each other. In this case, we assume that \\(\\theta_1\\), \\(\\theta_2\\), \\(\\ldots\\), \\(\\theta_k\\) are all independent of each other. In this case, we have \\(k\\) separate models to fit, each looking like\n\\[\\begin{align}\n&\\theta_i \\sim \\text{Beta}(1.1, 1.1),\\\\[1em]\n&n_i \\sim \\text{Binomial}(N_i, \\theta_i).\n\\end{align}\\]\nWhen we do the modeling in this way, we often report a value of \\(\\theta\\) that is given by the mean of the \\(\\theta_i\\)’s with some error bar.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modeling repeated experiments</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/modeling_repeated_experiments.html#the-best-of-both-worlds-a-hierarchical-model",
    "href": "lessons/hierarchical/modeling_repeated_experiments.html#the-best-of-both-worlds-a-hierarchical-model",
    "title": "43  Modeling repeated experiments",
    "section": "43.2 The best of both worlds: A hierarchical model",
    "text": "43.2 The best of both worlds: A hierarchical model\nEach of these extremes have their advantages. We are often trying to estimate a parameter that is more universal than our experiments, e.g., something that describes worms with Channelrhodopsin in the ASH neuron generally. So, pooling the experiments makes sense. On the other hand, we have reason to assume that there is going to be a different value of \\(\\theta\\) in different experiments, as biological systems are highly variable, not to mention measurement variations. So, how can we capture both of these effects?\nWe can consider a model in which there is a “global” reversal probability, which we will call \\(\\phi\\), and the values of \\(\\theta_i\\) may vary from this \\(\\phi\\) according to some probability distribution, \\(g(\\theta_i\\mid \\phi)\\). So now we have parameters \\(\\theta_1, \\theta_2, \\ldots, \\theta_k\\) and \\(\\phi\\). So, the posterior can be written using Bayes’s theorem, defining \\(\\theta = (\\theta_1, \\theta_2, \\ldots)\\), \\(N = (N_1, N_2, \\ldots)\\), and \\(n = (n_1, n_2, \\ldots)\\),\n\\[\\begin{align}\ng(\\phi,\\theta\\mid n, N) = \\frac{f(n,N\\mid \\phi, \\theta)\\, g(\\phi, \\theta)}{f(n, N)}.\n\\end{align}\\]\nNote, though, that the observed values of \\(n\\) do not depend directly on \\(\\phi\\), only on \\(\\theta\\). In other words, the observations are only indirectly dependent on \\(\\phi\\). So, we can write \\(f(n,N\\mid \\phi, \\theta) = f(n,N\\mid \\theta)\\). Thus, we have\n\\[\\begin{align}\ng(\\phi,\\theta\\mid n, N) = \\frac{f(n,N\\mid \\theta)\\,g(\\phi, \\theta)}{f(n, N)}.\n\\end{align}\\]\nNext, we can rewrite the prior using the definition of conditional probability.\n\\[\\begin{align}\ng(\\phi,\\theta) = g(\\theta\\mid \\phi)\\, g(\\phi).\n\\end{align}\\]\nSubstituting this back into our expression for the posterior, we have\n\\[\\begin{align}\ng(\\phi,\\theta\\mid n, N) = \\frac{f(n,N\\mid \\theta)\\, g(\\theta\\mid \\phi)\\, g(\\phi)}{f(n, N)}.\n\\end{align}\\]\nNow, if we read off the numerator of this equation, we see a chain of dependencies (conditioning). The experimental results \\(n\\) are conditioned on parameters \\(\\theta\\). Parameters \\(\\theta\\) are conditioned on hyperparameter \\(\\phi\\). Hyperparameter \\(\\phi\\) then has some hyperprior distribution. Any model that can be written as a chain of conditioning like this is called a hierarchical model, and the parameters that do not directly condition the data are called hyperparameters.\nSo, the hierarchical model captures both the experiment-to-experiment variability, as well as the “global” regulator of outcomes, the hyperparameter. Note that the product \\(g(\\theta\\mid \\phi)\\, g(\\phi)\\) comprises the prior, as it is independent of the observed data.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modeling repeated experiments</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/choosing_a_hierarchical_prior.html",
    "href": "lessons/hierarchical/choosing_a_hierarchical_prior.html",
    "title": "44  Choosing a hierarchical prior",
    "section": "",
    "text": "44.1 Exchangeability\n| Download notebook\nChoice of a hierarchical prior is not always as straightforward as for priors we are used to considering because we have to specify the hyperprior and all conditional priors, \\(g(\\theta\\mid \\phi)\\).\nThe conditional probability, \\(g(\\theta\\mid \\phi)\\), can take any reasonable form. In the case where we have no reason to believe that we can distinguish any one \\(\\theta_i\\) from another prior to the experiment, then the label “\\(i\\)” applied to the experiment may be exchanged with the label of any other experiment. I.e., \\(g(\\theta_1, \\theta_2, \\ldots, \\theta_k \\mid \\phi)\\) is invariant to permutations of the indices. Parameters behaving this way are said to be exchangeable. A common (simple) exchangeable distribution is\n\\[\\begin{align}\ng(\\theta\\mid \\phi) = \\prod_{i=1}^k g(\\theta_i\\mid \\phi),\n\\end{align}\\]\nwhich means that each of the parameters is an independent sample out of a distribution \\(g(\\theta_i\\mid \\phi)\\), which we often take to be the same for all \\(i\\). This is reasonable to do in the worm reversal example.\nIn all of the hierarchical models we will work with, we will assume exchangeability. In situations where the indices of the experiment contain real information, meaning that the prior is no longer invariant to permuations of the indices, we lose exchangeability. An example would be if we did one set of experiments on one instrument and another set of experiments on another instrument. If we suspect these different instrument may have a real effect on the measured data, we need to explicitly model the differences. This is an example of a factor model, which feature more nuance than the hierarchical models we will consider. We will not go into factor models in this course (but we definitely would if we had more time! Add it to the long list of beautiful advanced topics….). Of course, we may choose to ignore differences between the two instruments in our modeling and recover exchangeability.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Choosing a hierarchical prior</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/choosing_a_hierarchical_prior.html#choice-of-the-conditional-distribution",
    "href": "lessons/hierarchical/choosing_a_hierarchical_prior.html#choice-of-the-conditional-distribution",
    "title": "44  Choosing a hierarchical prior",
    "section": "44.2 Choice of the conditional distribution",
    "text": "44.2 Choice of the conditional distribution\nWe need to specify our prior, which for this hierarchical model means that we have to specify the conditional distribution, \\(g(\\theta_i\\mid \\phi)\\), as well as \\(g(\\phi)\\). We could assume a Beta prior for \\(\\phi\\); the one we chose in our original nonhierarchical model would be a good choice.\n\\[\\begin{align}\n\\phi \\sim \\text{Beta}(1.1, 1.1).\n\\end{align}\\]\nFor the conditional distribution \\(g(\\theta_i\\mid \\phi)\\), we might also assume it is Beta-distributed. This necessitates another parameter because the Beta distribution has two parameters.\nThe Beta distribution is typically written as\n\\[\\begin{align}\ng(\\theta\\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\,\n\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1},\n\\end{align}\\]\nwhere it is parametrized by positive constants \\(\\alpha\\) and \\(\\beta\\). The Beta distribution has mean and concentration, respectively, of\n\\[\\begin{align}\n\\phi &= \\frac{\\alpha}{\\alpha + \\beta}, \\\\[1em]\n\\kappa &= \\alpha + \\beta.\n\\end{align}\\]\nThe concentration \\(\\kappa\\) is a measure of how sharp the distribution is. The bigger \\(\\kappa\\) is, the most sharply peaked the distribution is. Since we would like to parametrize our Beta distribution with its mean \\(\\phi\\), we could use \\(\\kappa\\) as our other parameter. So, our expression for the posterior is\n\\[\\begin{align}\ng(\\theta, \\phi, \\kappa \\mid n, N) = \\frac{f(n,N\\mid \\theta)\\,\\left(\n\\prod_{i=1}^k g(\\theta_i\\mid \\phi, \\kappa)\\right)\\,g(\\phi, \\kappa)}{f(n, N)}.\n\\end{align}\\]\nWe are left to specify the hyperprior \\(g(\\phi, \\kappa)\\). We will take \\(\\phi\\) to come from a Beta distribution and \\(\\kappa\\) to come from an weakly informative Half-Normal. Note that to switch from a parametrization using \\(\\phi\\) and \\(\\kappa\\) to one using \\(\\alpha\\) and \\(\\beta\\), we can use\n\\[\\begin{align}\n&\\alpha = \\phi \\kappa\\\\[1em]\n&\\beta = (1-\\phi)\\kappa.\n\\end{align}\\]\nWith all of this, we can now put together our model.\n\\[\\begin{align}\n&\\phi \\sim \\text{Beta}(1.1, 1.1), \\\\[1em]\n&\\kappa \\sim \\text{HalfNorm}(0, 1000), \\\\[1em]\n&\\alpha = \\phi \\kappa, \\\\[1em]\n&\\beta = (1-\\phi)\\kappa,\\\\[1em]\n&\\theta_i \\sim \\text{Beta}(\\alpha, \\beta) \\;\\;\\forall i,\\\\[1em]\n&n_i \\sim \\text{Binom}(N_i, \\theta_i)\\;\\;\\forall i.\n\\end{align}\\]\nThis is a complete specification of a hierarchical model.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Choosing a hierarchical prior</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical_implementation.html",
    "href": "lessons/hierarchical/hierarchical_implementation.html",
    "title": "45  Implementation of hierarchical models",
    "section": "",
    "text": "45.1 Hierarchical model structure\n| Download notebook\nWe have learned about how to construct hierarchical models and how to sample from the resulting posterior distribution. In this lesson, we will dig a bit deeper and address some of the challenges in sampling out of hierarchical models.\nTo think about the structure of hierarchical models, we will consider the following experimental design. We are measuring fluorescent intensity of a reporter of gene expression in E. coli cells. One Monday, we prepare four batches of E. coli and grow them up on plates. From each plate, we select colonies. We then mount a slide with a selection from a given colony and use fluorescence microscopy to determine the fluorescence level of individual cells. We do a similar experiment on Wednesday, and then another on Thursday. We model the measured fluorescence values as Normally distributed.\nThere is a hierarchical structure here, depicted below.\nThe colony-level information then informs the data. In the above diagram, I have indicated how many individual cells are measured in each microscope experiment.\nThe structure here shows the conditioning. The measured data are parametrized by the colony-level information, which is itself parametrized by the batch-level information, which is itself parametrized by the day-level information, which is finally parametrized by the hyperparameters at the root of the hierarchy.\nTo keep track of all the hyperparameters, it is easiest to define arrays for the hyperparameters at each level. In this example, we have\n\\[\\begin{align}\n&\\text{Level 0 parameter: }\\phantom{\\text{s}} \\theta \\\\[1em]\n&\\text{Level 1 parameters: } \\theta_1 \\equiv (\\theta_1^1, \\theta_1^2, \\theta_1^3) \\\\[1em]\n&\\text{Level 2 parameters: } \\theta_2 \\equiv (\\theta_2^1, \\theta_2^2, \\theta_2^3, \\ldots \\theta_2^8) \\\\[1em]\n&\\text{Level 3 parameters: } \\theta_3 \\equiv (\\theta_3^1, \\theta_3^2, \\theta_3^3, \\ldots \\theta_3^{17}).\n\\end{align}\\]\nTo complete the formalization of the specification, we need arrays that tell us which upon which hyperparameter each parameter is conditioned. Level 1 is special because it is conditioned only by \\(\\theta\\). The respective elements in the level 2 parameters are conditioned by the following level 1 conditioners.\n\\[\\begin{align}\n&\\text{level 1 hyperparameter conditioners: } (\\theta_1^1, \\theta_1^1, \\theta_1^1, \\theta_1^1, \\theta_1^2, \\theta_1^2, \\theta_1^2, \\theta_1^3).\n\\end{align}\\]\nTo avoid all of the subscripting and superscripting, we can write this as\n\\[\\begin{align}\n\\text{index 1: } (1, 1, 1, 1, 2, 2, 2, 3),\n\\end{align}\\]\nwith level 3’s conditioners in level 2 being\n\\[\\begin{align}\n\\text{index 2: } (1, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 6, 7, 8, 8, 8, 8),\n\\end{align}\\]\nand finally the data conditioned by level 3’s parameters,\n\\[\\begin{align}\n\\text{index 3: } (&1,  1,  1,  1,  1,  2,  2,  3,  4,  4,  5,  5,  6,  6,  7,  7,  7,\\\\\n&8,  8,  8,  8,  9,  9, 10, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12,\\\\\n&13, 13, 13, 13, 13, 13, 14, 15, 15, 15, 16, 16, 17).\n\\end{align}\\]\nWe define that there are \\(J_1\\) parameters in level 1, \\(J_2\\) in level 2, and generically \\(J_k\\) in level \\(k\\). In this case,\n\\[\\begin{align}\n&J_1 = 3,\\\\[1em]\n&J_2 = 8,\\\\[1em]\n&J_3 = 17.\\\\[1em]\n\\end{align}\\]\nTo have a concrete model in mind, we will assume that all colonies have the same scale parameter, but differing location parameters. We assume further that all hyperparameters are conditioned on those at a level above via a Normal relationship, and we assume the same variance \\(\\tau\\) for relationships all the way down the hierarchy. That is \\(\\tau\\) applies for conditioning level 3 on level 2, level 2 on level 1, and level 1 on level 0. We consider a hyperprior for \\(\\theta\\) to be a Normal distribution centered on 10 arbitrary fluorescence units with a standard deviation of 3, and \\(\\sigma\\) to have a Half-Normal prior. Our statistical model is then defined as follows, with a weakly informative hyperprior on \\(\\tau\\).\n\\[\\begin{align}\n&\\theta \\sim \\text{Norm}(10, 3) \\\\[1em]\n&\\tau \\sim \\text{HalfNorm}(5) \\\\[1em]\n&\\theta_1 \\sim \\text{Norm}(\\theta, \\tau) \\\\[1em]\n&\\theta_2 \\sim \\text{Norm}(\\theta_1, \\tau) \\\\[1em]\n&\\theta_3 \\sim \\text{Norm}(\\theta_2, \\tau) \\\\[1em]\n&\\sigma \\sim \\text{HalfNorm}(5) \\\\[1em]\n&y \\sim \\text{Norm}(\\theta_3, \\sigma).\n\\end{align}\\]\nHere, we have defined the measurements as \\(y\\), and we have implied that the appropriate conditioning on which specific hyperparameters (as given by index 1, index 2, and index 3 defined above) is considered.\nThis method of organization is useful to keep conditioning straight in hierarchical models. As we will see momentarily, it is also convenient for implementing a hierarchical model in Stan.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of hierarchical models</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical_implementation.html#hierarchical-model-structure",
    "href": "lessons/hierarchical/hierarchical_implementation.html#hierarchical-model-structure",
    "title": "45  Implementation of hierarchical models",
    "section": "",
    "text": "hierarchical model\n\n\n\n\nLevel 0: This level has the hyperparameters \\(\\theta\\) and \\(\\sigma\\), the location and scale parameters of fluorescence intensity typical of an E. coli cell expressing the fluorescent gene of interest. These are the parameters we ultimately wish to get estimates for.\nLevel 1 corresponds to the day the experiment was performed. There will be variability from day to day, and the location and scale parameters for a given day are conditioned on the hyperparameters, but can vary from the parameters of other days.\nLevel 2 corresponds to which batch of cells were used on a given day.\nLevel 3 corresponds to the colony of cells chosen from the batch.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of hierarchical models</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical_implementation.html#coding-up-the-hierarchical-model-in-stan",
    "href": "lessons/hierarchical/hierarchical_implementation.html#coding-up-the-hierarchical-model-in-stan",
    "title": "45  Implementation of hierarchical models",
    "section": "45.2 Coding up the hierarchical model in Stan",
    "text": "45.2 Coding up the hierarchical model in Stan\nProvided we can define arrays for index 1, index 2, and index 3, coding the model up in Stan is surprisingly straightforward. To start, we will use a centered parametrization for clarity. (We will see the problems this causes with sampling momentarily.) Here is a Stan code for the hierarchical model.\ndata {\n  // Total number of data points\n  int N;\n  \n  // Number of entries in each level of the hierarchy\n  int J_1;\n  int J_2;\n  int J_3;\n  \n  //Index arrays to keep track of hierarchical structure\n  array[J_2] int index_1;\n  array[J_3] int index_2;\n  array[N] int index_3;\n  \n  // The measurements\n  array[N] real y;\n}\n\n\nparameters {\n  // Hyperparameters level 0\n  real theta;\n\n  // How hyperparameters vary\n  real&lt;lower=0&gt; tau;\n\n  // Hyperparameters level 1\n  vector[J_1] theta_1;\n\n  // Hyperparameters level 2\n  vector[J_2] theta_2;\n\n  // Parameters\n  vector[J_3] theta_3;\n  real&lt;lower=0&gt; sigma;\n}\n\n\nmodel {\n  theta ~ normal(10, 3);\n  sigma ~ normal(0, 5);\n  tau ~ normal(0, 5);\n\n  theta_1 ~ normal(theta, tau);\n  theta_2 ~ normal(theta_1[index_1], tau); \n  theta_3 ~ normal(theta_2[index_2], tau);\n\n  y ~ normal(theta_3[index_3], sigma);\n}\nBe sure to carefully read the Stan code. Importantly, note that theta_1,theta_2, and theta_3 are vector valued. Note also that we use indexing to specify which parameters correspond to which day, batch, and colony. In studying the code and the model above, you should be able to see how the hierarchical model is built in Stan.\n\n45.2.1 A quick aside: generating a data set\nI will now generate a tidy data frame containing a data set to analyze with this hierarchical model. These are fabricated data, hence the weird loading with a string (for compactness so we don’t have a big code cell).\n\ndata_str = \"\".join(\n    [\n        \"day,batch,colony,y\\nm,1,1,11.40\\nm,1,1,10.54\\n\",\n        \"m,1,1,12.17\\nm,1,1,12.41\\nm,1,1,9.97\\nm,1,2,10.76\\n\",\n        \"m,1,2,9.16\\nm,1,3,9.50\\nm,2,1,9.34\\nm,2,1,10.14\\n\",\n        \"m,2,2,10.72\\nm,2,2,10.63\\nm,3,1,11.37\\nm,3,1,10.51\\n\",\n        \"m,4,1,11.06\\nm,4,1,10.68\\nm,4,1,12.58\\nm,4,2,11.21\\n\",\n        \"m,4,2,11.07\\nm,4,2,10.74\\nm,4,2,11.68\\nm,4,3,10.65\\n\",\n        \"m,4,3,9.06\\nw,1,1,10.40\\nw,1,2,10.75\\nw,1,2,11.42\\n\",\n        \"w,1,2,10.42\\nw,1,2,9.18\\nw,1,2,10.69\\nw,1,2,9.37\\n\",\n        \"w,1,2,11.32\\nw,2,1,9.90\\nw,2,1,10.53\\nw,2,1,10.76\\n\",\n        \"w,3,1,11.08\\nw,3,1,9.27\\nw,3,1,12.01\\nw,3,1,12.20\\n\",\n        \"w,3,1,11.23\\nw,3,1,10.96\\nr,1,1,9.73\\nr,1,2,11.25\\n\",\n        \"r,1,2,9.99\\nr,1,2,10.12\\nr,1,3,9.65\\nr,1,3,10.18\\nr,1,4,12.70\\n\",\n    ]\n)\n\ndata_str = (\n    data_str.replace(\"m\", \"monday\").replace(\"w\", \"wednesday\").replace(\"r\", \"thursday\")\n)\ndf = pl.read_csv(io.StringIO(data_str))\n\n# Take a look\ndf.head()\n\n\nshape: (5, 4)\n\n\n\nday\nbatch\ncolony\ny\n\n\nstr\ni64\ni64\nf64\n\n\n\n\n\"monday\"\n1\n1\n11.4\n\n\n\"monday\"\n1\n1\n10.54\n\n\n\"monday\"\n1\n1\n12.17\n\n\n\"monday\"\n1\n1\n12.41\n\n\n\"monday\"\n1\n1\n9.97\n\n\n\n\n\n\nThe data are tidy with each row corresponding to a measurement with the appropriate metadata present to determine which day, batch, and colony the measurement was from. Let’s take a quick graphical look at the data set. We will color each data point by the colony number and group by day and batch.\n\nbokeh.io.show(\n    iqplot.strip(\n        df,\n        q=\"y\",\n        cats=[\"day\", \"batch\"],\n        color_column=\"colony\",\n        marker_kwargs=dict(alpha=0.6),\n    )\n)\n\n\n  \n\n\n\n\n\n\n\n45.2.2 Generating input data for Stan\nWe should now convert the data frame into a data dictionary that Stan likes, adhering to our more general hierarchical model specification above, i.e., replacing 'day' with 'index_1', 'batch' with 'index_2' and 'colony' with 'index_3'. Importantly, we should update the original data frame to include these indices (which will be included in Stan’s output) that match the respective categorical parameters in the original data set. The function bebi103.stan.df_to_datadict_hier() does this. You need to specify the tidy data frame you want to convey, the columns of the data frame corresponding to the levels of the hierarchy in order of the hierarchy, and the column(s) that have the measured data.\n\ndata, df = bebi103.stan.df_to_datadict_hier(\n    df, level_cols=[\"day\", \"batch\", \"colony\"], data_cols=\"y\"\n)\n\n# Take a look at the data dictionary\ndata\n\n{'N': 47,\n 'J_1': 3,\n 'J_2': 8,\n 'J_3': 17,\n 'index_1': array([1, 1, 1, 1, 2, 3, 3, 3]),\n 'index_2': array([1, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 7, 8]),\n 'index_3': array([ 1,  1,  1,  1,  1,  2,  2,  3,  4,  4,  5,  5,  6,  6,  7,  7,  7,\n         8,  8,  8,  8,  9,  9, 10, 11, 11, 11, 12, 12, 13, 14, 15, 15, 15,\n        15, 15, 15, 15, 16, 16, 16, 17, 17, 17, 17, 17, 17]),\n 'y': array([11.4 , 10.54, 12.17, 12.41,  9.97, 10.76,  9.16,  9.5 ,  9.34,\n        10.14, 10.72, 10.63, 11.37, 10.51, 11.06, 10.68, 12.58, 11.21,\n        11.07, 10.74, 11.68, 10.65,  9.06,  9.73, 11.25,  9.99, 10.12,\n         9.65, 10.18, 12.7 , 10.4 , 10.75, 11.42, 10.42,  9.18, 10.69,\n         9.37, 11.32,  9.9 , 10.53, 10.76, 11.08,  9.27, 12.01, 12.2 ,\n        11.23, 10.96])}\n\n\nIt is also instructive to look at the entire updated data frame so you can understand how the labeling of indices works.\n\ndf\n\n\nshape: (47, 7)\n\n\n\nday\nbatch\ncolony\ny\nday_stan\nbatch_stan\ncolony_stan\n\n\nstr\ni64\ni64\nf64\ni64\ni64\ni64\n\n\n\n\n\"monday\"\n1\n1\n11.4\n1\n1\n1\n\n\n\"monday\"\n1\n1\n10.54\n1\n1\n1\n\n\n\"monday\"\n1\n1\n12.17\n1\n1\n1\n\n\n\"monday\"\n1\n1\n12.41\n1\n1\n1\n\n\n\"monday\"\n1\n1\n9.97\n1\n1\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"wednesday\"\n3\n1\n9.27\n3\n8\n17\n\n\n\"wednesday\"\n3\n1\n12.01\n3\n8\n17\n\n\n\"wednesday\"\n3\n1\n12.2\n3\n8\n17\n\n\n\"wednesday\"\n3\n1\n11.23\n3\n8\n17\n\n\n\"wednesday\"\n3\n1\n10.96\n3\n8\n17\n\n\n\n\n\n\nWe now have the necessary data for Stan’s sampler.\n\n\n45.2.3 Drawing samples and checking diagnostics\nSo, let’s sample and see what we get!\n\nwith bebi103.stan.disable_logging():\n    sm_centered = cmdstanpy.CmdStanModel(stan_file='centered.stan')\n    samples_centered = sm_centered.sample(data=data, seed=3252)\n    samples_centered = az.from_cmdstanpy(posterior=samples_centered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nThat was fast! Hopefully everything worked ok for this model. Let’s start by checking all of the dianostics to see if there were any issues.\n\nbebi103.stan.check_all_diagnostics(samples_centered)\n\nESS for parameter tau is 120.6206946627826.\ntail-ESS for parameter tau is 193.46855809117585.\n  ESS or tail-ESS below 100 per chain indicates that expectation values\n  computed from samples are unlikely to be good approximations of the\n  true expectation values.\n\nRhat for parameter tau is 1.0181667967555237.\nRhat for parameter theta_3[3] is 1.0104461055496787.\n  Rank-normalized Rhat above 1.01 indicates that the chains very likely have not mixed.\n\n2 of 4000 (0.05%) iterations ended with a divergence.\n  Try running with larger adapt_delta to remove divergences.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nChain 0: E-BFMI = 0.17398128059170156\nChain 1: E-BFMI = 0.09851564271989993\nChain 2: E-BFMI = 0.12373822952024313\nChain 3: E-BFMI = 0.07573497349280298\n  E-BFMI below 0.3 indicates you may need to reparametrize your model.\n\n\n23\n\n\nWhew! Lots of problems here. Most strikingly, the effective sample size for \\(\\tau\\) is very small. Furthermore, there are a few divergences. Not many, and they could be false positives, but we should check to see if there is a pattern to the divergences. To start looking for the pattern, let’s make a parallel coordinate plot.\n\nparameters = (\n    [\"theta\", \"sigma\", \"tau\"]\n    + [f\"theta_1[{i}]\" for i in samples_centered.posterior.theta_1_dim_0.values]\n    + [f\"theta_2[{i}]\" for i in samples_centered.posterior.theta_2_dim_0.values]\n    + [f\"theta_3[{i}]\" for i in samples_centered.posterior.theta_3_dim_0.values]\n)\n\nbokeh.io.show(\n    bebi103.viz.parcoord(\n        samples_centered,\n        parameters=parameters,\n        xtick_label_orientation=\"vertical\",\n        transformation=\"minmax\",\n    )\n)\n\n\n  \n\n\n\n\n\nEven though there are very few divergences, they all go through low \\(\\tau\\). This is a symptom of hitting a funnel. We can see this in the corner plot.\n\nbokeh.io.show(\n    bebi103.viz.corner(samples_centered, parameters=[\"theta\", \"sigma\", \"tau\"])\n)\n\n\n  \n\n\n\n\n\nLooking at the tau versus theta plot, we see that the divergences are for small tau. Zooming in on the bottom of that plot shows that the sampler is hitting the entry to a funnel; it cannot sample down to tau values close to zero. We can possibly alleviate this problem by uncentering the model.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of hierarchical models</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical_implementation.html#a-noncentered-parametrization",
    "href": "lessons/hierarchical/hierarchical_implementation.html#a-noncentered-parametrization",
    "title": "45  Implementation of hierarchical models",
    "section": "45.3 A noncentered parametrization",
    "text": "45.3 A noncentered parametrization\nTo uncenter the model, we reparametrize our model as follows.\n\\[\\begin{align}\n&\\theta \\sim \\text{Norm}(10, 3) \\\\[1em]\n&\\tau \\sim \\text{HalfNorm}(5) \\\\[1em]\n&\\tilde{\\theta}_1 \\sim \\text{Norm}(0, 1) \\\\[1em]\n&\\theta_1 = \\theta + \\tau \\tilde{\\theta}_1 \\\\[1em]\n&\\tilde{\\theta}_2 \\sim \\text{Norm}(0, 1) \\\\[1em]\n&\\theta_2 = \\theta_1 + \\tau \\tilde{\\theta}_2 \\\\[1em]\n&\\tilde{\\theta}_3 \\sim \\text{Norm}(0, 1) \\\\[1em]\n&\\theta_3 = \\theta_2 + \\tau \\tilde{\\theta}_3 \\\\[1em]\n&\\sigma \\sim \\text{HalfNorm}(5) \\\\[1em]\n&y \\sim \\text{Norm}(\\theta_3, \\sigma).\n\\end{align}\\]\nThis frees the sampler to explore standard Normal distributions and then uses a transformation to sample the funnel. The models is called “noncentered” because the sampling is done around a value of zero, not the center of the \\(\\theta\\) distributions. We can then recenter these noncentered samples by applying a transformation such as \\(\\theta_1 = \\theta + \\tau \\tilde{\\theta}_1\\), where \\(\\tilde{\\theta}_1\\) are the noncentered samples.\nLet’s code this up in Stan. The Stan code is:\ndata {\n  // Total number of data points\n  int N;\n  \n  // Number of entries in each level of the hierarchy\n  int J_1;\n  int J_2;\n  int J_3;\n  \n  //Index arrays to keep track of hierarchical structure\n  array[J_2] int index_1;\n  array[J_3] int index_2;\n  array[N] int index_3;\n  \n  // The measurements\n  array[N] real y;\n}\n\n\nparameters {\n  // Hyperparameters level 0\n  real theta;\n\n  // How hyperparameters vary\n  real&lt;lower=0&gt; tau;\n\n  // Hyperparameters level 1\n  vector[J_1] theta_1_tilde;\n\n  // Hyperparameters level 2\n  vector[J_2] theta_2_tilde;\n\n  // Parameters\n  vector[J_3] theta_3_tilde;\n  real&lt;lower=0&gt; sigma;\n}\n\n\ntransformed parameters {\n  // Transformations from noncentered\n  vector[J_1] theta_1 = theta + tau * theta_1_tilde;\n  vector[J_2] theta_2 = theta_1[index_1] + tau * theta_2_tilde;\n  vector[J_3] theta_3 = theta_2[index_2] + tau * theta_3_tilde;\n}\n\n\nmodel {\n  theta ~ normal(10, 3);\n  sigma ~ normal(0, 5);\n  tau ~ normal(0, 5);\n\n  theta_1_tilde ~ normal(0, 1);\n  theta_2_tilde ~ normal(0, 1);\n  theta_3_tilde ~ normal(0, 1);\n\n  y ~ normal(theta_3[index_3], sigma);\n}\nLet’s draw our samples and see if we were able to alleviate the problems we diagnosed with the centered parametrizations.\n\nwith bebi103.stan.disable_logging():\n    sm_noncentered = cmdstanpy.CmdStanModel(stan_file='noncentered.stan')\n    samples_noncentered = sm_noncentered.sample(data=data, seed=3252)\n    samples_noncentered = az.from_cmdstanpy(posterior=samples_noncentered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nLet’s first check the diagnostics.\n\nbebi103.stan.check_all_diagnostics(samples_noncentered)\n\nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n22 of 4000 (0.55%) iterations ended with a divergence.\n  Try running with larger adapt_delta to remove divergences.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 10.\n\nE-BFMI indicated no pathological behavior.\n\n\n4\n\n\nThe problems with effective sample size and Rhat are gone, but we still have divergences. Let’s again take a look at where the divergences are coming from with a parallel coordinate plot.\n\nbokeh.io.show(\n    bebi103.viz.parcoord(\n        samples_noncentered,\n        parameters=parameters,\n        xtick_label_orientation=\"vertical\",\n        transformation=\"minmax\",\n    )\n)\n\n\n  \n\n\n\n\n\nIt is hard to see an immediate pattern in the divergences here. Let’s look at the corner plot to see if anything is obvious.\n\nbokeh.io.show(\n    bebi103.viz.corner(samples_noncentered, parameters=[\"theta\", \"sigma\", \"tau\"])\n)\n\n\n  \n\n\n\n\n\nThere does not seem to be any apparent pattern in the divergences, which means that they could be false positives. We can deal with them by increasing the adapt_delta parameter of the sampler, which I won’t do here (you can try it yourself; it works). Importantly, zooming in on the tau versus theta plot shows that the sampler is now properly sampling small values of \\(\\tau\\), effectively sampling down toward zero. We can see this more clearly by comparing the samples from the centered versus noncentered parametrizations with \\(\\tau\\) on a logarithmic scale.\n\np = bokeh.plotting.figure(\n    height=400, width=450, x_axis_label=\"θ\", y_axis_label=\"τ\", y_axis_type=\"log\"\n)\np.scatter(\n    samples_centered.posterior.theta.values.flatten(),\n    samples_centered.posterior.tau.values.flatten(),\n    legend_label=\"centered\",\n    alpha=0.2,\n)\np.scatter(\n    samples_noncentered.posterior.theta.values.flatten(),\n    samples_noncentered.posterior.tau.values.flatten(),\n    legend_label=\"noncentered\",\n    color=\"orange\",\n    alpha=0.2,\n)\np.legend.location = \"bottom_right\"\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe have much better sampling for low \\(\\tau\\). Let’s think for a moment about why this is important. A large value of \\(\\tau\\) means that the parameters governing the outcomes of the respective experiments are essentially independent. A small value of \\(\\tau\\) means that the parameters governing the experiments are all nearly the same. Thus, small \\(\\tau\\) corresponds to pooling all of the results together in a single data set. So, you can think of \\(\\tau\\) like a slider; small \\(\\tau\\) gives the extreme model where the experiments are all pooled together, and large \\(\\tau\\) gives the opposite extreme, where the experiments are all independent. The whole point of using a hierarchical model is to capture all of the possible behavior between (and including) these two extremes. If we cannot effectively sample \\(\\tau\\), we are not capturing the possible structure of the data set.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of hierarchical models</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical_implementation.html#conclusions",
    "href": "lessons/hierarchical/hierarchical_implementation.html#conclusions",
    "title": "45  Implementation of hierarchical models",
    "section": "45.4 Conclusions",
    "text": "45.4 Conclusions\nHierarchical modeling presents special challenges for samplers. There are tricks we can do to improve the performance of HMC samplers in exploring the parameter space, such as using noncentered parametrizations and encouraging the sampler to take smaller steps by tuning the adapt_delta parameter. Importantly, though, you should carefully check the diagnostics of your sampler. This is true even if you are not working with a hierarchical model.\n\nbebi103.stan.clean_cmdstan()\n\n\n%load_ext watermark\n%watermark -v -p numpy,polars,cmdstanpy,arviz,bokeh,iqplot,bebi103,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.30.0\ncmdstanpy : 1.2.5\narviz     : 0.21.0\nbokeh     : 3.6.2\niqplot    : 0.3.7\nbebi103   : 0.1.27\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of hierarchical models</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/generalization.html",
    "href": "lessons/hierarchical/generalization.html",
    "title": "46  Generalization of hierarchical models",
    "section": "",
    "text": "| Download notebook\n\nThe worm reversal problem is easily generalized. You can imagine having more levels of the hierarchy. This is just more steps in the chain of dependencies that are factored in the prior. For general parameters \\(\\theta\\) and hyperparameters \\(\\phi\\), we have, for data set \\(y\\),\n\\[\\begin{align}\ng(\\theta, \\phi \\mid y) = \\frac{f(y\\mid \\theta)\\, g(\\theta \\mid \\phi)\\,g(\\phi)}{f(y)}\n\\end{align}\\]\nfor a two-level hierarchical model. For a three-level hierarchical model, we can consider hyperparameters \\(\\xi\\) that condition \\(\\phi\\) which in turn condition \\(\\theta\\), giving\n\\[\\begin{align}\ng(\\theta, \\phi, \\xi \\mid y) = \\frac{f(y\\mid \\theta)\\, g(\\theta \\mid \\phi)\\,g(\\phi\\mid \\xi)\\,g(\\xi)}{f(y)},\n\\end{align}\\]\nand so on for four, five, etc., level hierarchical models. As we have seen in the course, the work is all in coming up with the models for the likelihood \\(f(y\\mid \\theta)\\), and prior, \\(g(\\theta \\mid \\phi)\\,g(\\phi)\\), in the case of a two-level hierarchical model. For coming up with the conditional portion of the prior, \\(g(\\theta \\mid \\phi)\\), we often assume a Gaussian distribution because this often describes experiment-to-experiment variability. (The Beta distribution we used in the worm reversal example is approximately Gaussian and has the convenient feature that it is defined on the interval \\([0,1]\\).) Bayes’s theorem gives you the posterior, and it is then “just” a matter of computing it by sampling from it. In coming lessons, we will use Stan to sample out of hierarchical models and discuss the difficulties involved with doing that.",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Generalization of hierarchical models</span>"
    ]
  },
  {
    "objectID": "lessons/hierarchical/hierarchical_implementation_worms.html",
    "href": "lessons/hierarchical/hierarchical_implementation_worms.html",
    "title": "47  Implementation of a hierarchical model",
    "section": "",
    "text": "47.1 Computing environment\n| Download notebook\nWe will do MCMC on the hierarchical posterior for worm reversal data. As a reminder, the model is\n\\[\\begin{align}\n&\\phi \\sim \\text{Beta}(1.1, 1.1), \\\\[1em]\n&\\kappa \\sim \\text{HalfNorm}(0, 1000), \\\\[1em]\n&\\alpha = \\phi \\kappa, \\\\[1em]\n&\\beta = (1-\\phi)\\kappa,\\\\[1em]\n&\\theta_i \\sim \\text{Beta}(\\alpha, \\beta) \\;\\;\\forall i,\\\\[1em]\n&n_i \\sim \\text{Binom}(N_i, \\theta_i)\\;\\;\\forall i.\n\\end{align}\\]\nTo demonstrate how the hierarchical model works, we will consider the data sets from 2015, 2016, and 2017, and an additional three synthetic experiments that have 14/40, 5/34, and 110/660 reversals, respectively. These three experiments were not actually performed; I am using them here to demonstrate some effects we see in hierarchical models. In particular, the last experiment has many more trials and would dominate pooled data if we were not using a hierarchical model.\nWe will not perform prior predictive checks or other diagnostics, but simply demonstrate how a hierarchical model is built using Stan and investigate some of the resulting inferences.\nNext, as usual, we define our model using Stan.\nWe can compile it and draw samples.\nNow, let’s look a corner plot to see how we did with the sampling.\nWe can see the results more clearly if we plot the marginalized distributions on top of each other. This also allows us to see how the respective experiments serve to determine \\(\\phi\\), the global reversal probability. I will also plot the most probable value of \\(\\theta_i\\) in the independent experiment model (i.e., all experiments are independent) as a diamond. We also plot the most probable reversal probability of the pooled model (all experiments pooled together) as a black diamond, again with a Uniform prior.\nWe see that the individual parameter values tend to “shrink” toward the hyperparameter value. The hyperparameter value, in turn, is different than if we pooled all the data together. Notably, the black diamond, which represents the MAP estimate of the reversal probability if we pooled all of the data together, is significantly different than under the hierarchical model. This is because the data set with many more measurements overwhelms the other data sets if we pool the results.\nWe are probably most interested in the hyperparameter \\(\\phi\\), so let’s compute its median and 80% credible interval.\n%load_ext watermark\n%watermark -v -p numpy,cmdstanpy,arviz,iqplot,bebi103,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\ncmdstanpy : 1.2.5\narviz     : 0.21.0\niqplot    : 0.3.7\nbebi103   : 0.1.27\nbokeh     : 3.6.2\njupyterlab: 4.3.7",
    "crumbs": [
      "Hierarchical models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of a hierarchical model</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/pca_and_related.html",
    "href": "lessons/pca_and_related/pca_and_related.html",
    "title": "Principal component analysis and related models",
    "section": "",
    "text": "Neuroscience abounds with models involving latent parameters. A latent parameter is a parameter or random variable that cannot be observed. They are central to both encoding and decoding problems, which are at the heart of neuroscience.\nAs a reminder, an encoding problem addresses how stimuli or other inputs produce a neuronal response. That is, in an encoding problem, we seek to infer neuronal activity given an observed stimulus or behavior. By contrast, in a decoding problem, we seek to infer what stimulus resulted in an observed neuronal activity or what behavior results from an observed neuronal activity. In both cases, there are latent variables that link stimuli and behaviors to and from neuronal activity.\nAs a simple example, we can imagine observing, e.g., by calcium imaging, activity of many neurons in the brain. There may only be a few simple “causes” of this observed behavior.1 In the following lessons, we will see that factor analysis and the associated models of principal component analysis and probabilistic principal component analysis provide a set of formal models in which lower-dimensional latent variables may be inferred from high-dimensional observational data. For this reason, these models are sometimes referred to as models for dimensionality reduction.",
    "crumbs": [
      "Principal component analysis and related models"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/pca_and_related.html#footnotes",
    "href": "lessons/pca_and_related/pca_and_related.html#footnotes",
    "title": "Principal component analysis and related models",
    "section": "",
    "text": "I am being intentional with the use of the word “cause” here. Neuroscientists ascribe the word cause somewhat liberally to mean parameters on which observed data are conditioned. See, e.g., Chapter 10 of Dayan and Abbott. This is different from more restrictive definitions of causes in the context of causal inference; see Judea Pearl’s book.↩︎",
    "crumbs": [
      "Principal component analysis and related models"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html",
    "href": "lessons/pca_and_related/heuristic_pca.html",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "",
    "text": "48.1 Principal component analysis: A heuristic approach\n| Download notebook\nIn our discussion of principal component analysis, we will first approach the problem heuristically without defining a full generative model. We will then demonstrate a Bayesian model behind this heuristic.",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html#principal-component-analysis-a-heuristic-approach",
    "href": "lessons/pca_and_related/heuristic_pca.html#principal-component-analysis-a-heuristic-approach",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "",
    "text": "48.1.1 The problem of PCA\nWe often obtain high-dimensional data sets, in which each observation has many dimensions. In the parlance of practitioners of machine learning, each multidimensional observation is called a sample and each dimension is called a feature. We will use the observation/dimension and sample/feature nomenclature interchangeably. Considering the Palmer’s penguin data set, each observation (sample) is a penguin, and the dimensions (features) are bill depth, bill length, flipper length, and body mass.\nThe features, though many, may be related. We can imagine that there are unobservable variables, called latent variables at play the influence many features. For example, consider the four features of the penguin. They could all scale with the caloric intake of the penguins, in which case the latent variable of caloric intake could use used to predict all four of the features; it is responsible for most of the variation in what is observed. (We have seen latent variables before when we \\(z_{ik}\\) in Gaussian mixture models we encountered in Chapter 40.)\nLet us now formalize this. Let \\(\\mathbf{y}\\in \\mathbb{R}^D\\) be a set of observable features. There are \\(D\\) of them. In the case of the penguins, \\(D = 4\\). Let \\(\\mathbf{z} \\in \\mathbb{R}^L\\) be the set of latent variables that influence the observations. Typically, \\(L &lt; D\\) and often \\(L \\ll D\\), and, as such, if we can infer the latent variable \\(\\mathbf{z}\\), we say we have achieved dimensionality reduction. Specifically, we define a model for \\(\\mathbf{y}\\), \\(\\mathbf{y} = h(\\mathbf{z};\\theta)\\), where \\(\\theta\\) is some set of parameters. So, to achieve dimensionality reduction, we want to find a function \\(h(\\mathbf{z};\\theta)\\) that most faithfully reproduces the observed \\(\\mathbf{y}\\).\n\n\n48.1.2 Linear \\(h(\\mathbf{z};\\theta)\\)\nAs is commonly done, we will restrict the functions \\(h(\\mathbf{z};\\theta)\\) we choose to be linear functions. Further, we stipulate that \\(h(\\mathbf{z};\\theta)\\) serves to orthogonally project \\(\\mathbf{z}\\) onto \\(\\mathbf{y}\\).\n\\[\\begin{align}\nh(\\mathbf{z};\\mathsf{W}) = \\mathsf{W}\\cdot \\mathbf{z},\n\\end{align}\n\\]\nwhere the parameters \\(\\theta\\) are given by the \\(D \\times L\\) matrix \\(\\mathsf{W}\\), commonly referred to as a loading matrix. This matrix is semiorthogonal amd unitary such that the columns are orthonormal vectors and \\(\\mathsf{W}^\\mathsf{T}\\cdot \\mathsf{W} = \\mathsf{I}\\), the identity matrix. If \\(\\mathbf{w}_j\\) is a column of \\(\\mathsf{W}\\), then\n\\[\\begin{align}\n\\mathbf{w}_j^\\mathsf{T}\\cdot \\mathbf{w}_k = \\delta_{jk},\n\\end{align}\n\\]\nwhere \\(\\delta_{jk}\\) is the Kronecker delta, which is equal to one when \\(j = k\\) and zero otherwise.\nNow, let \\(\\hat{\\mathbf{y}}\\in \\mathbb{R}^D\\) be the a data point we get by applying \\(h(\\mathbf{z};\\mathsf{W})\\),\n\\[\\begin{align}\n\\hat{\\mathbf{y}} = h(\\mathbf{z}; \\mathsf{W}) = \\mathsf{W} \\cdot \\mathbf{z}.\n\\end{align}\n\\tag{48.1}\\]\nWe seek a loading matrix \\(\\mathsf{W}\\) and set of latent variables \\(\\{\\mathbf{z}\\}\\) that most faithfully reproduce the measured data \\(\\{\\mathbf{y}\\}\\), where I am using the braces to denote a set of data; that is \\(\\{\\mathbf{y}\\} = \\{\\mathbf{y}_1, \\mathbf{y}_2, \\ldots \\mathbf{y}_N\\}\\), where \\(N\\) is the number of observations we have made. That is, we want \\(\\mathbf{y}_i\\) to be as close as possible to \\(\\hat{\\mathbf{y}}_i\\) for all \\(i\\). Toward this end, we define a loss function as the mean square distance between \\(\\mathbf{y}_i\\) and \\(\\hat{\\mathbf{y}}_i\\).\n\\[\\begin{align}\n\\text{loss}(\\mathsf{W}, \\{\\mathbf{z}\\}) = \\frac{1}{N}\\sum_{i=1}^N \\lVert \\mathbf{y}_i -\n\\hat{\\mathbf{y}}_i\\rVert_2^2 = \\frac{1}{N}\\sum_{i=1}^N \\lVert \\mathbf{y}_i -\n\\mathsf{W}\\cdot \\mathbf{z}_i\\rVert_2^2,\n\\end{align}\n\\tag{48.2}\\]\nThe goal of principal component analysis is to find the \\(\\mathsf{W}\\) that minimizes this loss function. Once this optimal \\(\\mathsf{W}\\) is found, we compute \\(\\hat{\\mathbf{z}} = \\mathsf{W}^\\mathsf{T}\\cdot \\mathbf{y}\\); the entries of \\(\\hat{\\mathbf{z}}\\) are called the principal components.\n\n\n48.1.3 The optimal projection\nIn this section, we work out what loading matrix \\(\\mathsf{W}\\) optimizes the above loss function. Be careful not to get lost in the weeds of the mathematics that follows. Just remember that we are simply finding the projection that minimizes the mean squared difference between measured and projected data.\nAs we work to find the optimal projection, it will help us to write the loss function in terms of the columns of \\(\\mathsf{W}\\).\n\\[\\begin{align}\n\\text{loss}(\\mathsf{W}, \\{\\mathbf{z}\\}) &= \\frac{1}{N}\\sum_{i=1}^N \\left\\lVert \\mathbf{y}_i - \\sum_{j=1}^L z_{ij}\\mathbf{w}_j\\right\\rVert_2^2 \\\\[1em]\n&= \\frac{1}{N}\\sum_{i=1}^N\\left(\\mathbf{y}_i - \\sum_{j=1}^L z_{ij}\\mathbf{w}_j\\right)^\\mathsf{T}\\cdot\\left(\\mathbf{y}_i - \\sum_{j=1}^L z_{ij}\\mathbf{w}_j\\right) \\\\[1em]\n&= \\frac{1}{N}\\sum_{i=1}^N\\left(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - 2\\sum_{j=1}^L z_{ij}\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j + \\sum_{j=1}^L\\sum_{k=1}^L z_{ij}z_{ik}\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_k\\right).\n\\end{align}\\]\nWe wish to find the \\(\\{\\mathbf{w}_j\\}\\) and \\(\\{\\mathbf{z}_j\\}\\) that minimize the loss function. To do so while enforcing the orthonormality of \\(\\mathsf{W}\\), we define a Lagrangian\n\\[\\begin{align}\n\\mathcal{L} &= \\text{loss}(\\mathsf{W}, \\{\\mathbf{z}\\}) + \\sum_{j=1}^L \\lambda_j(1 - \\mathbf{w}_j^\\mathsf{T}\\cdot \\mathbf{w}_j) + \\sum_{j=1}^L\\sum_{k=j+1}^L \\mu_{jk}\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_k \\\\[1em]\n&= \\frac{1}{N}\\sum_{i=1}^N\\left(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - 2\\sum_{j=1}^L z_{ij}\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j + \\sum_{j=1}^L\\sum_{k=1}^L z_{ij}z_{ik}\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_k\\right) \\\\[1em]\n&\\;\\;\\;\\;+ \\sum_{j=1}^L \\lambda_j(\\mathbf{w}_j^\\mathsf{T}\\cdot \\mathbf{w}_j - 1) + \\sum_{j=1}^L\\sum_{k=j+1}^L \\mu_{jk}\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_k.\n\\end{align}\\]\nwhere the Lagrange multipliers denoted with \\(\\lambda\\) enforce the normality of the columns of \\(\\mathsf{W}\\) and those denoted with \\(\\mu\\) enforce the orthogonality of the columns of \\(\\mathsf{W}\\).\nBecause the loss function and the normality constraints are all strictly convex, the necessary and sufficient conditions for minimizing the loss function are the Karush-Kuhn-Tucker (KKT) conditions,\n\\[\\begin{align}\n&\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_j} = \\mathbf{0}, \\\\[1em]\n&\\frac{\\partial \\mathcal{L}}{\\partial z_{ij}} = 0, \\\\[1em]\n&\\frac{\\partial \\mathcal{L}}{\\partial\\lambda_j} = 0, \\\\[1em]\n&\\frac{\\partial \\mathcal{L}}{\\partial\\mu_{jk}} = 0.\n\\end{align}\\]\nfor all \\(i\\in[1,N]\\), \\(j\\in[1,L]\\), and \\(k \\in [j+1,L]\\). To avoid stumbling over indices that are present in sums, I prefer to differentiate with respect to a variable with a specific index, e.g., with respect to \\(z_{lm}\\). With that in mind, let us compute the derivatives of the KKT conditions one-by-one, in reverse order as they are listed above.\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial\\mu_{lm}} = \\mu_{lm}\\mathbf{w}_l^\\mathsf{T}\\cdot\\mathbf{w}_m = 0,\n\\end{align}\\]\nnoting that \\(m &gt; l\\). Not surprisingly, this means that \\(\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_k = 0\\) for all \\(j\\ne k\\); the columns of \\(\\mathsf{W}\\) are orthogonal.\nTurning now to the derivative with respect to the \\(\\lambda\\)’s,\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial\\lambda_l} = \\mathbf{w}_l^\\mathsf{T}\\cdot \\mathbf{w}_l - 1 = 0,\n\\end{align}\\]\nwhich means that \\(\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_j = 1\\). With the previous KKT condition, this means that the columns of \\(\\mathsf{W}\\) are orthonormal, or\n\\[\\begin{align}\n\\mathbf{w}_j^\\mathsf{T}\\cdot \\mathbf{w}_k = \\delta_{jk}.\n\\end{align}\\]\nNow considering the derivative with respect to \\(z_{lm}\\).\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial z_{lm}} &= \\frac{1}{N}\\left(-2\\mathbf{y}_l^\\mathsf{T}\\cdot\\mathbf{w}_m + 2\\sum_{k=1}^Lz_{lm}\\mathbf{w}_m^T\\cdot \\mathbf{w}_k\\right)\\\\[1em]\n&= -\\frac{2}{N}\\left(\\mathbf{y}_l^\\mathsf{T}\\cdot\\mathbf{w}_m - z_{lm}\\right) = 0,\n\\end{align}\\]\nwhere we have used \\(\\mathbf{w}_m^\\mathsf{T}\\cdot \\mathbf{w}_k = \\delta_{mk}\\), which we worked out from the first two KKT conditions we considered. Thus, we have,\n\\[\\begin{align}\nz_{ij} = \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j = \\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{y}_i.\n\\end{align}\\]\nWe have satisfied three of the four KKT conditions. We can substitute these results into the loss function because doing so does not change the feasible set of minimizers. The updated Lagrangian is then\n\\[\\begin{align}\n\\text{loss}(\\mathsf{W}) =\n&= \\frac{1}{N}\\sum_{i=1}^N\\left(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - 2\\sum_{j=1}^L (\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j)\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j + \\sum_{j=1}^L(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j)^2\\right) \\\\[1em]\n&= \\frac{1}{N}\\sum_{i=1}^N\\left(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j)^2\\right) \\\\[1em]\n&= \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\left[\\frac{1}{N}\\sum_{i=1}^N(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j)^2\\right] \\\\[1em]\n&= \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j,\n\\end{align}\\]\nwhere we have recognized the bracketed term as a quadratic form with the \\(D\\times D\\) matrix \\(\\hat{\\mathsf{\\Sigma}}\\) having entries given by\n\\[\\begin{align}\n\\hat{\\Sigma}_{jk} = \\frac{1}{N}\\sum_{i=1}^N y_{ij}\\, y_{ik}.\n\\end{align}\\]\nWe will clarify the suggestive symbol we chose for this matrix momentarily. The updated Lagrangian is then\n\\[\\begin{align}\n\\mathcal{L} &= \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j \\\\[1em]\n&\\;\\;\\;\\;+ \\sum_{j=1}^L \\lambda_j(\\mathbf{w}_j^\\mathsf{T}\\cdot \\mathbf{w}_j - 1) - \\sum_{j=1}^L\\sum_{k=j+1}^L \\mu_{jk}\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_k.\n\\end{align}\\]\nNow, differentiating with respect to \\(\\mathbf{w}_l\\) and setting the result to zero to satisfy our final KKT condition, we have\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_l} = -2\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_l + 2\\lambda_l\\,\\mathbf{w}_l + \\sum_{k=1}^{l-1} \\mu_{kl}\\mathbf{w}_k + \\sum_{k=l+1}^{L} \\mu_{lk}\\mathbf{w}_k = \\mathbf{0}.\n\\end{align}\\]\nDotting both sides of this equation with \\(\\mathbf{w}_l^\\mathsf{T}\\) yields\n\\[\\begin{align}\n-2\\mathbf{w}_l^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_l + 2\\lambda_j\\,\\mathbf{w}_l^\\mathsf{T}\\cdot\\mathbf{w}_j + \\sum_{k=1}^{l-1} \\mu_{kj}\\mathbf{w}_l^\\mathsf{T}\\cdot\\mathbf{w}_k + \\sum_{k=l+1}^{L} \\mu_{lk}\\mathbf{w}_l^\\mathsf{T}\\cdot\\mathbf{w}_k = 0.\n\\end{align}\\]\nUsing\n\\[\\begin{align}\n\\mathbf{w}_l^\\mathsf{T}\\cdot \\mathbf{w}_k = 0,\n\\end{align}\\]\nfor \\(l \\ne k\\), this reduces to\n\\[\\begin{align}\n-2\\mathbf{w}_l^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_l + 2\\lambda_l\\mathbf{w}_l^\\mathsf{T}\\cdot\\mathbf{w}_l = 0,\n\\end{align}\\]\nwhich we can rewrite as\n\\[\\begin{align}\n\\frac{\\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j}{\\mathbf{w}_j^\\mathsf{T}\\cdot\\mathbf{w}_j} = \\lambda_j.\n\\end{align}\\]\nWe recognize this equation as an expression for a Rayleigh quotient. Evidently, then, the Lagrange multipliers \\(\\lambda_j\\) are the eigenvalues of \\(\\hat{\\mathsf{\\Sigma}}\\) and \\(\\mathbf{w}_j\\) are the normalized eigenvectors. Inserting this expression for \\(\\mathbf{w}_j\\) into the loss function yields\n\\[\\begin{align}\n\\text{loss}(\\mathsf{W}) = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\mathbf{w}_j^\\mathsf{T}\\cdot(\\lambda_j\\mathbf{w}_j)\n= \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\lambda_j.\n\\end{align}\\]\nThe loss function is minimized if we choose the \\(L\\) largest eigenvalues of \\(\\hat{\\mathsf{\\Sigma}}\\). Therefore, the columns of \\(\\mathsf{W}\\) are comprised of the eigenvectors of \\(\\hat{\\mathsf{\\Sigma}}\\) corresponding to its largest eigenvalues.\n\n\n48.1.4 Centering and scaling, the interpretation of \\(\\hat{\\mathsf{\\Sigma}}\\), and maximal variance\nAssume for a moment that the mean of \\(\\mathbf{y}\\) is zero,\n\\[\\begin{align}\n\\bar{\\mathbf{y}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i = \\mathbf{0},\n\\end{align}\n\\]\nand the variance of each of the \\(D\\) entries in \\(\\mathbf{y}\\) is one,\n\\[\\begin{align}\ns_j = \\frac{1}{N}\\sum_{i=1}^N (y_{i,j} - \\bar{y}_j)^2 = \\frac{1}{N}\\sum_{i=1}^N y_{i,j}^2 = 1\\;\\forall j\\in[1, D].\n\\end{align}\n\\]\nIn that case, the entries of \\(\\hat{\\mathsf{\\Sigma}}\\) are\n\\[\\begin{align}\n\\hat{\\Sigma}_{jk} = \\frac{1}{N}\\sum_{i=1}^N y_{ij}\\, y_{ik} = \\frac{1}{N}\\sum_{i=1}^N (y_{ij} - \\bar{y_j})(y_{ik} - \\bar{y_k}),\n\\end{align}\n\\tag{48.3}\\]\nwhich we recognize as the elements of the empirical covariance matrix (or, equivalently, since the diagonal entries are all one, an empirical correlation matrix). Thus, \\(\\hat{\\mathsf{\\Sigma}}\\) is the plug-in estimate for the covariance matrix when using centered-and-scaled data.\nConsider a principal component \\(z_{ij}\\). The plug-in estimate for the variance of this is\n\\[\\begin{align}\n\\widehat{\\mathbb{V}(z_{ij})} = \\frac{1}{N}\\sum_{i=1}^N z_{ij}^2 - \\left(\\frac{1}{N}\\sum_{i=1}^Nz_{ij}\\right)^2.\n\\end{align}\n\\]\nLet us compute the first moment.\n\\[\\begin{align}\n\\frac{1}{N}\\sum_{i=1}^Nz_{ij} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{w}_{j}^\\mathsf{T} \\cdot \\mathbf{y}_i = \\mathbf{w}_{j}^\\mathsf{T}\\cdot\\left(\\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i\\right) = 0,\n\\end{align}\\]\nwhere we have recognized the average in parentheses to be zero since the data are centered. Therefore, the plug-in estimate for the variance is\n\\[\\begin{align}\n\\widehat{\\mathbb{V}(z_{ij})} = \\frac{1}{N}\\sum_{i=1}^N z_{ij}^2 = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{N}\\sum_{i=1}^N(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j)^2.\n\\end{align}\\]\nWe have seen this expression before. This is a quaratic form involving the covariance matrix \\(\\hat{\\mathsf{\\Sigma}}\\).\n\\[\\begin{align}\n\\widehat{\\mathbb{V}(z_{ij})} = \\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j.\n\\end{align}\\]\nRecall that the loss function is\n\\[\\begin{align}\n\\text{loss}(\\mathsf{W}) = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j\n= \\text{constant} - \\widehat{\\mathbb{V}(z_{ij})}.\n\\end{align}\\]\nTherefore, finding the transformation \\(\\mathbf{w}_j\\) that minimizes the loss function is the same as finding the transformation that maximizes the variance of the projection. This is why the principal components are often referred to as the directions that have maximal variance.\nAs a final note, I mention that it is important to center and scale the data set before performing PCA because given directions may have high variance just because of the scaling of the measurement. This is generally the variance we do not wish to interpret.\n\n\n48.1.5 Prescription for PCA\nFollowing the above analysis, we arrive at a prescription for PCA. After choosing the number of principal components \\(L\\), do the following.\n\nCenter and scale the data set by subtracting the mean and dividing by the standard deviation along each dimension.\nCompute the empirical covariance matrix.\nCompute the first \\(L\\) eigenvalues and eigenvectors.\nConstruct the loading matrix \\(\\mathsf{W}\\) from the eigenvectors corresponding to the \\(L\\) largest eigenvalues.\nThe principal components are \\(\\mathbf{z}_i = \\mathbf{W}^\\mathsf{T} \\cdot \\mathbf{y}_i \\;\\forall i\\).\n\n\n\n48.1.6 Nonidentifiability of PCA\nThe loading matrix \\(\\mathsf{W}\\) is comprised of the eigenvectors of the empirical covariance matrix and therefore the columns are orthogonal, since the covariance matrix is symmetric. However, any of these eigenvectors can be multiplied by a scalar (which is true in general for an eigenvector; if \\(\\mathbf{v}\\) is an eigenvector of \\(\\mathsf{A}\\), then so is \\(\\alpha \\mathbf{v}\\) for real \\(\\alpha \\ne 0\\).) We can insist that the columns of \\(\\mathsf{W}\\) for an orthonormal basis, such that the magnitude of each eigenvector is 1, which is what is commonly done. However, any given column of \\(\\mathsf{W}\\) may be multiplied by \\(-1\\), still resulting in a nonidentifiability. We therefore cannot uniquely find a loading matrix \\(\\mathsf{W}\\) and therefore a unique set of principal components.",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html#sec-pca-remedios",
    "href": "lessons/pca_and_related/heuristic_pca.html#sec-pca-remedios",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "48.2 PCA of a data set",
    "text": "48.2 PCA of a data set\nWe will perform PCA on data from Remedios, et al. that we visited in ?sec-matlab-files. Recall that in that data set, we had recording from 115 neurons over 10 or so minutes. Let’s load in the data set and convert everything to Numpy arrays as we did in ?sec-matlab-files.\n\n# Load in the data set and separate into Numpy arrays\ndata = scipy.io.loadmat(os.path.join(data_path, 'hypothalamus_calcium_imaging_remedios_et_al.mat'))\nneural_data = data['neural_data']     # Row is time, column is neuron\nattack_vector = data['attack_vector'].flatten()\nsex_vector = data['sex_vector'].flatten()\n\nWe also have time points, which we can set up knowing that the sampling rate is 30 Hz.\n\n# Time points at 30 Hz sampling\nt = np.arange(len(attack_vector)) / 30\n\nFinally, we can center and scale the data. We will also transpose the neural data so that it has dimension \\(N\\times D\\) for ease in comparing to the above theoretical results.\n\n# Transpose\ny = neural_data.T\n\n# Center data set by subtracting the mean from each time point and dividing by the standard deviation\ny = (y - np.mean(y, axis=0)) / np.std(y, axis=0)\n\nWe can now carry out the prescription of PCA. We are hoping to visualize the trajectory of the neuronal activity of the mouse on a 2D plot, so we will specify that there are \\(L = 2\\) latent variables (principal componenents).\n\n# Two principle components\nL = 2\n\n# Compute the plugin estimate for the covariance matrix\ncov = np.cov(y.T)\n\n# Compute the eigensystem\neigenvalues, eigenvectors = np.linalg.eig(cov)\n\n# Sort highest to lowest\nidx = eigenvalues.argsort()[::-1]\n\n# Extract largest L to build eigenvalues and loading matrix\nlam = eigenvalues[idx][:L]\nW = eigenvectors[:, idx][:, :L]\n\n# Compute principal components\nz = np.dot(W.T, y.T)\n\nWe will plot the visualization of the principal components in a moment, but for now, we will take the opportunity to compute the fraction of the variance that each principal component contributes. Given that the eigenvalues are the eigenvalues of the covariance matrix, each eigenvalue gives the scale of the variance along the given eigenvector. Thus, the fraction of the total variance that runs along principal component \\(j\\) is \\(\\lambda_j / \\sum_{j=1}^L \\lambda_j\\).\n\nlam / eigenvalues.sum()\n\narray([0.22153642, 0.15257567])\n\n\nEvidently, the first two principal components account for nearly 40% of the total variance.\nNow, we will proceed to plot the principal components. We will plot them against each other and color the glyphs by time or by sex of the presented mouse. We will also plot each of the two principal components vs time.\n\np1 = bokeh.plotting.figure(\n    frame_height=350,\n    frame_width=350,\n    x_axis_label=\"PC 1\",\n    y_axis_label=\"PC 2\",\n)\n\np2 = bokeh.plotting.figure(\n    frame_height=150,\n    frame_width=550,\n    x_axis_label=\"time\",\n    y_axis_label=\"PC\",\n)\n\n# Set up date for the plot\ntime_color = bebi103.viz.q_to_color(t, bokeh.palettes.Viridis256)\nsex_color = [\"orchid\" if s else \"dodgerblue\" for s in sex_vector]\ncds = bokeh.models.ColumnDataSource(dict(pc1=z[0], pc2=z[1], t=t, color=time_color))\n\n# We'll allow selection of which color we want to visualize\ncolor_selector = bokeh.models.RadioButtonGroup(labels=[\"time\", \"sex\"], active=0)\njs_code = \"\"\"\ncds.data['color'] = color_selector.active == 0 ? time_color : sex_color;\ncds.change.emit();\n\"\"\"\n\ncolor_selector.js_on_event(\n    \"button_click\",\n    bokeh.models.CustomJS(\n        code=js_code,\n        args=dict(\n            time_color=time_color,\n            sex_color=sex_color,\n            cds=cds,\n            color_selector=color_selector,\n        ),\n    ),\n)\n\n# Populate glyphs\np1.scatter(source=cds, x=\"pc1\", y=\"pc2\", color=\"color\")\np2.line(source=cds, x='t', y='pc1', legend_label='PC 1')\np2.line(source=cds, x='t', y='pc2', color=\"orange\", legend_label='PC 2')\n\n# Lay out and show!\nbokeh.io.show(\n    bokeh.layouts.column(\n        bokeh.layouts.row(p1, bokeh.layouts.column(bokeh.models.Div(text=\"Color by\"), color_selector)),\n        p2\n    )\n)\n\n\n  \n\n\n\n\n\nWhen looking at the coloring by sex, it is clear that there is a difference along the PC 1-axis depending on what sex of mouse is presented. When the male mouse is introduced about 250 seconds into the experiment, PC 1 shifts to exceed PC 2.\n\n48.2.1 Reconstruction of neuronal measurements from principal components\nBy restricting ourselves to only two latent variables, we necessarily have omitted some of the dynamics observed in the system. To assess what we have lost, we can reconstruct the neuronal signal from the estimates of the latent variables and the corresponding loading matrix \\(\\mathsf{W}\\). Recall from Equation 48.1 that the reconstructed data set is\n\\[\\begin{align}\n\\hat{\\mathbf{y}}_i = h(\\mathbf{z}_i; \\mathsf{W}) = \\mathsf{W} \\cdot \\mathbf{z}_i\\;\\forall i.\n\\end{align}\n\\]\nWe can directly compute this and compare to the measured signal.\n\n# Compute the reconstruction\ny_hat = np.dot(W, z)\n\n# Uncenter and unscale the reconstruction\ny_hat = np.std(neural_data, axis=0) * y_hat + np.mean(neural_data, axis=0)\n\n# Kwargs for plotting\nkwargs = dict(\n    x_interpixel_distance=1/30,\n    y_interpixel_distance=1,\n    frame_height=150,\n    frame_width=600,\n    x_axis_label=\"time (s)\",\n    y_axis_label=\"neuron\",\n)\n\n# Plot original data\np_neuron = bebi103.image.imshow(neural_data, **kwargs)\n\n# Plot reconstructed data\np_hat = bebi103.image.imshow(y_hat, **kwargs)\n\np_neuron.yaxis.major_label_text_font_size = '0pt'\np_hat.yaxis.major_label_text_font_size = '0pt'\n\nbokeh.io.show(bokeh.layouts.column(p_neuron, p_hat))\n\n\n  \n\n\n\n\n\nQualitatively, we see that the reconstruction recapitulates the key features of the measured data.",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html#principal-component-analysis-using-singular-value-decomposition",
    "href": "lessons/pca_and_related/heuristic_pca.html#principal-component-analysis-using-singular-value-decomposition",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "48.3 Principal component analysis using singular value decomposition",
    "text": "48.3 Principal component analysis using singular value decomposition\nPerforming PCA by directly computing eigenvalues is instructive. It gives clear, direct interpretation of what the estimates latent variables mean. Here, we show that PCA may be equivalently computed using singular value decomposition. We will not go into the details here, but will instead state a few theorems from linear algebra and comment that using SVD can result in faster calculations that directly computing the eigenvalues. First, the theorems.\n\nAny real \\(m\\times n\\) matrix \\(\\mathsf{A}\\) may be factored as \\(\\mathsf{A} = \\mathsf{U}\\cdot\\mathsf{S}\\cdot\\mathsf{V}^\\mathsf{T}\\), where \\(\\mathsf{U}\\) is a real \\(m\\times m\\) orthogonal unitary matrix (such that the columns of \\(\\mathsf{U}\\) for orthogonal vectors and \\(\\mathsf{U}^{-1} = \\mathsf{U}^\\mathsf{T}\\)), \\(\\mathsf{S}\\) is an \\(m\\times n\\) diagonal matrix with nonnegative real entries, and \\(\\mathsf{V}\\) is an \\(n\\times n\\) real unitary orthogonal matrix. This factorization is called a singular value decomposition. The diagonal entries of \\(\\mathsf{S}\\) are referred to as the singular values of \\(\\mathsf{A}\\).\nGiven a singular value decomposition, the columns of \\(\\mathsf{U}\\) are the eigenvectors of \\(\\mathsf{A}\\cdot \\mathsf{A}^\\mathsf{T}\\), the diagonal entries of \\(\\mathsf{S}\\) are the eigenvalues of \\(\\mathsf{A}^\\mathsf{T}\\cdot \\mathsf{A}\\) (which are equal to the eigenvalues of \\(\\mathsf{A}\\cdot \\mathsf{A}^\\mathsf{T}\\)), and the columns of \\(\\mathsf{V}\\) are the eigenvectors of \\(\\mathsf{A}^\\mathsf{T} \\cdot \\mathsf{A}\\).\nAny \\(n\\times n\\) matrix \\(\\mathsf{B}\\) with \\(n\\) linearly independent eigenvectors may be factored as \\(\\mathsf{Q}\\cdot\\mathsf{\\Lambda}\\cdot\\mathsf{Q}^{-1}\\), where \\(\\mathsf{Q}\\) is \\(n\\times n\\) and its columns are the eigenvectors of \\(\\mathsf{B}\\) and \\(\\mathsf{\\Lambda}\\) is a diagonal matrix whose diagonal entries are the eigenvalues of \\(\\mathsf{B}\\). Such a factorization is called an eigendecomposition.\nIf \\(\\mathsf{B}\\) is real symmetric positive definite, then \\(\\mathsf{Q}\\) is a unitary matrix, and \\(\\mathbf{B} = \\mathsf{Q}\\cdot\\mathsf{\\Lambda}\\cdot\\mathsf{Q}^\\mathsf{T}\\).\n\nNow, consider \\(\\hat{\\mathsf{\\Sigma}}\\), the plug-in estimate for the covariance matrix. We wrote it element by element in Equation 48.3, but we can write it in a convenient matrix form by defining a \\(N\\times D\\) matrix \\(\\mathsf{Y}\\) where row \\(i\\) is \\(\\mathbf{y}_i\\). Then, given that the data are centered, \\(\\hat{\\mathsf{\\Sigma}} = \\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y}/N\\).\n\\[\\begin{align}\n\\hat{\\mathsf{\\Sigma}} = \\frac{1}{N}\\,\\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y} = \\frac{1}{N}\\mathsf{Q}\\cdot \\mathsf{\\Lambda}\\cdot\\mathsf{Q}^\\mathsf{T},\n\\end{align}\n\\]\nwhere we have used the fact that \\(\\mathsf{Q}\\) is unitary since \\(\\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y}\\) is positive definite. We have already worked out that the loading matrix \\(\\mathsf{W}\\) is given by the first \\(L\\) eigenvectors of \\(\\hat{\\Sigma}\\), so that \\(\\mathsf{Q} = \\mathsf{W}\\).\nNow, consider the singular value decomposition of \\(\\mathsf{Y}\\),\n\\[\\begin{align}\n\\mathsf{Y} = \\mathsf{U}\\cdot \\mathsf{S}\\cdot\\mathsf{V}^\\mathsf{T}.\n\\end{align}\n\\]\nThe matrix \\(\\mathsf{V}\\) is comprised of the eigenvectors of \\(\\mathsf{Y}^\\mathsf{T} \\cdot \\mathsf{Y}\\). But this is also \\(\\mathsf{W}\\). So, we can use SVD computing eigenvalues/vectors to find \\(\\mathsf{W}\\).\nComputing the eigenvectors of an \\(D\\times D\\) matrix (which is what \\(\\hat{\\mathsf{\\Sigma}}\\) is) is an \\(\\mathcal{O}(D^3)\\) calculation. The calculation of \\(\\hat{\\mathsf{\\Sigma}} = \\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y}\\) is itself an \\(\\mathcal{O}(ND^2)\\) calculation, making the computational complexity of performing PCA by eigendecomposition as \\(\\mathcal{O}(ND^2)\\) + \\(\\mathcal{D^3}\\). Computing the SVD for an \\(N\\times D\\) matrix, which we have to do in this case (the SVD of \\(\\mathsf{Y}\\)), is an \\(\\mathcal{O}(ND^2) + \\mathcal{O}(D^3)\\) calculation, which is the same complexity of the eigendecomposition method. However, if we only need to compute the first \\(L\\) eigenvectors, the randomized SVD algorithm developed by Caltech’s own Joel Tropp can do the calculation in \\(\\mathcal{O}(NL^2) + \\mathcal{O}(L^3)\\) time, which is a big improvement if \\(L \\ll D\\) as it often is, and indeed is in this case.",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html#sec-pca-sklean",
    "href": "lessons/pca_and_related/heuristic_pca.html#sec-pca-sklean",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "48.4 Principal component analysis using scikit-learn",
    "text": "48.4 Principal component analysis using scikit-learn\nScikit-learn implements PCA using the aforementioned randomized SVD algorithm. A PCA instance is instantiated using sklearn.decomposition.PCA(), with \\(L\\), the number of components, provided.\n\n# Instantiate a PCA object\npca = sklearn.decomposition.PCA(n_components=2)\n\nAs usual with the scikit-learn API, we call the fit() method to perform PCA. The input data is expected to be \\(N\\times D\\), as we have defined for \\(\\mathsf{Y}\\) above and as we have stored in the variable \\(y\\).\n\n# Perform the fit\npca.fit(y)\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: divide by zero encountered in matmul\n  C = X.T @ X\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: overflow encountered in matmul\n  C = X.T @ X\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: invalid value encountered in matmul\n  C = X.T @ X\n\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA(n_components=2) \n\n\nWe got a couple of warnings encountered during some of the matrix calculations, but the PCA fit was accomplished. We can extract the columns of the loading matrix \\(\\mathsf{W}\\) using the components_ attribute. Here, I will show that we get the same result as we did for the eigenvalue-based calculation we did above, which should be the case potentially up to a multiplicative constant of \\(-1\\).\n\nnp.isclose(np.abs(pca.components_ / W.T), 1).all()\n\nnp.True_\n\n\nSuccess!\nWe have already seen that we can recover the loading matrix \\(\\mathsf{W}\\) using pca.components_. To recover other pertinent results, we can use the following attributes/methods.\n\nExplained variance, \\(\\lambda_j/\\sum_{k=1}^L\\lambda_k\\): explained_var = pca.explained_variance_ratio_.\nProjection onto principal components, \\(\\mathbf{z}\\): z = pca.transform(y).\nReconstructed data, \\(\\hat{\\mathbf{y}}\\): y_hat = pca.inverse_transform(z).\n\nAs with the analysis we did by hand above, we have to center and scale (or uncenter and unscale if we are computing reconstructed data).",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html#sec-center-scale-sklearn",
    "href": "lessons/pca_and_related/heuristic_pca.html#sec-center-scale-sklearn",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "48.5 Centering and scaling with scikit-learn",
    "text": "48.5 Centering and scaling with scikit-learn\nAs a final note, I’ll mention that scikit-learn offers automated centering and scaling. A StandardScaler instance is instantiated and “fit” based on the data set.\n\nscaler = sklearn.preprocessing.StandardScaler().fit(neural_data)\n\nWith the scaler in place, the data may be scaled and unscaled.\n\ny = scaler.transform(neural_data)\noriginal_data = scaler.inverse_transform(y)\n\nWe can automate the PCA process to get reconstructed data as follows. (We will silence warnings just for aesthetic purposes of these notes. In general, it is a bad idea to silence warnings in your workflow unless you really know what you are doing.)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    scaler = sklearn.preprocessing.StandardScaler().fit(neural_data)\n    y = scaler.transform(neural_data)\n    pca = sklearn.decomposition.PCA(n_components=2).fit(y)\n    z = pca.transform(y)\n\n    reconstructed_neural_data = scaler.inverse_transform(pca.inverse_transform(z))",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/heuristic_pca.html#computing-environment",
    "href": "lessons/pca_and_related/heuristic_pca.html#computing-environment",
    "title": "48  Principal component analysis: A heuristic approach",
    "section": "48.6 Computing environment",
    "text": "48.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,sklearn,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nsklearn   : 1.6.1\nbokeh     : 3.6.2\nbebi103   : 0.1.27\njupyterlab: 4.4.3",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Principal component analysis: A heuristic approach</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/factor_analysis.html",
    "href": "lessons/pca_and_related/factor_analysis.html",
    "title": "49  Factor analysis",
    "section": "",
    "text": "49.1 Formulation of factor analysis\n| Download notebook\nIn Chapter 48, we took a heuristic approach to principal component analysis in which we did the following.\nTo be clear, this transformation is optimal only in the sense of the loss function we defined.\nWe now approach this problem with more careful probabilistic modeling.\nIn factor analysis, we have a similar goal as in PCA in that we define a model \\(\\mathbf{y} = h(\\mathbf{z};\\theta)\\), where \\(\\theta\\) is some set of parameters, and we want to infer a function \\(h(\\mathbf{z};\\theta)\\). As in PCA, we restrict \\(h(\\mathbf{z};\\theta)\\) to be linear function defined by a \\(D\\times L\\) loading matrix \\(\\mathsf{W}\\) such that\n\\[\\begin{align}\nh(\\mathbf{z};\\mathsf{W}) = \\mathsf{W}\\cdot \\mathbf{z} + \\boldsymbol{\\mu}.\n\\end{align}\n\\]\nHere we have included an additive term \\(\\boldsymbol{\\mu}\\) that we had set to zero in our treatment of PCA.",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/factor_analysis.html#formulation-of-factor-analysis",
    "href": "lessons/pca_and_related/factor_analysis.html#formulation-of-factor-analysis",
    "title": "49  Factor analysis",
    "section": "",
    "text": "49.1.1 Generative distribution of factor analysis\nIn a factor analysis, in addition to choosing a linear function \\(h(\\mathbf{z};\\mathsf{W})\\), we assume an i.i.d. Normal likelihood,\n\\[\\begin{align}\n\\mathbf{y}_i \\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi} \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi})\\;\\forall i,\n\\end{align}\n\\]\nwhere \\(\\mathsf{\\Psi}\\) is a \\(D\\times D\\) covariance matrix describing how the observed data vary from the linear model. We typically take \\(\\mathsf{\\Psi}\\) to be diagonal; the residuals of the observations are uncorrelated (though the observations are correlated through the latent variables \\(\\mathbf{z}\\)). We therefore will define the vector \\(\\boldsymbol{\\sigma}\\) to be the diagonal elements of \\(\\mathsf{\\Psi}\\) such that \\(\\Psi = \\text{diag}(\\boldsymbol{\\sigma}^2)\\), where \\(\\boldsymbol{\\sigma}^2\\) denotes element-wise squaring of the elements of \\(\\boldsymbol{\\sigma}\\).\n\nWe need to specify a prior on the latent variables \\(\\mathbf{z}\\). In factor analysis, we take the latent variables to be \\(L\\)-variate Normally distributed.   \\[\\begin{align}\n\\mathbf{z}_i \\mid \\boldsymbol{\\mu}_0, \\mathsf{\\Sigma}_0 \\sim \\text{Norm}(\\boldsymbol{\\mu}_0, \\mathsf{\\Sigma}_0)\\;\\forall i.\n\\end{align}\\]\nNote that since we are using a linear model and \\(\\mathbf{z}_i\\) gets transformed according to \\(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}\\), we can absorb \\(\\boldsymbol{\\mu}_0\\) and \\(\\mathsf{\\Sigma}_0\\) into \\(\\mathsf{W}\\) and \\(\\boldsymbol{\\mu}\\). Absorbing these as described, we can write\n\\[\\begin{align}\n\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i,\n\\end{align}\n\\]\nwhere \\(\\mathsf{I}\\) is the \\(L\\times L\\) identity matrix. The full generative model for a factor analysis is then\n\\[\\begin{align}\n&\\boldsymbol{\\mu} \\sim \\text{prior for }\\boldsymbol{\\mu}, \\\\[1em]\n&\\mathsf{W} \\sim \\text{prior for }\\mathsf{W}, \\\\[1em]\n&\\boldsymbol{\\sigma} \\sim \\text{prior for }\\boldsymbol{\\sigma} \\\\[1em]\n&\\mathsf{\\Psi} = \\text{diag}(\\boldsymbol{\\sigma}^2) \\\\[1em]\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid\\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi} \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi})\\; \\forall i\n\\end{align}\n\\]\nFor clarity and for reference, the dimensionality of the variables are shown in the table below. The index \\(i\\) indexes data points, and there are \\(N\\) of them.\n\n\n\n\n\n\n\n\nVariable\nDescription\nDimension\n\n\n\n\n\\(\\boldsymbol{\\mu}\\)\nlocation parameter of measurements\n\\(D\\)-vector\n\n\n\\(\\mathsf{W}\\)\nloading matrix\n\\(D \\times L\\) matrix\n\n\n\\(\\boldsymbol{\\sigma}\\)\nstandard deviation for each measurement\npositive \\(D\\)-vector\n\n\n\\(\\mathsf{\\Psi}\\)\nmatrix representation of \\(\\boldsymbol{\\sigma}^2\\)\n\\(D \\times D\\) nonnegative diagonal matrix\n\n\n$_i $\nLatent variable for datum \\(i\\)\n\\(L\\)-vector\n\n\n$_i $\nDatum \\(i\\)\n\\(D\\)-vector\n\n\n\nObviously, we need priors for \\(\\mathsf{W}\\), \\(\\boldsymbol{\\mu}\\), and \\(\\mathsf{\\Psi}\\). We will temporarily leave those priors unspecified and turn our attention to marginal distributions and then to identifiability of the parameters of a factor analysis.\n\n\n49.1.2 Marginal likelihood in factor analysis\nWe note that we can write a marginalized likelihood for datum \\(i\\) as\n\\[\\begin{align}\nf(\\mathbf{y}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi}) &= \\int\\mathrm{d}\\mathbf{z}_i\\,g(\\mathbf{y}_i, \\mathbf{z}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi})\n= \\int\\mathrm{d}\\mathbf{z}_i\\,f(\\mathbf{y}_i\\mid  \\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi})\\,g(\\mathbf{z}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi})\n=\\int\\mathrm{d}\\mathbf{z}_i\\,f(\\mathbf{y}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi})\\,g(\\mathbf{z}_i),\n\\end{align}\n\\]\nwhere we have used the fact that the latent variables \\(\\mathbf{z}_i\\) are not conditioned on the parameters \\(\\boldsymbol{\\mu}\\), \\(\\mathsf{W}\\), and \\(\\mathsf{\\Psi}\\). Computing the integral, we get a marginalized likelihood of\n\\[\\begin{align}\nf(\\mathbf{y}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi}) = \\int\\mathrm{d}\\mathbf{z}_i\\,\\mathcal{N}(\\mathbf{y}_i\\mid \\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi})\\,\\mathcal{N}(\\mathbf{z}_i\\mid\\mathbf{0}, \\mathsf{I})\n= \\mathcal{N}(\\mathbf{y}_i\\mid \\boldsymbol{\\mu},\\mathsf{W}\\cdot\\mathsf{W}^\\mathsf{T} + \\mathsf{\\Psi}),\n\\end{align}\n\\]\nwhere we have used properties of Gaussian integrals to compute the integral, such that\n\\[\\begin{align}\n\\mathbf{y}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi}\\sim \\text{Norm}(\\boldsymbol{\\mu},\\mathsf{W}\\cdot\\mathsf{W}^\\mathsf{T} + \\mathsf{\\Psi}).\n\\end{align}\n\\tag{49.1}\\]\nThis marginalized likelihood makes clear what factor analysis does: It expresses the observed data as generated by a low-dimensional Gaussian distribution with mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\mathsf{W}\\cdot\\mathsf{W}^\\mathsf{T} + \\mathsf{\\Psi}\\).\n\n\n49.1.3 Recognition distribution in factor analysis\nThe recognition distribution is the distribution of latent variables conditioned on the observed data, \\(g(\\mathbf{z}_i\\mid \\mathbf{y}_i, \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi})\\). Bayes’s theorem gives\n\\[\\begin{align}\ng(\\mathbf{z}_i \\mid \\mathbf{y}_i, \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi}) = \\frac{f(\\mathbf{y}_i\\mid  \\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi})\\,g(\\mathbf{z}_i)}{f(\\mathbf{y}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi})}.\n\\end{align}\n\\]\nBecause the denominator has no \\(\\mathbf{z}_i\\)-dependence, it is a normalization constant. The recognition distribution is then a product of two Normal distributions, since\n\\[\\begin{align}\n&\\mathbf{y}_i \\mid\\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi} \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi}),\\\\[1em]\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I}).\n\\end{align}\n\\]\nAfter some algebraic grunge, we can work out that\n\\[\\begin{align}\n\\mathbf{z}_i\\mid \\mathbf{y}_i, \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi} \\sim \\text{Norm}(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i}, \\mathsf{\\Sigma}_{\\mathbf{z}}),\n\\end{align}\n\\]\nwhere\n\\[\\begin{align}\n\\mathsf{\\Sigma}_{\\mathbf{z}} = (\\mathsf{I} + \\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{\\Psi}^{-1}\\cdot\\mathsf{W})^{-1}\n\\end{align}\n\\tag{49.2}\\]\nand\n\\[\\begin{align}\n\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i} = \\mathsf{\\Sigma}_{\\mathbf{z}} \\cdot\\mathsf{W}^\\mathsf{T}\\cdot \\mathsf{\\Psi}^{-1}\\cdot(\\mathbf{y_i - \\boldsymbol{\\mu}}).\n\\end{align}\n\\tag{49.3}\\]\nWe note that evaluating the recognition distribution is part of the E step of an EM algorithm for finding the MAP of a factor model, as described in Section 49.2.\n\n\n49.1.4 Nonidentifiability of the factor loading matrix and latent variables\nIn general, the factor loading matrix \\(\\mathsf{W}\\) and latent variable \\(\\mathbf{z}\\) are not identifiable. This can be seen by considering an arbitrary \\(L\\times L\\) orthonormal matrix \\(\\mathsf{Q}\\) (meaning that the rows and vectors of \\(\\mathsf{Q}\\) are orthonormal vectors such that \\(\\mathsf{Q}\\cdot\\mathsf{Q}^\\mathsf{T} = \\mathsf{I}\\)). We define a new loading matrix and latent variable vector according to\n\\[\\begin{align}\n&\\mathsf{W}_\\mathrm{new} = \\mathsf{W} \\cdot \\mathsf{Q}^\\mathsf{T}, \\\\[1em]\n&\\mathbf{z}_\\mathrm{new} = \\mathsf{Q}\\cdot\\mathbf{z}.\n\\end{align}\\]\nThen,\n\\[\\begin{align}\n\\mathsf{W}_\\mathrm{new}\\cdot \\mathbf{z}_\\mathrm{new} = \\mathsf{W} \\cdot \\mathsf{Q}^\\mathsf{T}\\cdot\\mathsf{Q}\\cdot\\mathbf{z}\n= \\mathsf{W}\\cdot\\mathbf{z}.\n\\end{align}\\]\nThus, two different loading matrices and vector of latent variables gives exactly the same likelihood upon multiplication by an orthonormal matrix.\nThis presents a real problem! The variables we are most interested in, the latent variables and the factor loading matrix, are not in general identifiable. We will discuss methods to deal with this nonidentifiability momentarily, but for now we will proceed without it giving us too much concern. Chris Bishop gives a nice motivation for this approach in his book Pattern Recognition and Machine Learning.\n\nWe shall view factor analysis as a form of latent variable density model, in which the form of the latent space is of interest but not the particular choice of coordinates used to describe it.\n\n\n\n49.1.5 Specifying priors\nWe have left priors for \\(\\boldsymbol{\\mu}\\), \\(\\mathsf{W}\\), and \\(\\boldsymbol{\\Psi}\\) unspecified. The rotational nonidentifiability makes prior assignment difficult, as we would like to ascribe a prior that breaks the nonidentifiability. There are several approaches to this. One is to take the same approach we did with PCA (see Chapter 48), and insist in our prior that the columns of \\(\\mathsf{W}\\) are othronormal. This breaks much of the nonidentifiability because in this case, \\(\\mathsf{W}\\cdot \\mathsf{Q}^\\mathsf{T}\\) is itself orthonormal, as can be shown by noting that\n\\[\\begin{align}\n\\left(\\mathsf{W}\\cdot \\mathsf{Q}^\\mathsf{T}\\right)^\\mathsf{T} \\cdot \\left(\\mathsf{W}\\cdot \\mathsf{Q}^\\mathsf{T}\\right) = \\mathsf{Q} \\cdot \\mathsf{W}^\\mathsf{T} \\cdot \\mathsf{W}\\cdot \\mathsf{Q}^\\mathsf{T} = \\mathsf{Q} \\cdot \\mathsf{Q}^\\mathsf{T} = \\mathsf{I}.\n\\end{align}\\]\nHowever, any given column of \\(\\mathsf{W}\\) can still be arbitrarily multiplied by \\(-1\\), so \\(\\mathsf{W}\\) and therefore also the latent variables \\(\\mathbf{z}\\), are not identifiable (see also Section 48.1.6).\nEven if we wanted to formulate a prior to insist that the columns of \\(\\mathsf{W}\\) are orthonormal, this is difficult to implement. An easier, but fraught, approach is to force \\(\\mathsf{W}\\) to be lower triangular. Forcing this upon the load matrix can lead to constraints on the resulting latent variables \\(\\mathbf{z}\\), clouding their interpretation.\nIn our analysis, we will punt on this and take the egregious step of leaving the priors unspecified and follow Bishop’s philosophy we are using the factor analysis to expose the form of the latent space and not concern ourselves with the particularities of coordinate choices. We will also forego evaluating the full posterior using MCMC because of the awful nonidentifiabilities and will proceed to find a MAP (using an indefinite article because there are many MAPs!).\n(Rick Farouni has written a couple of nice blog posts about the nonidentifiability challenges of factor analysis, available here and here.)",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/factor_analysis.html#sec-em-factor-analysis",
    "href": "lessons/pca_and_related/factor_analysis.html#sec-em-factor-analysis",
    "title": "49  Factor analysis",
    "section": "49.2 EM for factor analysis",
    "text": "49.2 EM for factor analysis\nAs a latent variable model, factor analysis is well-suited for the EM algorithm. We can take the usual approach for deriving the EM algorithm by evaluating the recognition distribution, defining the parameter-dependent part of the surrogate function, and then finding the parameter values that maximize it. It turns out that the MAP estimate for \\(\\boldsymbol{\\mu}\\) is the sample mean, so that can be computed directly outside of the EM algorithm.\n\\[\\begin{align}\n\\boldsymbol{\\mu}_\\mathrm{MAP} = \\bar{\\mathbf{y}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i.\n\\end{align}\n\\]\nIn what follows, recall that \\(\\Psi = \\text{diag}(\\boldsymbol{\\sigma}^2)\\). We also define\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\sigma}}^2 = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i \\odot \\mathbf{y}_i\n\\end{align}\n\\]\nto be the plug-in estimate for \\(\\boldsymbol{\\sigma}^2\\), where the symbol \\(\\odot\\) denotes element-wise multiplication.\n\nInitialize \\(\\mathsf{W}\\) and \\(\\boldsymbol{\\sigma}\\).\nCompute \\(\\mathsf{\\Sigma}_\\mathbf{z} = (\\mathsf{I} + \\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{\\Psi}^{-1}\\cdot \\mathsf{W})^{-1}\\). (E step)\nCompute the \\(\\mathbf{E}_i = \\mathsf{\\Sigma}_\\mathbf{z}\\cdot\\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{\\Psi}^{-1}\\cdot(\\mathbf{y}_i - \\bar{\\mathbf{y}})\\) for all \\(i\\). (E step)\nCompute \\(\\mathsf{E}_{ii} = \\mathsf{\\Sigma}_\\mathbf{z} + \\mathbf{E}_i\\cdot \\mathbf{E}_i^\\mathsf{T}\\) for all \\(i\\). (E step)\nCompute \\(\\displaystyle{\\mathsf{W}^* = \\left(\\sum_{i=1}^N(\\mathbf{y}_i - \\bar{\\mathbf{y}})\\cdot \\mathbf{E}_i^\\mathsf{T}\\right)\\left(\\sum_{i=1}^N \\mathsf{E}_{ii}\\right)^{-1}}\\). (M step)\nCompute \\(\\displaystyle{(\\boldsymbol{\\sigma}^*)^2 = \\hat{\\boldsymbol{\\sigma}}^2 - \\mathsf{W}^*\\cdot\\left(\\frac{1}{N}\\sum_{i=1}^N \\mathbf{E}_i\\cdot(\\mathbf{y}_i - \\bar{\\boldsymbol{y}})\\right)}\\). (M step)\nIf the difference between the log posterior (which in our case is equal to the log likelihood) evaluated at \\(\\boldsymbol{\\mu}_\\mathrm{MAP}\\), \\(\\mathsf{W}^*\\) and \\(\\boldsymbol{\\sigma}^*\\) and the log posterior evaluated at \\(\\boldsymbol{\\mu}_\\mathrm{MAP}\\), \\(\\mathsf{W}\\) and \\(\\boldsymbol{\\sigma}\\) is smaller than a predefined threshold, STOP and record \\(\\boldsymbol{\\mu}_\\mathrm{MAP}\\), \\(\\mathsf{W}^*\\) and \\(\\boldsymbol{\\sigma}^*\\) as the MAP estimate. Otherwise, go to 6.\nSet \\(\\mathsf{W} = \\mathsf{W}^*\\) and \\(\\boldsymbol{\\sigma} = \\boldsymbol{\\sigma}^*\\).\nGo to 2.\n\n\n49.2.1 Factor analysis with scikit-learn\nWe will not code up the EM algorithm here, as we did for a Gaussian mixture models (see ?sec-gmm-em-example), as it is not particularly instructive. Instead, we will utilize the implementation in scikit-learn. Note, though, that if we did have priors on the parameters, we would need to update the above algorithm and implement it ourselves, since scikit-learn’s implementation does not have priors.\nWe will use the same data set from Remedios, et al. that we did in Section 48.2. We first load in the data set.\n\n# Load in the data set and separate into Numpy arrays\ndata = scipy.io.loadmat(os.path.join(data_path, 'hypothalamus_calcium_imaging_remedios_et_al.mat'))\nneural_data = data['neural_data']\nattack_vector = data['attack_vector'].flatten()\nsex_vector = data['sex_vector'].flatten()\n\n# Time points at 30 Hz sampling\nt = np.arange(len(attack_vector)) / 30\n\nScikit-learn’s implementation of factor analysis assumes centered data. This is because we know the MAP value of \\(\\boldsymbol{\\mu}\\) analytically. So, we can go ahead and compute that directly.\n\n# MAP estimate for parameter mu\nmu = np.mean(neural_data, axis=1)\n\nWe should therefore center the data. We will not scale the data set, since the factor analysis involves estimation of the variance via the parameter \\(\\boldsymbol{\\sigma}\\) along each dimension.\n\n# Centered data set by subtracting the mean\nscaler = sklearn.preprocessing.StandardScaler(with_std=False).fit(neural_data.T)\ny = scaler.transform(neural_data.T)\n\nNext, we can instantiate a FactorAnalysis instance, which requires specification of the number of components. We will again use two components. We will go ahead and run the fit() method to perform the factor analysis.\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    fa = sklearn.decomposition.FactorAnalysis(n_components=2).fit(y)\n\nWe can now explore the results of the factor analysis. The MAP estimate for the location parameter (up to an additive constant) of the recognition distribution, \\(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i}\\), is computed using the transform() method.\n\nmu_z = fa.transform(y)\n\nThe parameter \\(\\boldsymbol{\\sigma}^2\\) is given by the noise_variance_ parameter.\n\nsigma = np.sqrt(fa.noise_variance_)\n\nThe loading matrix is given by the components_ attribute.\n\n# D x L loading matrix is the transpose of the components_ attribute\nW = fa.components_.T\n\nScikit-learn does not compute the covariance matrix of the recognition distribution, \\(\\mathsf{\\Sigma}_\\mathbf{z}\\). As shown in Section 49.1.3, \\(\\mathsf{\\Sigma}_\\mathbf{z}\\) is the same for all data points, given by Equation 49.2. We can therefore compute it.\n\nSigma = np.linalg.inv(np.eye(2) + W.T @ np.diag(1 / fa.noise_variance_) @ W)\n\n# Take a look\nSigma\n\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_33816/4192835866.py:1: RuntimeWarning: divide by zero encountered in matmul\n  Sigma = np.linalg.inv(np.eye(2) + W.T @ np.diag(1 / fa.noise_variance_) @ W)\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_33816/4192835866.py:1: RuntimeWarning: overflow encountered in matmul\n  Sigma = np.linalg.inv(np.eye(2) + W.T @ np.diag(1 / fa.noise_variance_) @ W)\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_33816/4192835866.py:1: RuntimeWarning: invalid value encountered in matmul\n  Sigma = np.linalg.inv(np.eye(2) + W.T @ np.diag(1 / fa.noise_variance_) @ W)\n\n\narray([[1.41289949e-02, 1.09676565e-15],\n       [1.09645742e-15, 2.29261582e-02]])\n\n\nEvidently, \\(\\mathsf{\\Sigma}_\\mathbf{z}\\) is approximately diagonal. This is not the case in general for factor analysis, but is in this case; the components of the latent variables are essentially independent of each other.\n\n\n49.2.2 Visualizing FA results\nIn a factor analysis, instead of having a point estimate for the latent variables \\(\\mathbf{z}\\) as in PCA, we have a posterior distribution for them. We have already computed its parameters \\(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}}\\) and \\(\\mathsf{\\Sigma}_\\mathbf{z}\\). If we want to make a plot of one latent variable plotted against another, we can do so using ellipse glyphs that encompass 95% of the posterior probability mass. To plot an ellipse shape, Bokeh requires a major axis length, minor axis length, and angle from horizontal of the ellipse. Let \\(\\lambda_1\\) and \\(\\lambda_2\\) be the eigenvalues of \\(\\mathsf{\\Sigma}_\\mathbf{z}\\) with \\(\\lambda_1 \\le \\lambda_2\\) with corresponding eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). Then, for a 2D Normal distribution, the ellipse that encompasses a fraction \\(p\\) of the total posterior probability mass has:\n\nMajor axis length: \\(2\\sqrt{\\lambda_2\\,\\chi_2^2(p)}\\)\nMinor axis length: \\(2\\sqrt{\\lambda_1\\,\\chi_2^2(p)}\\)\nAngle from horizontal: \\(\\arctan(v_{2,y} / v_{2,x})\\)\n\nHere, \\(\\chi_2^2(p)\\) represents the percentile of a \\(\\chi\\)-squared distribution with two degrees of freedom. We can calculate these quantities from \\(\\mathsf{\\Sigma}_{\\mathbf{z}}\\).\n\nchi = st.chi2.ppf(0.95, df=2)\n\n# Eigensystem (ordered using eigh)\nlam, v = np.linalg.eigh(Sigma)\n\n# Major and minor axes\nb = 2 * np.sqrt(lam[1] * chi)\na = 2 * np.sqrt(lam[0] * chi)\n\n# Angle from vertical\nangle = np.atan2(v[1, 1], v[1, 0])\n\nNow let’s make the plot!\n\np = bokeh.plotting.figure(\n    frame_height=350,\n    frame_width=350,\n    x_axis_label=\"z₁\",\n    y_axis_label=\"z₂\",\n)\n\n# Set up date for the plot\ntime_color = bebi103.viz.q_to_color(t, bokeh.palettes.Viridis256)\nsex_color = [\"orchid\" if s else \"dodgerblue\" for s in sex_vector]\ncds = bokeh.models.ColumnDataSource(dict(z1=mu_z[:, 0], z2=mu_z[:, 1], t=t, color=time_color))\n\n# We'll allow selection of which color we want to visualize\ncolor_selector = bokeh.models.RadioButtonGroup(labels=[\"time\", \"sex\"], active=0)\njs_code = \"\"\"\ncds.data['color'] = color_selector.active == 0 ? time_color : sex_color;\ncds.change.emit();\n\"\"\"\n\ncolor_selector.js_on_event(\n    \"button_click\",\n    bokeh.models.CustomJS(\n        code=js_code,\n        args=dict(\n            time_color=time_color,\n            sex_color=sex_color,\n            cds=cds,\n            color_selector=color_selector,\n        ),\n    ),\n)\n\n# Populate glyphs\np.ellipse(source=cds, x=\"z1\", y=\"z2\", width=b, height=a, angle=angle, alpha=0.2, color=\"color\")\n\n# Lay out and show!\nbokeh.io.show(\n    bokeh.layouts.row(p, bokeh.layouts.column(bokeh.models.Div(text=\"Color by\"), color_selector)),\n)\n\n\n  \n\n\n\n\n\nThis is a very similar result to what we got when doing PCA, but we can see a measure if the posterior variation in the inferred latent variables.\nReconstruction of the data based on the reduced representation is not as straightfoward, as we would have the sample out of the generative distribution parametrized but the posterior distribution. We could take \\(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid \\mathbf{y}}\\) as a point estimate for the latent parameter \\(\\mathbf{z}\\).\n\n# Compute the reconstruction\ny_hat = np.dot(W, mu_z.T).T\n\n# Uncenter the reconstruction\ny_hat = scaler.inverse_transform(y_hat)\n\n# Kwargs for plotting\nkwargs = dict(\n    x_interpixel_distance=1/30,\n    y_interpixel_distance=1,\n    frame_height=150,\n    frame_width=600,\n    x_axis_label=\"time (s)\",\n    y_axis_label=\"neuron\",\n)\n\n# Plot original data\np_neuron = bebi103.image.imshow(neural_data, **kwargs)\n\n# Plot reconstructed data\np_hat = bebi103.image.imshow(y_hat.T, **kwargs)\n\np_neuron.yaxis.major_label_text_font_size = '0pt'\np_hat.yaxis.major_label_text_font_size = '0pt'\n\nbokeh.io.show(bokeh.layouts.column(p_neuron, p_hat))",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/factor_analysis.html#computing-environment",
    "href": "lessons/pca_and_related/factor_analysis.html#computing-environment",
    "title": "49  Factor analysis",
    "section": "Computing environment",
    "text": "Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,scipy,sklearn,bebi103,bokeh,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nsklearn   : 1.6.1\nbebi103   : 0.1.27\nbokeh     : 3.6.2\njupyterlab: 4.4.3",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/pca_nmf_as_fa.html",
    "href": "lessons/pca_and_related/pca_nmf_as_fa.html",
    "title": "50  Special cases of factor analysis",
    "section": "",
    "text": "50.1 Probabilistic PCA\nIn Chapter 49, we wrote the generative model for factor analysis.\n\\[\\begin{align}\n&\\boldsymbol{\\mu} \\sim \\text{prior for }\\boldsymbol{\\mu}, \\\\[1em]\n&\\mathsf{W} \\sim \\text{prior for }\\mathsf{W}, \\\\[1em]\n&\\boldsymbol{\\sigma} \\sim \\text{prior for }\\boldsymbol{\\sigma} \\\\[1em]\n&\\mathsf{\\Psi} = \\text{diag}(\\boldsymbol{\\sigma}^2) \\\\[1em]\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid\\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\mathsf{\\Psi} \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi})\\; \\forall i\n\\end{align}\n\\]\nHere, we consider three special cases of factor analysis that correspond to techniques commonly known as probabilistic principal component analysis (PPCA), PCA (which we have already seen from a heuristic point of view), and nonnegative matrix factorization (NMF).\nWe consider first the special case where each entry in \\(\\boldsymbol{\\sigma}\\) is the same, say \\(\\sigma\\). In that case, \\(\\mathsf{\\Psi} = \\sigma\\mathsf{I}\\). We further restrict \\(\\mathsf{W}\\) to have orthonormal columns. The assumption here is that each data point is drawn from a Normal distribution with the same variance, but with mean \\(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}\\). This special case is called probabilistic principle component analysis (PPCA).",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Special cases of factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/pca_nmf_as_fa.html#probabilistic-pca",
    "href": "lessons/pca_and_related/pca_nmf_as_fa.html#probabilistic-pca",
    "title": "50  Special cases of factor analysis",
    "section": "",
    "text": "50.1.1 PPCA generative model\nThe full model is\n\\[\\begin{align}\n&\\boldsymbol{\\mu} \\sim \\text{prior for }\\boldsymbol{\\mu} \\text{ (usually assumed Uniform)}, \\\\[1em]\n&\\mathsf{W} \\text{ with orthonormal columns, otherwise uninformative prior}, \\\\[1em]\n&\\sigma \\sim \\text{prior for }\\sigma \\text{ (usually assumed Uniform)} \\\\[1em]\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid\\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\sigma \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\sigma^2 \\mathsf{I})\\; \\forall i\n\\end{align}\n\\]\nProbabilistic PCA is a reasonable model; it assumes that each data point results from a linear combination of the latent variables and is uncorrelated with all other data points (except through the latent variables) with homoscedasticity.\n\n\n50.1.2 MAP estimate for PPCA\nPPCA has an added advantage that the MAP estimate may be computed analytically for uniform priors on \\(\\boldsymbol{\\mu}\\) and \\(\\sigma\\). The result is\n\\[\\begin{align}\n&\\boldsymbol{\\mu}_\\mathrm{MAP} = \\bar{\\mathbf{y}},\\\\[1em]\n&\\mathsf{W}_\\mathrm{MAP} = \\mathsf{U}\\cdot(\\mathsf{\\Lambda} - \\sigma^2\\mathsf{I})^{\\frac{1}{2}} \\\\[1em]\n&\\sigma^2 = \\frac{1}{D - L}\\sum_{j=L+1}^D \\lambda_j,\n\\end{align}\n\\tag{50.1}\\]\nwhere \\(=(\\lambda_1, \\ldots, \\lambda_D)\\) are the eigenvalues of the empirical covariance matrix\n\\[\\begin{align}\n\\hat{\\mathsf{\\Sigma}} = \\frac{1}{N}\\sum_{i=1}^N(\\mathbf{y}_i - \\bar{y})\\cdot(\\mathbf{y}_i - \\bar{y})^\\mathsf{T}\n\\end{align}\\]\nordered from largest to smallest, \\(\\mathsf{\\Lambda} = \\text{diag}((\\lambda_1, \\ldots, \\lambda_L))\\), and \\(\\mathsf{U}\\) is a matrix whose columns are the eigenvectors corresponding to eigenvalues \\(\\lambda_1, \\ldots, \\lambda_L\\).\n\n\n50.1.3 Marginal likelihood for PPCA\nReferring to the marginal likelihood from factor analysis, Equation 49.1 and considering the special case where all \\(\\sigma\\)’s are the same, the marginal likelihood for PPCA is\n\\[\\begin{align}\n\\mathbf{y}_i\\mid \\boldsymbol{\\mu}, \\mathsf{W}, \\sigma\\sim \\text{Norm}(\\boldsymbol{\\mu},\\mathsf{W}\\cdot\\mathsf{W}^\\mathsf{T} + \\sigma^2\n\\,\\mathsf{I}).\n\\end{align}\n\\]\n\n\n50.1.4 Recognition distribution for PPCA\nRecall from Section 49.1.3 that the recognition distribution for factor analysis is\n\\[\\begin{align}\n\\mathbf{z}_i\\mid \\mathbf{y}_i, \\boldsymbol{\\mu}, \\mathsf{W}, \\mathsf{\\Psi} \\sim \\text{Norm}(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i}, \\mathsf{\\Sigma}_{\\mathbf{z}}),\n\\end{align}\n\\]\nwhere\n\\[\\begin{align}\n\\mathsf{\\Sigma}_{\\mathbf{z}} = (\\mathsf{I} + \\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{\\Psi}^{-1}\\cdot\\mathsf{W})^{-1}\n\\end{align}\n\\]\nand\n\\[\\begin{align}\n\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i} = \\mathsf{\\Sigma}_{\\mathbf{z}} \\cdot\\mathsf{W}^\\mathsf{T}\\cdot \\mathsf{\\Psi}^{-1}\\cdot(\\mathbf{y_i - \\boldsymbol{\\mu}}).\n\\end{align}\n\\]\nIn the special case of PPCA where \\(\\mathsf{\\Psi} = \\sigma^2 \\mathsf{I}\\), the scale and location parameters for the recognition distribution are\n\\[\\begin{align}\n&\\mathsf{\\Sigma}_{\\mathbf{z}} = \\sigma^2\\,(\\sigma^{2}\\,\\mathsf{I} + \\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{W})^{-1},\\\\[1em]\n&\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i} = (\\sigma^{2}\\,\\mathsf{I} + \\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{W})^{-1}\\cdot\\mathsf{W}^\\mathsf{T}\\cdot(\\mathbf{y_i - \\boldsymbol{\\mu}}) = (\\sigma^{2}\\,\\mathsf{I} + \\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{W})^{-1}\\cdot\\mathsf{W}^\\mathsf{T}\\cdot(\\mathbf{y_i - \\bar{\\mathbf{y}}}),\n\\end{align}\n\\]\nwhere in the last equality we have plugged in the MAP estimate for \\(\\boldsymbol{\\mu}\\).",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Special cases of factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/pca_nmf_as_fa.html#pca-as-a-limit-of-ppca",
    "href": "lessons/pca_and_related/pca_nmf_as_fa.html#pca-as-a-limit-of-ppca",
    "title": "50  Special cases of factor analysis",
    "section": "50.2 PCA as a limit of PPCA",
    "text": "50.2 PCA as a limit of PPCA\nConsider now the limit of PPCA where \\(\\sigma\\) goes to zero. That is, we consider the limit where there is no noise in the measured data. The scale parameter of the recognition distribution vanishes and the location parameter becomes\n\\[\\begin{align}\n\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i} = (\\mathsf{W}^\\mathsf{T}\\cdot\\mathsf{W})^{-1}\\cdot\\mathsf{W}^\\mathsf{T}\\cdot(\\mathbf{y_i - \\bar{\\mathbf{y}}}),\n\\end{align}\n\\]\nwhich is the solution to the least squares problem\n\\[\\begin{align}\n\\mathbf{z}_i = \\text{arg min}_{\\mathbf{z}_i} \\left\\lVert (\\mathbf{y}_i - \\bar{\\mathbf{y}}_i) - \\mathsf{W}\\cdot\\mathbf{z}_i\\right\\rVert_2^2.\n\\end{align}\n\\]\nComparing to the PCA loss function, Equation 48.2, this is exactly what PCA does (provided the data are centered)! So, PCA is the noise-free (\\(\\sigma\\to 0\\)) limit of PPCA, which is itself the special case of a factor model with \\(\\boldsymbol{\\sigma} = \\sigma\\,\\mathbf{1}\\).",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Special cases of factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/pca_and_related/pca_nmf_as_fa.html#nonnegative-matrix-factorization",
    "href": "lessons/pca_and_related/pca_nmf_as_fa.html#nonnegative-matrix-factorization",
    "title": "50  Special cases of factor analysis",
    "section": "50.3 Nonnegative matrix factorization",
    "text": "50.3 Nonnegative matrix factorization\nIn factor analysis and its special cases of PPCA and PCA, we have been organizing our data into \\(D\\)-vectors \\(\\mathbf{y}_i\\), where \\(i \\in[1, N]\\), and assuming a multivariate Normal likelihood. For example, for PPCA,\n\\[\\begin{align}\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\mid\\boldsymbol{\\mu}, \\mathsf{W}, \\mathbf{z}_i, \\sigma \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\sigma^2 \\mathsf{I})\\; \\forall i,\n\\end{align}\n\\tag{50.2}\\]\nwhere we introduce an \\(L\\)-vector \\(\\mathbf{z}_i\\) for each data point.\nWe can arrange this in a different way. We define an \\(N\\times D\\) matrix of data \\(\\mathsf{Y}\\) whose rows consist of the \\(\\mathbf{y}_i\\) we have been considering. We can similarly group the latent variables into a \\(N \\times L\\) matrix \\(\\mathsf{Z}\\), where each row is comprised of an \\(L\\)-dimensional latent variable. We now write\n\\[\\begin{align}\ny_{i,j} \\mid \\mathsf{W}, \\mathsf{Z}, \\sigma \\sim \\text{Norm}(A_{ij}, \\sigma)\\; \\forall i,j,\n\\end{align}\n\\tag{50.3}\\]\nwhere \\(A_{ij}\\) is an entry in a matrix \\(\\mathsf{A}\\) defined as\n\\[\\begin{align}\n\\mathsf{A} = (\\mathsf{W}\\cdot\\mathsf{Z}^\\mathsf{T})^\\mathsf{T},\n\\end{align}\\]\nwhich is equivalent to Equation 50.2. If we have uniform priors with the likelihood given by Equation 50.3, the MAP finding problem is equivalent to finding the minimizer of\n\\[\\begin{align}\n\\left\\lVert \\mathsf{Y} - (\\mathsf{W}\\cdot\\mathsf{Z}^\\mathsf{T})^\\mathsf{T} \\right\\rVert_F^2,\n\\end{align}\n\\]\nwhere the Frobenius norm of a matrix \\(\\mathsf{M}\\) is defined as the sum of the squares of its entries,\n\\[\\begin{align}\n\\left\\lVert \\mathsf{M}\\right\\rVert_F^2 = \\sum_i\\sum_j\\,M_{ij}^2 = \\mathrm{tr}(\\mathsf{M}^\\mathsf{T}\\cdot\\mathsf{M}).\n\\end{align}\n\\]\nWe would additionally need to find the MAP estimate for \\(\\sigma\\), which in the PPCA case is given by Equation 50.1.\nIf \\(\\mathsf{Y}\\) has all nonnegative entries, we can add an additional constraint that \\(\\mathsf{Z} &gt; 0\\) and \\(\\mathsf{W} &gt; 0\\). This is conceptually done with the prior for \\(\\mathsf{W}\\) and \\(\\mathsf{Z}\\), but in practice the positivity is a constraint on the optimization problem of minimizing the Frobenius norm. In this constrained case, Equation 50.1 no longer gives a MAP estimate for \\(\\sigma\\). Nonetheless, we see two nonegative matrices, \\(\\mathsf{W}\\) and \\(\\mathsf{Z}\\) that minimize the above Frobenius norm. The result is referred to as the nonnegative matrix factorization (NMF) of the data set \\(\\mathsf{Y}\\).\nSome researchers claim that NMF results are more interpretable than PCA of PPCA because computing \\(\\mathsf{W}\\cdot \\mathsf{Z}^\\mathsf{T}\\) amounts to constructively adding components to reach an approximation of the observed data \\(\\mathsf{Y}\\). Within an interpretation of the model, however, it is a similar model as PPCA, except with a positive constraint on the parameters.\nNMF is implemented in scikit-learn.\nAs a final note about NMF, be careful about nomenclature. In much of the NMF literature, what we are calling \\(\\mathsf{Z}\\) is called \\(\\mathsf{W}\\) and what we are calling \\(\\mathsf{W}\\) is called \\(\\mathsf{H}\\). This is really confusing. Be sure to read documentation of any package you use.",
    "crumbs": [
      "Principal component analysis and related models",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Special cases of factor analysis</span>"
    ]
  },
  {
    "objectID": "lessons/HMM/hmm.html",
    "href": "lessons/HMM/hmm.html",
    "title": "Hidden Markov models",
    "section": "",
    "text": "Thus far, we have worked with time series, but here we formally introduce their treatment. A time series consists of data measured in succession in time. The treatment of time series is a vast subject, and we will eventually deal with some of the many ways to model them, including filtering and smoothing.\nHere, we introduce the concept of a hidden Markov model, or HMM. In its simplest incarnation, which is what we will study, the observation at each point in time is dependent on a latent variable at that point in time. The latent variable is itself dependent on the value of the latent variable at the previous point in time. The latent variables take on a finite discrete set of states over time.\nAs an example, one may think of the mood of a subject as a latent variable and neuronal activity as the observation.",
    "crumbs": [
      "Hidden Markov models"
    ]
  },
  {
    "objectID": "lessons/HMM/hmm_gaussian_emission.html",
    "href": "lessons/HMM/hmm_gaussian_emission.html",
    "title": "51  Hidden Markov models",
    "section": "",
    "text": "51.1 Generative model for HMMs\n| Download notebook\nConsider the following experiment. A mouse’s movement is tracked over time. At each time point, we measure the mouse’s velocity. We plot of the mouse’s velocity over time is shown below.\nWe believe that the mouse may be in two states, a restful still state and a moving walking state. The restful state has a typical movement speed of \\(v_s\\) and the moving state has a typical speed of \\(v_w\\). We cannot observe the state of the mouse, but only its velocity. The state (still or moving) is a discrete (in this case categorical) latent variable, and the velocity is an observed variable. This is an example of an experiment that is amenable to a hidden Markov model (HMM). In an HMM, a discrete latent variable changes state over time. At any given time point, the observed variable is dependent on the latent variable. This is shown graphically in the figure below. The latent variable at time point \\(t\\) is \\(z_t\\) and the observation at time point \\(t\\) is \\(y_t\\).\nLet us now write down a generative model for a generic HMM. Let \\(K\\) be the number of discrete states accessible to the latent variables. We define \\(\\mathsf{\\Gamma}\\) to be the \\(K\\times K\\) transition matrix. \\(\\Gamma_{ij}\\) is the probability of a latent variable transitioning from state \\(i\\) to state \\(j\\) in a given time step. “Transitions” from \\(i\\) to \\(i\\) are allowed, and this is called the self-loop probability. Each row of \\(\\mathsf{\\Gamma}\\), which we define as \\(\\boldsymbol{\\Gamma}_i\\), therefore sums to one,\n\\[\\begin{align}\n\\sum_{j=1}^K \\Gamma_{ij} = 1.\n\\end{align}\n\\]\nLet \\({z}_t \\in [1, \\ldots K]\\) be the state at time point \\(t\\). Let \\(\\boldsymbol{\\rho} = [\\rho_1,\\ldots,\\rho_K]\\) with \\(\\rho_i\\) being the probability that \\(z_1 = i\\).\nWith these definitions in mind, we can write the generative model. The likelihood is parametrized by \\(\\phi_z\\), a set of parameters that depend on the identity of \\(z\\) at a given time. That is to say that the likelihood for a given data point is \\(f(y_t\\mid \\phi_{z_t})\\). In the context of HMMs, the likelihood is referred to as an emission probability distribution.\nPutting it all together, the model for a HMM is\n\\[\\begin{align}\n&\\text{priors for } \\phi_z, \\\\[1em]\n&\\boldsymbol{\\rho} \\sim \\text{prior for } \\boldsymbol{\\rho} \\text{, usually Dirichlet},\\\\[1em]\n&\\boldsymbol{\\Gamma}_i \\sim \\text{prior for } \\boldsymbol{\\Gamma}_i \\text{, usually Dirichlet} \\; \\forall i,\\\\[1em]\n&z_1 \\mid \\boldsymbol{\\rho} \\sim \\text{Categorical}(\\boldsymbol{\\rho}),\\\\[1em]\n&z_t \\mid z_{t-1}, \\mathsf{\\Gamma} \\sim \\text{Categorical}(\\boldsymbol{\\Gamma}_{z_{t-1}})\\;\\forall t\\ge 2, \\\\[1em]\n&y_t \\mid z_t, \\phi_{z_t} \\sim \\text{emission probability parametrized by } \\phi_{z_t} \\;\\forall t.\n\\end{align}\n\\tag{51.1}\\]",
    "crumbs": [
      "Hidden Markov models",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hidden Markov models</span>"
    ]
  },
  {
    "objectID": "lessons/HMM/hmm_gaussian_emission.html#generative-model-for-hmms",
    "href": "lessons/HMM/hmm_gaussian_emission.html#generative-model-for-hmms",
    "title": "51  Hidden Markov models",
    "section": "",
    "text": "51.1.1 Generative model for the mouse\nFor the mouse we are observing move and rest, there are two states (\\(K = 2\\)), moving and resting. We can model the velocity of the mouse in the respective states as Normally distributed with parameters \\(v_m, \\sigma_m\\) for moving and \\(v_s, \\sigma_s\\) for resting (still). This specifies our emission distribution with \\(\\phi_z = \\{\\mu_z, \\sigma_z\\}\\). The model is then (with speeds in units of cm/s)\n\\[\\begin{align}\n&v_s \\sim \\text{Norm}(0.5, 0.5),\\\\[1em]\n&\\sigma_s \\sim \\text{HalfNorm}(0, 1.0),\\\\[1em]\n&v_m \\sim \\text{Norm}(8.0, 5.0),\\\\[1em]\n&\\sigma_m \\sim \\text{HalfNorm}(0, 10.0),\\\\[1em]\n&\\boldsymbol{\\rho} \\sim \\text{Dirichlet}(1, 1),\\\\[1em]\n&\\boldsymbol{\\Gamma}_1 \\sim \\text{Dirichlet}(70, 3),\\\\[1em]\n&\\boldsymbol{\\Gamma}_2 \\sim \\text{Dirichlet}(3, 70),\\\\[1em]\n&z_1 \\mid \\boldsymbol{\\rho} \\sim \\text{Categorical}(\\boldsymbol{\\rho}),\\\\[1em]\n&z_t \\mid z_{t-1}, \\mathsf{\\Gamma} \\sim \\text{Categorical}(\\boldsymbol{\\Gamma}_{z_{t-1}})\\;\\forall t\\ge 2, \\\\[1em]\n&y_t \\mid z_t, \\mu_{z_t}, \\phi_{z_t} \\sim  \\text{Norm}(\\mu_{z_t}, \\sigma_{z_t}) \\;\\forall t.\n\\end{align}\n\\]\nI have chosen a Normal prior \\(v_m\\) based on my understanding on how fast mice walk. I chose a prior for \\(v_s\\) such that the mouse is nearly still. I chose reasonable priors for the standard deviations of the Normal emission distribution. I will explain exactly why I chose Inverse Gamma priors in following sections.\nI chose an uninformative prior for the initial state probabilities \\(\\boldsymbol{\\rho}\\). I chose Dirichlet priors for the rows of the transition matrix that correspond to high self-loop probabilities because the frequency of sampling in the experiment (30 Hz) is much faster than the transition rate out of a given state.",
    "crumbs": [
      "Hidden Markov models",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hidden Markov models</span>"
    ]
  },
  {
    "objectID": "lessons/HMM/hmm_gaussian_emission.html#inference-with-hmms",
    "href": "lessons/HMM/hmm_gaussian_emission.html#inference-with-hmms",
    "title": "51  Hidden Markov models",
    "section": "51.2 Inference with HMMs",
    "text": "51.2 Inference with HMMs\nThe full posterior of an HMM is\n\\[\\begin{align}\ng(z, \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\} \\mid y) = \\frac{f(y\\mid z, \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})\\,g(z, \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})}{f(y)}.\n\\end{align}\n\\]\nIf we naively tried to infer the latent variables in addition to the transition matrix, initial probabilities, and emission distribution parameters (which are of primary interest), we would have a very high dimensional inference problem, since the number of latent variables, being equal to the total number of observations, can be very large. It is therefore much more practical to work with the marginal distribution with the latent variables marginalized away and then compute the posterior distribution of the latent variables from the inferred marginal distributions according to\n\\[\\begin{align}\ng(z\\mid y, \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\}) = \\frac{f(y\\mid z, \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})\\,g(z\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma})}{f(y\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})},\n\\end{align}\n\\tag{51.2}\\]\nwhere we have used the fact that \\(g(z\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\}) = g(z\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma})\\). The denominator in the above expression is the marginalized likelihood.\n\\[\\begin{align}\nf(y\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\}) = \\sum_z f(y\\mid z, \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})\\,g(z\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma}).\n\\end{align}\n\\tag{51.3}\\]\nWhile this is a good strategy, it is complicated by fact that, unlike in a Gaussian mixture model that we saw in ?sec-em-gmm, we cannot easily compute the marginal likelihood. This is because \\(z_t\\) is dependent on \\(z_{t-1}\\) for all \\(t\\) and we therefore cannot treat the sum over each \\(z_t\\) independently.\nThere are nonetheless clever algorithms for computing the marginal likelihood. These are the so-called forward-backward algorithms, and are nicely described in section 13.2.2 of Bishop’s Pattern Recognition and Machine Learning book. Fortunately, these algorithms are implement in both of the packages we will use to analyze HMMs, Stan and hmmlearn.\n\n51.2.1 Inference of an HMM using MCMC\nStan has handy built-in functions for inference using HMMs. Most importantly, the hmm_marginal() function computes the log likelihood with the latent variables marginalized away. That is, it computes \\(f(y\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})\\), given by Equation 51.3. This allows inference using the marginal model, computing the posterior,\n\\[\\begin{align}\ng(\\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\} \\mid y) = \\frac{f(y\\mid \\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})\\,g(\\boldsymbol{\\rho}, \\mathsf{\\Gamma}, \\{\\phi_z\\})}{f(y)}.\n\\end{align}\n\\]\nThe probability distribution of the latent variables for each time point (a Categorical distribution) may be calculated according to Equation 51.2. This is also implemented in Stan with the hmm_hidden_state_prob() function, though there is a known bug with that function as of June 2025. We can also sample the latent variable using the hmm_latent_rng() function.\nLet’s now implement our model in Stan and get some samples! The Stan code is stored in mouse_speed_hmm_2_states.stan and is shown below. We right the Stan code to be generically used for HMMs with univariate Gaussian emission distributions with parts of the code necessary to modify for our specific problem labeled as such.\ndata {\n    int&lt;lower=2&gt; N; // Number of observations (number of time points)\n    array[N] real y;\n}\n\n\ntransformed data {\n    // Number of hidden states. SPECIFIC FOR MODEL.\n    int K = 2;\n}\n\n\nparameters {\n    // Make location parameters of Gaussians ordered to break nonident.\n    ordered[K] mu;\n    array[K] real&lt;lower=0.0&gt; sigma;\n\n    // Initial state probabilities\n    simplex[K] rho;\n\n    // Rows of the transition matrix\n    array[K] simplex[K] Gamma_rows;\n}\n\n\ntransformed parameters {\n    // Make Gamma into a matrix\n    matrix[K, K] Gamma;\n    for (k in 1:K) {\n        Gamma[k, :] = Gamma_rows[k]';\n    }\n\n    // omega[k, i] = f(y_i | z_i=k , mu_k, sigma_k)\n    matrix[K, N] log_omega;\n\n    // Compute the log likelihoods in each possible state\n    for (i in 1:N) {\n        for (k in 1:K) {\n          log_omega[k, i] = normal_lpdf(y[i] | mu[k], sigma[k]);\n        }\n    }\n}\n\n\nmodel {\n    // PRIORS; SPECIFIC FOR MODEL\n    // Priors for velocities\n    mu[1] ~ normal(0.5, 0.5);     // Resting state\n    mu[2] ~ normal(8.0, 5.0);     // Moving state\n\n    // Priors for standard deviations of velocities\n    sigma[1] ~ normal(0.0, 1.0);   // Resting state\n    sigma[2] ~ normal(0.0, 10.0);  // Moving state\n\n    // Prior on initial probabilities\n    rho ~ dirichlet([1.0, 1.0]);\n\n    // Priors on transition probabilities\n    Gamma_rows[1] ~ dirichlet([70.0, 3.0]);\n    Gamma_rows[2] ~ dirichlet([3.0, 70.0]);\n    // END OF PRIORS SPECIFIC FOR MODEL\n\n    // Likelihood\n    target += hmm_marginal(log_omega, Gamma, rho);\n}\n\n\ngenerated quantities {\n    // We could compute the probability of values of latent variables\n    // But there is a known bug in this: https://github.com/stan-dev/math/issues/2677\n    // matrix[K, N] z_probs = hmm_hidden_state_prob(log_omega, Gamma, rho);\n\n    // Sample latent variables\n    array[N] int z = hmm_latent_rng(log_omega, Gamma, rho);\n\n    // Posterior predictive\n    array[N] real y_ppc = normal_rng(mu[z], sigma[z]);\n}\nNote that we have specified that the location parameters of the Normal emission distribution are ordered in an attempt to break a label-switching nonidentifiability. Note also that we need to specify that each row of the transition matrix \\(\\mathsf{\\Gamma}\\) is a simplex such that all entries add to one. We then specify a prior for each row of \\(\\mathsf{\\Gamma}\\). Naturally, \\(\\mathbf{\\rho}\\) must also be a simplex data type.\nLet’s compile!\n\nsm2 = cmdstanpy.CmdStanModel(stan_file='mouse_speed_hmm_2_states.stan')\n\n01:07:04 - cmdstanpy - INFO - compiling stan file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/HMM/mouse_speed_hmm_2_states.stan to exe file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/HMM/mouse_speed_hmm_2_states\n01:07:11 - cmdstanpy - INFO - compiled model executable: /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/HMM/mouse_speed_hmm_2_states\n\n\nIn general, HMM models can be massively multimodal and nonidentifiable. We have already attempted to address the label-switching nonidentifiability by ordering the location parameters of the emission distribution. We nonetheless need to give the chains of the sampler a chance of sampling the same mode by starting then in a similar place. As a trick to initialize the chains, we can ignore the latent variables and time dependence and treat the measurements as a Gaussian mixture model of \\(K\\) mixtures and infer the MAP ( actually and MLE, since we will use scikit-learn, which does not take into account priors) means and covariances. We can then start the chains at the MAP \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\) values from the GMM.\n\n# Measured data\nt = df['time (s)'].to_numpy()\ny = df['speed (cm/s)'].to_numpy()\n\n# Number of hidden states\nK = 2\n\n\ndef hmm_univariate_gauss_emission_inits(y, K):\n    \"\"\"Initial positions of location and scale parameters for an HMM \n    with a univariate Normal emission distribution. Uses a GMM to \n    approximate modes.\n\n    Parameters\n    ----------\n    y : Numpy array, shape (N,)\n        Observations.\n\n    K : int\n        Number of latent states.\n\n    Returns\n    -------\n    mu : Numpy array, shape (K,)\n        Initial chain positions of location parameters of the Normal\n        emission distribution.\n    sigma : Numpy array, shape (K,)\n        Initial chain positions of scale parameters of the Normal\n        emission distribution.\n    \"\"\"\n    # Set up GMM, ignoring time-variation\n    gmm = sklearn.mixture.GaussianMixture(K)\n    gmm.fit(y[:, np.newaxis])\n\n    # Extract and sort means and variances\n    mu = gmm.means_.flatten()\n    sigma2 = gmm.covariances_.flatten()\n    inds = np.argsort(mu)\n\n    # Return guesses for mu and sigma\n    return mu[inds], np.sqrt(sigma2[inds])\n\n# Compute inits for our two-latent state case\nmu_init, sigma_init = hmm_univariate_gauss_emission_inits(y, K)\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: divide by zero encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: overflow encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: invalid value encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n\n\nNow that we have our initial chain positions, we can do the sampling!\n\ndata = dict(N=len(y), y=y)\n\n# Acquire samples and store as ArviZ instance\nwith bebi103.stan.disable_logging():\n    samples = sm2.sample(data=data, inits=dict(mu=mu_init, sigma=sigma_init))\n    samples = az.from_cmdstanpy(samples, posterior_predictive='y_ppc')\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nWe can start by looking at the corner plot of the emission distribution parameters.\n\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples,\n        parameters=[\n            (\"mu[0]\", \"vₛ (cm/s)\"),\n            (\"mu[1]\", \"vₘ (cm/s)\"),\n            (\"sigma[0]\", \"σₛ (cm/s)\"),\n            (\"sigma[1]\", \"σₘ (cm/s)\"),\n        ],\n        xtick_label_orientation=np.pi / 4,\n    )\n)\n\n\n  \n\n\n\n\n\nSomething appears fishy here. In looking at the exploratory plot of the data above, the resting velocity is less than 1 cm/s and the walking velocity is about 5 cm/s. So are getting a larger resting velocity and a much larger moving velocity than we might guess from looking at the plot. Let us now do a posterior predictive check to investigate.\n\n# Extract posterior predictive samples\ny_ppc = samples.posterior_predictive.y_ppc.stack(\n    {\"sample\": (\"chain\", \"draw\")}\n).transpose(\"sample\", \"y_ppc_dim_0\")\n\n# Make a predictive plot\nbokeh.io.show(\n    bebi103.viz.predictive_regression(\n        y_ppc,\n        samples_x=df[\"time (s)\"].to_numpy(),\n        data=np.vstack((t, y)).transpose(),\n        frame_width=700,\n        frame_height=250,\n        x_axis_label='time (s)',\n        y_axis_label='speed (cm/s)'\n    )\n)\n\n\n  \n\n\n\n\n\nApparently, our model is not distinguishing between a resting state with very low speed and moving slowly. The occasional large velocities have resulted in two states, one moving really fast (running), and one moving slowly (both walking and resting). So, we apparently need three latent states. Let’s adjust our model accordingly by adding a “running” state with high speed, bearing in mind that laboratory mice can run at about 50 cm/s.\n\\[\\begin{align}\n&v_s \\sim \\text{Norm}(0.5, 0.5),\\\\[1em]\n&\\sigma_s \\sim \\text{HalfNorm}(0, 1.0),\\\\[1em]\n&v_m \\sim \\text{Norm}(8.0, 5.0),\\\\[1em]\n&\\sigma_m \\sim \\text{HalfNorm}(0, 10.0),\\\\[1em]\n&v_r \\sim \\text{Norm}(50.0, 30.0),\\\\[1em]\n&\\sigma_r \\sim \\text{HalfNorm}(0, 40.0),\\\\[1em]\n&\\boldsymbol{\\rho} \\sim \\text{Dirichlet}(1, 1, 1),\\\\[1em]\n&\\boldsymbol{\\Gamma}_1 \\sim \\text{Dirichlet}(70, 3, 3),\\\\[1em]\n&\\boldsymbol{\\Gamma}_2 \\sim \\text{Dirichlet}(3, 70, 3),\\\\[1em]\n&\\boldsymbol{\\Gamma}_3 \\sim \\text{Dirichlet}(3, 3, 70),\\\\[1em]\n&z_1 \\mid \\boldsymbol{\\rho} \\sim \\text{Categorical}(\\boldsymbol{\\rho}),\\\\[1em]\n&z_t \\mid z_{t-1}, \\mathsf{\\Gamma} \\sim \\text{Categorical}(\\boldsymbol{\\Gamma}_{z_{t-1}})\\;\\forall t\\ge 2, \\\\[1em]\n&y_t \\mid z_t, \\mu_{z_t}, \\phi_{z_t} \\sim  \\text{Norm}(\\mu_{z_t}, \\sigma_{z_t}) \\;\\forall t.\n\\end{align}\n\\]\nThe updated Stan code is below.\n\n\n# Compile!\nsm3 = cmdstanpy.CmdStanModel(stan_file='mouse_speed_hmm_3_states.stan')\n\n# Initial positions of chains\nK = 3\nmu_init, sigma_init = hmm_univariate_gauss_emission_inits(y, 3)\n\n# Sample!\nwith bebi103.stan.disable_logging():\n    samples = sm3.sample(data=data, inits=dict(mu=mu_init, sigma=sigma_init))\n    samples = az.from_cmdstanpy(samples, posterior_predictive='y_ppc')\n\n01:08:49 - cmdstanpy - INFO - compiling stan file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/HMM/mouse_speed_hmm_3_states.stan to exe file /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/HMM/mouse_speed_hmm_3_states\n01:08:55 - cmdstanpy - INFO - compiled model executable: /Users/bois/Dropbox/git/datasai/2025/content/content/lessons/HMM/mouse_speed_hmm_3_states\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: divide by zero encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: overflow encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: invalid value encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                \n\n\nIt’s a bit more work for the sampler with more states, but we can still sample the posterior in a few minutes. Let’s now look at the corner plot of the emission distribution parameters.\n\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples,\n        parameters=[\n            (\"mu[0]\", \"vₛ (cm/s)\"),\n            (\"mu[1]\", \"vₘ (cm/s)\"),\n            (\"mu[2]\", \"vᵣ (cm/s)\"),\n            (\"sigma[0]\", \"σₛ (cm/s)\"),\n            (\"sigma[1]\", \"σₘ (cm/s)\"),\n            (\"sigma[2]\", \"σᵣ (cm/s)\"),\n        ],\n        xtick_label_orientation=np.pi / 4,\n    )\n)\n\n\n  \n\n\n\n\n\nThis already looks more reasonable , with a resting, walking, and running speed. The running speed has a much higher variance.\nWe can perform a posterior predictive check.\n\n# Extract posterior predictive samples\ny_ppc = samples.posterior_predictive.y_ppc.stack(\n    {\"sample\": (\"chain\", \"draw\")}\n).transpose(\"sample\", \"y_ppc_dim_0\")\n\n# Make a predictive plot\nbokeh.io.show(\n    bebi103.viz.predictive_regression(\n        y_ppc,\n        samples_x=df[\"time (s)\"].to_numpy(),\n        data=np.vstack((t, y)).transpose(),\n        frame_width=700,\n        frame_height=250,\n        x_axis_label='time (s)',\n        y_axis_label='speed (cm/s)'\n    )\n)\n\n\n  \n\n\n\n\n\nThis looks very good as well.\nThe transition matrix is of interest, so let’s make a corner plot of the samples of its entries. We will only consider the self looping probabilities and the transitions out of the walking state for concision in the notebook, but we could include more if we wish.\n\nbokeh.io.show(\n    bebi103.viz.corner(\n        samples,\n        parameters = ['Gamma[0,0]', 'Gamma[1,0]', 'Gamma[1,1]', 'Gamma[1,2]', 'Gamma[2,2]'],\n        xtick_label_orientation=np.pi/4,\n    )\n)\n\n\n  \n\n\n\n\n\nThe results indicate that a walking mouse tends to stay walking, and if it transitions, it tends to transition to resting as opposed to running. A running mouse is less likely to stay running that a resting mouse is to stay resting or a walking mouse is to stay walking.\nFinally, we can predict the most likely state based on the samples of the latent variables \\(z\\). To make this annotation, I write a function to make a plot with annotation of discrete categorical latent variables.\n\ndef plot_trace(\n    t,\n    y,\n    z,\n    palette=bokeh.palettes.Category10_10,\n    frame_height=250,\n    frame_width=700,\n    **kwargs\n):\n    \"\"\"Plot a trace of y vs t with latent parameter values annotated\n    with colored shading.\n    \"\"\"\n    # Make sure we have enough colors\n    if len(np.unique(z)) &gt; len(palette):\n        raise RuntimeError(\"Too many different latent variables for palette.\")\n\n    # Indices where latent variable changes\n    inds = np.concatenate(([0], np.where(z[:-1] != z[1:])[0] + 1, [len(z) - 1]))\n\n    # Value of each block of z-values\n    z_vals = (z[inds[:-1]] - z.min()).astype(int)\n\n    # Instantiate figure\n    p = bokeh.plotting.figure(\n        frame_height=250, frame_width=700, x_range=[t.min(), t.max()], **kwargs\n    )\n\n    # Shade by most likely value of latent variable\n    for i in range(len(inds) - 1):\n        p.add_layout(\n            bokeh.models.BoxAnnotation(\n                left=t[inds[i]],\n                right=t[inds[i + 1]],\n                fill_alpha=0.2,\n                fill_color=palette[z_vals[i]],\n                line_alpha=0.0,\n            )\n        )\n\n    # Add trace in black\n    p.line(t, y, line_color=\"black\")\n\n    return p\n\nWe will use the MAP estimate of the latent variables for each time point to color the plot.\n\n# Compute most probable latent variable values from samples\nz_map, _ = st.mode(samples.posterior.z.stack(dict(sample=['chain', 'draw'])), axis=1)\n\n# Make the plot!\nbokeh.io.show(plot_trace(t, y, z_map, palette=['#7fc97f', '#beaed4', '#fdc086']))",
    "crumbs": [
      "Hidden Markov models",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hidden Markov models</span>"
    ]
  },
  {
    "objectID": "lessons/HMM/hmm_gaussian_emission.html#map-estimation-of-an-hmm",
    "href": "lessons/HMM/hmm_gaussian_emission.html#map-estimation-of-an-hmm",
    "title": "51  Hidden Markov models",
    "section": "51.3 MAP estimation of an HMM",
    "text": "51.3 MAP estimation of an HMM\nAs usual, MCMC gives a full picture of the posterior distribution, but at a cost. We saw that our sampler ran for a few minutes to get the samples. We may wish to get a MAP estimate for the parameters.\n\n51.3.1 MAP parameters with Stan’s optimizer\nWe could directly acquire those estimates using Stan’s built-in optimizer.\n\n# Perform the optimization\nmap_params = sm3.optimize(\n    data=data, inits=dict(mu=mu_init, sigma=sigma_init)\n).optimized_params_dict\n\n01:12:10 - cmdstanpy - INFO - Chain [1] start processing\n01:12:12 - cmdstanpy - INFO - Chain [1] done processing\n\n\nIndeed, this works, as we see when we pull out the MAP values it computed.\n\nfor key, val in map_params.items():\n    if 'mu' in key or 'sigma' in key or 'Gamma[' in key:\n        print(key + ': ' + str(val))\n\nmu[1]: 1.00618\nmu[2]: 5.01065\nmu[3]: 28.9742\nsigma[1]: 0.223921\nsigma[2]: 1.22235\nsigma[3]: 5.69748\nGamma[1,1]: 0.977369\nGamma[2,1]: 0.00817876\nGamma[3,1]: 0.0168502\nGamma[1,2]: 0.0219174\nGamma[2,2]: 0.989821\nGamma[3,2]: 0.0380309\nGamma[1,3]: 0.000713258\nGamma[2,3]: 0.00200039\nGamma[3,3]: 0.945119\n\n\n\n\n51.3.2 HMMs with hmmlearn\nThe hmmlearn package provides a convenient scikit-learn-like interface for working with HMMs. It computes MAP estimates of HMMs with Categorial, multi- or univariate Gaussian, multi- or univariate Gaussian mixture model, Multinomial, or Poisson emission distributions. It uses clever marginalization and EM-based techniques to do so. hmmlearn does not properly handle priors despite confusing use of the word “prior” in the documentation (see, e.g., this issue). It only find maximizing parameters in the maximum likelihood sense, i.e., with uniform uninformative priors.\nLet use now attempt to get MAP (MAP-ish, since it is really a MLE) parameters.\n\n# Instantiate model\nmodel = hmm.GaussianHMM(n_components=3, covariance_type='full', n_iter=100)\n\n# Get optimal parameters\nmodel.fit(y[:, np.newaxis])\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: divide by zero encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: overflow encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:237: RuntimeWarning: invalid value encountered in matmul\n  current_pot = closest_dist_sq @ sample_weight\n\n\nGaussianHMM(covariance_type='full', n_components=3, n_iter=100)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianHMMiFittedGaussianHMM(covariance_type='full', n_components=3, n_iter=100) \n\n\nNow that the model is fit, we can fetch the optimal parameter values. Of interest, we can get the following values.\n\nLocation parameters of emission distribution, \\(\\mu\\): model.means_\nScale parameters of emission distribution, \\(\\sigma\\): model.covars_\nTransition matrix, \\(\\mathsf{\\Gamma}\\): model.transmat_\nLatent variables, \\(z\\): model.predict(y[:, np.newaxis])\n\nFor comparison with our MCMC result, let’s look at the location parameters.\n\nmodel.means_\n\narray([[ 3.89365152],\n       [29.97672674],\n       [20.0183887 ]])\n\n\nNot the same order, but essentially the same values. This is because we have lots of data so the priors are overwhelmed and the maximum likelihood estimate is close to the MAP.\n\nbebi103.stan.clean_cmdstan()",
    "crumbs": [
      "Hidden Markov models",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hidden Markov models</span>"
    ]
  },
  {
    "objectID": "lessons/HMM/hmm_gaussian_emission.html#computing-environment",
    "href": "lessons/HMM/hmm_gaussian_emission.html#computing-environment",
    "title": "51  Hidden Markov models",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,cmdstanpy,arviz,sklearn,hmmlearn,daft,bebi103,bokeh,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\npolars    : 1.27.1\ncmdstanpy : 1.2.5\narviz     : 0.21.0\nsklearn   : 1.6.1\nhmmlearn  : 0.3.3\ndaft      : 0.1.4\nbebi103   : 0.1.27\nbokeh     : 3.6.2\njupyterlab: 4.4.3\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Hidden Markov models",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hidden Markov models</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm.html",
    "href": "lessons/GLM/glm.html",
    "title": "Generalized linear models",
    "section": "",
    "text": "In this section we introduce a widely used class of models called generalized linear models.",
    "crumbs": [
      "Generalized linear models"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_intro.html",
    "href": "lessons/GLM/glm_intro.html",
    "title": "52  Generalized linear models: An introduction",
    "section": "",
    "text": "52.1 An example: Multinomial logistic regression\nImagine the following neural decoding problem. I measure the activity of several neurons, possibly by measuring their spiking rate, fluorescence with calcium imaging, or some other measure of activity. I simultaneously measure a behavior, which I will call \\(y\\). This could be a categorical description of behavior (like “sniff,” “attack”, “mount,” “ignore”), or some continuous description, like applied force by the musculature. I want to determine how neuronal activity is linked to behavior with a mathematical model. I then may want to use that model to predict behavior from measured neuronal activity.\nLet’s think about this mathematically. We define by \\(\\mathbf{x}\\) to be a vector of neuronal activities. That is, \\(x_1\\) is the instantaneous activity of neuron 1, \\(x_2\\) is that of neuron 2, etc. The observed behavior at any point in time will be some function of \\(\\mathbf{x}\\). More specifically, the probability distribution of the quantitative description of the behavior depends on \\(\\mathbf{x}\\). That is to say that the likelihood, \\(f(y\\mid \\mathbf{x}, \\theta)\\) is conditioned on \\(\\mathbf{x}\\) and some set of parameters \\(\\theta\\). Most generally, we can say this conditioning is based on some function of \\(\\mathbf{x}\\). For reasons that will become clear momentarily, we will define this functional dependence as a composed function \\(\\ell^{-1}(\\xi\n(\\mathbf{x}))\\), where the functions \\(\\ell^{-1}\\) and \\(\\xi\\) may both be parametrized by some subset of the parameters \\(\\theta\\). The function \\(\\ell^{-1}\\) is referred to as the inverse link function, which is sometimes called the mean function.\nAs we turn to the question of how to choose \\(\\xi\\) and \\(\\ell^{-1}\\), it helps to have a concrete example in mind. Imagine that the observed behavior is one of four behaviors, sniff, attack, mount, and ignore, which we can index respectively as 1, 2, 3, and 4.\nHow should we choose \\(\\xi(\\mathbf{x})\\)? We could build the function \\(\\xi(\\mathbf{x})\\) through careful modeling based on physical and biological principles. But, as is often the case, we may have no idea what kind of function to use for \\(\\xi\\)! We can take a page from the physicists’ handbook in this case, and write \\(\\xi(\\mathbf{x})\\) as a Taylor series to first order in \\(\\mathbf{x}\\). For example, if the output of \\(\\xi\\) needs to be a scalar for input into \\(\\ell^{-1}\\), then\n\\[\\begin{align}\n\\xi(\\mathbf{x} ; \\alpha, \\boldsymbol{\\beta}) = \\alpha + \\boldsymbol{\\beta}\\cdot \\mathbf{x}.\n\\end{align}\n\\]\nIn our case, the output needs to be a 4-vector, since \\(\\ell^{-1}(\\xi(\\mathbf{x}))\\) needs to return a simplex of four probabilities. In this case,\n\\[\\begin{align}\n\\xi(\\mathbf{x} ; \\boldsymbol{\\alpha}, \\mathsf{\\beta}) = \\boldsymbol{\\alpha} + \\mathsf{\\beta}\\cdot \\mathbf{x}.\n\\end{align}\n\\]\nHow might we choose \\(\\ell^{-1}\\) then? Because \\(\\xi(\\mathbf{x})\\) is a linear function, its output is unbounded. So, we need a function that converts the unbounded output of \\(\\xi(\\mathbf{x})\\) into a simplex of probabilities. The softmax function is a great choice!. The softmax function is defined as follows. Let \\(\\mathbf{a} = (a_1, a_2, \\ldots)\\). Then,\n\\[\\begin{align}\n\\text{softmax}(\\mathbf{a}) = \\frac{\\left(\\mathrm{e}^{a_1}, \\mathrm{e}^{a_2},\\ldots\\right)^\\mathsf{T}}{\\displaystyle{\\sum_i \\mathrm{e}^{a_i}}}.\n\\end{align}\n\\]\nSo, choosing the softmax function as our inverse link function and considering many i.i.d. measurements \\(y_i\\), we have as our model\n\\[\\begin{align}\n&\\boldsymbol{\\alpha} \\sim \\text{prior for }\\boldsymbol{\\alpha}, \\\\[1em]\n&\\mathsf{\\beta} \\sim \\text{prior for }\\mathsf{\\beta}, \\\\[1em]\n&y_i \\mid \\mathbf{x}_i, \\boldsymbol{\\alpha}, \\mathsf{\\beta} = \\text{Categorical}(\\text{softmax}(\\boldsymbol{\\alpha} + \\mathsf{\\beta}\\cdot \\mathbf{x}_i)) \\;\\forall i.\n\\end{align}\n\\]\nThis model is referred to as a multinomial logistic regression model.",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Generalized linear models: An introduction</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_intro.html#generalized-linear-models",
    "href": "lessons/GLM/glm_intro.html#generalized-linear-models",
    "title": "52  Generalized linear models: An introduction",
    "section": "52.2 Generalized linear models",
    "text": "52.2 Generalized linear models\nThe model we have just described is a special case of a class of models known as generalized linear models, or GLM. The idea behind these models is that the output of a linear function of the input variables \\(\\mathbf{x}\\) is passed into an inverse link function whose output parametrizes the likelihood of observations \\(y\\).[^1]\nSeveral GLMs have special names. In what follows, it is useful to define the standard logistic function, also known as an inverse logit function, which is a special case of the softmax function when there are only two possible values.\n\\[\\begin{align}\n\\text{logit}^{-1}(a) = \\frac{\\mathrm{e}^{a}}{1 + \\mathrm{e}^{a}} = \\frac{1}{1 + \\mathrm{e}^{-a}}.\n\\end{align}\n\\]\nThe likelihoods of some named GLMs are in the table below.\n\n\n\n\n\n\n\n\nGLM name\nlikelihood\ninverse link function\n\n\n\n\nLinear regression\n\\(y\\mid \\mathbf{x}, \\alpha, \\boldsymbol{\\beta}, \\sigma \\sim \\text{Norm}(\\alpha + \\boldsymbol{\\beta} \\cdot \\mathbf{x}, \\sigma)\\)\n\\(\\ell^{-1}(\\eta) = \\eta\\)\n\n\nLogistic regression\n\\(y\\mid \\mathbf{x}, \\alpha, \\boldsymbol{\\beta} \\sim \\text{Bernoulli}(\\text{logit}^{-1}(\\alpha + \\boldsymbol{\\beta} \\cdot \\mathbf{x}))\\)\n\\(\\ell^{-1}(\\eta) = \\text{logit}^{-1}(\\eta)\\)\n\n\nMultinomial logistic regression\n\\(y\\mid \\mathbf{x}, \\alpha, \\boldsymbol{\\beta} \\sim \\text{Categorical}(\\text{softmax}(\\boldsymbol{\\alpha} + \\mathsf{\\beta} \\cdot \\mathbf{x}))\\)\n\\(\\ell^{-1}(\\boldsymbol{\\eta}) = \\text{softmax}(\\boldsymbol{\\eta})\\)\n\n\nBinomial regression\n\\(y \\mid \\mathbf{x}, N, \\alpha, \\boldsymbol{\\beta} \\sim  \\text{Binom}(N, \\text{logit}^{-1}(\\alpha + \\boldsymbol{\\beta}\\cdot\\mathbf{x}))\\)\n\\(\\ell^{-1}(\\eta) = \\text{logit}^{-1}(\\eta)\\)\n\n\nMultinomial regression\n\\(y \\mid \\mathbf{x}, N, \\boldsymbol{\\alpha}, \\mathsf{\\beta} \\sim  \\text{Multinomial}(N, \\text{softmax}(\\boldsymbol{\\alpha} + \\mathsf{\\beta}\\cdot\\mathbf{x}))\\)\n\\(\\ell^{-1}(\\boldsymbol{\\eta}) = \\text{softmax}(\\boldsymbol{\\eta})\\)\n\n\nPoisson regression\n\\(y\\mid \\mathbf{x}, \\alpha, \\boldsymbol{\\beta} \\sim \\text{Poisson}(\\mathrm{e}^{\\alpha + \\boldsymbol{\\beta}\\cdot\\mathbf{x}})\\)\n\\(\\ell^{-1}(\\eta) = \\mathrm{e}^{\\eta}\\)\n\n\nGamma regression\n\\(y\\mid \\mathbf{x}, \\alpha, \\boldsymbol{\\beta}, a \\sim \\text{Gamma}(a, a/\\ell^{-1}(\\alpha + \\boldsymbol{\\beta}\\cdot\\mathbf{x})\\)\n\\(\\ell^{-1}(\\eta) = \\mathrm{e}^{\\eta}\\) or \\(\\ell^{-1}(\\eta) = 1/{\\eta}\\)",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Generalized linear models: An introduction</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_intro.html#hierarchical-glms",
    "href": "lessons/GLM/glm_intro.html#hierarchical-glms",
    "title": "52  Generalized linear models: An introduction",
    "section": "52.3 Hierarchical GLMs",
    "text": "52.3 Hierarchical GLMs\nNote that if the parameters \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\) in the GLM are themselves dependent on hyperparameters, we have a hierarchical GLM. These are quite widely encountered and present the usual challenges of hierarchical models.\n[^1] Strictly speaking, GLMs are defined based on the exponential dispersion family of distributions where the canonical parameters are linear functions of the input variables \\(\\mathbf{x}\\), but I do not want to get too tied down with what qualifies as a GLM. I prefer to think generatively and develop the most appropriate models for my applications, which many times may end up being GLMs.",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Generalized linear models: An introduction</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_in_practice.html",
    "href": "lessons/GLM/glm_in_practice.html",
    "title": "53  GLMs applied to neurons and aggression",
    "section": "",
    "text": "53.1 Centering and scaling\n| Download notebook\nIn this lesson, I will demonstrate the use of a GLM in practice. Specifically, we will perform a logistic regression using the data from Remedios, et al. that we have already encountered in ?sec-matlab-files.\nNow that we are modeling these data in earnest, it is important to note some facts from the paper about the data set.\nWe have already loaded and visualized the data set in ?sec-matlab-files. I will expose the code to load in the data set, but hide the visualization code.\nClearly, the neuronal activity and attack behavior changes when presented with a male. We could model the neuronal activity based on sex presentation, but that is not our task here. Here, we will model how neuronal activity affects attack behavior.\nAs we approach our analysis, it is wise to first check that the data are as we think. If the neural data are fluorescence difference relative to the mean, then the mean signal (averaged over all time points) of each neuron should be zero. Let’s check.\nneural_data.mean(axis=0)\n\narray([-0.50595541,  2.80241833,  0.65463783, -0.05233957,  1.91911147,\n        1.05586614,  0.36410999,  0.87441479,  0.43653782,  2.80454759,\n        1.8669907 , -0.91001794, -2.39086245,  2.36254721,  0.60409699,\n        0.54337706,  3.85461084, -1.23488834, -0.23093228,  1.96520683,\n        0.20655417, -1.55486544,  1.59377787,  0.76639594,  1.07599103,\n        1.67454872, -2.15025706, -1.59622908, -1.09768275,  2.07154601,\n        0.23631976,  1.47605528,  0.56212051, -0.97385873,  1.10629566,\n        0.57570062, -0.79088113, -0.97003822,  0.73574308,  1.58055879,\n        2.15372034,  0.80003251, -0.4833639 , -0.40458862,  0.59728615,\n        0.28368438, -0.9700867 , -0.38844875,  0.62099988, -0.23944945,\n       -0.44615113, -0.7285588 ,  1.06474938, -0.31719077, -0.30342351,\n       -1.3040946 ,  0.64354608,  0.58876965,  0.19892946, -0.37661813,\n       -0.82218469,  0.98216878,  0.60068407,  0.62474366, -0.80829488,\n        0.47448697,  0.35872685,  0.360741  , -0.45829286,  0.67356388,\n        0.24463406,  0.01289178, -1.00545236, -0.28845844, -0.68860872,\n        1.19482826,  0.82365939, -0.80095742,  0.79133022, -1.36316494,\n       -0.01224409,  0.88449665, -0.51000284,  0.19967641,  0.4952818 ,\n        0.31396237, -1.31430482, -0.48578166, -1.70865534,  0.1779136 ,\n        0.01354794, -0.1001097 ,  0.63450004, -0.64862253, -0.62575994,\n       -0.35120753,  1.24988798, -0.00664137, -0.10881931, -0.39084366,\n        0.572822  ,  0.06283101, -1.74370732,  0.52895888,  1.2437877 ,\n        0.31988937, -0.87772531, -1.12787452,  0.14820647,  0.77293784,\n       -0.36212311,  0.02396753, -0.20294646,  0.37525496,  0.61183772])\nClearly, this is not the case. The methods section of the paper discusses some post processing after segmentation, but it is not clear how that was done. We will therefore center and scale the data, centering by the mean of all measurements and scaling by the standard deviation of all measurements.\n# Centered-and-scaled neuronal data\nX = (neural_data - neural_data.mean()) / neural_data.std()",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>GLMs applied to neurons and aggression</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_in_practice.html#logistic-regression-model",
    "href": "lessons/GLM/glm_in_practice.html#logistic-regression-model",
    "title": "53  GLMs applied to neurons and aggression",
    "section": "53.2 Logistic regression model",
    "text": "53.2 Logistic regression model\nNow, we will proceed with modeling.\nOur goal here is to model how neural activity leads to attack behavior. We will model the probability of a mouse being in attack mode based only on instantaneous neuronal activity. That is, we will not explicitly model any temporal behavior.\nLet \\(\\theta\\) be the probability of a mouse being in attack mode. Let \\(\\mathbf{x}\\) be a vector where each entry is a quantitation of the activity of a neuron; each entry corresponds to a separate neuron. So, \\(\\theta = \\theta(\\mathbf{x})\\). We denote by \\(f(\\mathbf{x})\\) the log odds ratio of being in attack mode,\n\\[\\begin{align}\nf(\\mathbf{x}) = \\ln\\left(\\frac{\\theta(\\mathbf{x})}{1 - \\theta(\\mathbf{x})}\\right) \\equiv \\mathrm{logit}(\\theta(\\mathbf{x})),\n\\end{align}\n\\]\nwhere the odds ratio is \\(\\theta / (1 - \\theta)\\), and varies from zero to infinity. The log odds ratio is referred to as a logit function and its value can take any real value. We can invert this function to get\n\\[\\begin{align}\n\\theta(\\mathbf{x}) = \\mathrm{logit}^{-1}(f(\\mathbf{x})) = \\frac{1}{1 + \\mathrm{e}^{-f(\\mathbf{x})}}.\n\\end{align}\n\\]\nWe are left to model the log odds ratio as a function of neuronal activity. As is often a useful strategy when we do not have a specific theoretical in mind, we assume \\(f(\\mathbf{x})\\) is continuous with continuous derivatives and may be approximated as a Taylor series about \\(\\mathbf{x} = 0\\),\n\\[\\begin{align}\nf(\\mathbf{x}) = \\alpha + \\boldsymbol{\\beta}\\cdot \\mathbf{x} + \\frac{1}{2}\\,\\mathbf{x}^\\mathsf{T}\\cdot\\mathsf{B}\\cdot \\mathbf{x} + \\ldots ,\n\\end{align}\n\\]\nwhere \\(\\alpha\\) is scalar valued, \\(\\beta\\) is vector valued, \\(\\mathsf{B}\\) is matrix valued, etc. We will truncate the Taylor expansion to first order such that\n\\[\\begin{align}\nf(\\mathbf{x}) \\approx \\alpha + \\boldsymbol{\\beta}\\cdot \\mathbf{x}.\n\\end{align}\n\\]\nNote that by neglecting the higher order terms, we have neglected all interdependence of the neurons. This is in general the case of GLMs. Since the output is a linear function of the inputs \\(\\mathbf{x}\\), any interdependence of the entries in \\(\\mathbf{x}\\) is not modeled. Thus, our theoretical model for the probability of a mouse being in attack mode is\n\\[\\begin{align}\n\\theta(\\mathbf{x}) = \\frac{1}{1 + \\mathrm{e}^{\\alpha + \\boldsymbol{\\beta}\\cdot \\mathbf{x}}}.\n\\end{align}\n\\]\nSay at time \\(t_i\\), a mouse has neuronal activity \\(\\mathbf{x}_i\\) and probability \\(\\theta_i\\) of being in attack mode. Let \\(a_i\\) be one if the mouse is observed to be in attack mode at time point \\(t_i\\) and zero if the mouse is not in attack mode. Then, we have\n\\[\\begin{align}\na_i \\sim \\text{Bernoulli}(\\theta_i)\\;\\forall i.\n\\end{align}\n\\]\nWe will take the centered-and-scaled relative fluorescence of a neuron in calcium imaging as a quantitation of its activity. We neglect all time dependence, and assume that each \\(\\theta\\) is i.i.d. and each \\(a_i\\) is also i.i.d. This completes the specification of the likelihood,\n\\[\\begin{align}\n&\\theta_i = \\mathrm{logit}^{-1}\\left(\\alpha + \\boldsymbol{\\beta}\\cdot \\mathbf{x}_i\\right)\\;\\forall i,\\\\[1em]\n&a_i \\sim \\text{Bernoulli}(\\theta_i)\\;\\forall i.\n\\end{align}\n\\]\nNext, we need to specify the priors for \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\). We have centered and scaled the data, so we do not expect big variation in the orders of magnitude of the parameters. However, I am not really sure what their scale should be a priori. I will therefore choose a zero-mean Normal distribution with an inferred scale parameter, which I shall call \\(\\tau\\). I will then choose a weakly informative HalfNormal prior for \\(\\tau\\). Note that this choice of priors is sometimes referred to as a regularization, specifically ridge regularization. In this context, \\(\\tau\\) is usually expressed as \\(\\lambda \\equiv 1/\\tau^2\\), with \\(\\lambda\\) being referred to as the sparcity parameter. Our complete model is then\n\\[\\begin{align}\n&\\tau \\sim \\text{Norm}(0, 100),\\\\[1em]\n&\\alpha \\sim \\text{Norm}(0, \\tau),\\\\[1em]\n&\\beta_i \\sim \\text{Norm}(0, \\tau) \\; \\forall i,\\\\[1em]\n&\\theta_i = \\mathrm{logit}^{-1}\\left(\\alpha + \\boldsymbol{\\beta}\\cdot \\mathbf{x}_i\\right)\\;\\forall i,\\\\[1em]\n&a_i \\sim \\text{Bernoulli}(\\theta_i)\\;\\forall i.\n\\end{align}\n\\]\nWe can code this up in a Stan model. We could code our model as follows.\ndata {\n  int N_neurons;\n  int N_time_points;\n  array[N_time_points] int a;\n  matrix[N_time_points, N_neurons] X;\n}\n\n\nparameters {\n  real alpha;\n  vector[N_neurons] beta_;\n\n  // Inverse sparcity parameter\n  real&lt;lower=0&gt; tau;\n}\n\n\ntransformed parameters {\n  // Sparcity parameter as typically defined\n  real lambda = 1.0 / tau^2;\n\n  // Probability of being in attack state\n  vector[N_time_points] theta = inv_logit(alpha + X * beta_);\n}\n\n\nmodel {\n  alpha ~ normal(0, tau);\n  beta_ ~ normal(0, tau);\n  a ~ bernoulli(theta);\n}\nFortunately, though, Stan has allows for a Bernoulli distribution with a logit-based generalized linear model, which is a fancy phrase for what we have just done. So, we use the following Stan code.\ndata {\n  int N_neurons;\n  int N_time_points;\n  array[N_time_points] int a;\n  matrix[N_time_points, N_neurons] X;\n}\n\n\nparameters {\n  real alpha;\n  vector[N_neurons] beta_;\n\n  // Inverse sparcity parameter\n  real&lt;lower=0&gt; tau;\n}\n\n\ntransformed parameters {\n  // Sparcity parameter as typically defined\n  real lambda = 1.0 / tau^2;\n}\n\n\nmodel {\n  tau ~ normal(0, 100);\n  alpha ~ normal(0, tau);\n  beta_ ~ normal(0, tau);\n  a ~ bernoulli_logit_glm(X, alpha, beta_);\n}\nLet’s compile and sample! From experience with this model, I know that I need to increase the maximum tree depth for the no U-turn recursion beyond the default of 10.\n\n# Data dictionary to send to Stan\ndata = dict(\n    a=attack_vector,\n    X=X,\n    N_time_points=X.shape[0],\n    N_neurons=X.shape[1],\n)\n\n# Compile\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file='logistic_ridge.stan')\n\n# For convenience, if I have already saved the samples, keep them, otherwise sample\ntry:\n    samples = az.from_netcdf('full.nc')\nexcept:\n    with bebi103.stan.disable_logging():\n        samples = az.from_cmdstanpy(sm.sample(data=data, max_treedepth=15))\n    samples.to_netcdf('full.nc')\n\nAs usual, we inspect the health of the sampler by checking diagnostics.\n\nbebi103.stan.check_all_diagnostics(samples, max_treedepth=15)\n\nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 15.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\n\n\nLooks good!\nNow, we can investigate the results. To start with, let’s make a corner plot of the sparcity parameter, the parameter \\(\\alpha\\), and the first few \\(\\beta\\)’s. I find it easier to reason about the sparcity parameter in terms of \\(\\tau\\) and not \\(\\lambda\\), so we will use that.\n\nbokeh.io.show(\n    bebi103.viz.corner(samples, parameters=[\"tau\", \"alpha\"] + [f\"beta_[{i}]\" for i in range(4)])\n)\n\n\n  \n\n\n\n\n\nThe sparcity parameter is about 10, allowing for variation in parameter values. Nonetheless, it is of interest that the data must be quite informative, as, for example, \\(\\alpha\\) is far outside its prescribed prior range.\nLet us now plot the 95% credible intervals of all of the \\(\\beta\\)’s. We will first sort them by the median posterior value for each of visualization.\n\n# Compute credible interval\ncred_int_beta = samples.posterior.beta_.quantile(\n    q=[0.0275, 0.5, 0.975], dim=[\"chain\", \"draw\"]\n).values.transpose()\n\nneuron_inds = cred_int_beta[:, 1].argsort()\n\nsummaries = [\n    dict(label=str(i), estimate=c[1], conf_int=[c[0], c[2]])\n    for i, c in zip(neuron_inds, cred_int_beta[neuron_inds, :])\n]\n\np_cred_ints = bebi103.viz.confints(\n    summaries, x_axis_label=\"β\", y_axis_label=\"neuron\", frame_height=1000\n)\nbokeh.io.show(p_cred_ints)\n\n\n  \n\n\n\n\n\nNote that the credible interval for a parameter that is uninformed by the data (essentially the middle 95% of a standard normal) is centered at zero and goes roughly from -1.96 to 1.96. Neurons 51 and 87 show roughly this. Interestingly, a great many of the contributions to the attack behavior of specific neurons are in fact informed by the data. Note that the parameter \\(\\alpha\\) is small, about \\(-50\\), which means that the non-attack state is the “ground state” of quiescent neurons.\nLet us now do a posterior predictive check to see if this model and predict when a mouse will be in an attack state. To do so, we compute \\(\\theta\\) for each time point for each set of parameter values. We then compute a credible region for \\(\\theta\\) and compare to the observed attack times.\n\ndef inv_logit_glm(X, alpha, beta):\n    return 1 / (1 + np.exp(-alpha - np.dot(X, beta)))\n\n\n# Compute theta\ntheta_full = np.array(\n    [\n        inv_logit_glm(X, alpha, beta)\n        for alpha, beta in zip(\n            samples.posterior.alpha.stack({\"sample\": (\"chain\", \"draw\")}).values,\n            samples.posterior.beta_.stack(\n                {\"sample\": (\"chain\", \"draw\")}\n            ).values.transpose(),\n        )\n    ]\n)\n\n# Confidence region\ntheta_full = np.percentile(theta_full, [2.5, 50, 97.5], axis=0)\n\n# Make plot\np_pred = bokeh.plotting.figure(\n    frame_height=150,\n    frame_width=600,\n    x_axis_label=\"time (s)\",\n    y_axis_label=\"attack prob\",\n)\np_pred.scatter(t, attack_vector, size=2, color=attack_color)\nbebi103.viz.fill_between(\n    t,\n    theta_full[0, :],\n    t,\n    theta_full[2, :],\n    patch_kwargs=dict(alpha=0.3),\n    show_line=False,\n    p=p_pred,\n)\np_pred.line(t, theta_full[1, :])\n\nbokeh.io.show(p_pred)\n\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_51710/548793961.py:2: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-alpha - np.dot(X, beta)))\n\n\n\n  \n\n\n\n\n\nTo properly view the plot, it is important to zoom in at regions of interest. This looks quite good. There are almost no false positives for attack, and real attack events happen when \\(\\theta\\) is substantial.",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>GLMs applied to neurons and aggression</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_in_practice.html#cross-validation",
    "href": "lessons/GLM/glm_in_practice.html#cross-validation",
    "title": "53  GLMs applied to neurons and aggression",
    "section": "53.3 Cross validation",
    "text": "53.3 Cross validation\nThe posterior distribution encodes all of our knowledge about the parameters. Nonetheless, it is useful to perform a cross validation. We will therefore refit the model omitting half of the data points (omitted data points chosen at random, since we are neglecting time-dependence) and then check the posterior distribution of the parameter values we get and predictive checks against the entire data set.\nFirst, let’s make a training set, which is half of the data.\n\n# Boolean array of data points to include\nrng = np.random.default_rng(seed=3252)\ninds = np.array([True]*(len(t) // 2 + len(t) % 2) + [False] * (len(t) // 2))\nrng.shuffle(inds)\n\n# Split into test and train\nX_train = X[inds, :]\nX_test = X[~inds, :]\na_train = attack_vector[inds]\na_test = attack_vector[~inds]\n\nNow, we can get samples from the model using only the training data set.\n\ndata_train = dict(\n    a=a_train,\n    X=X_train,\n    N_time_points=X_train.shape[0],\n    N_neurons=X_train.shape[1],\n)\n\n# For convenience, if I have already saved the samples, keep them, otherwise sample\ntry:\n    samples_train = az.from_netcdf('train.nc')\nexcept:\n    with bebi103.stan.disable_logging():\n        samples_train = az.from_cmdstanpy(\n            sm.sample(data=data_train, max_treedepth=15, iter_sampling=2000, thin=2)\n        )\n    samples_train.to_netcdf('train.nc')\n\nWe of course perform the usual diagnostic checks.\n\nbebi103.stan.check_all_diagnostics(samples_train, max_treedepth=15)\n\nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 15.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\n\n\nLooks good! Again, we’ll make a corner plot with the sparcity parameter, \\(\\alpha\\), and the first few \\(\\beta\\)’s.\n\nbokeh.io.show(\n    bebi103.viz.corner(samples_train, parameters=[\"tau\", \"alpha\"] + [f\"beta_[{i}]\" for i in range(4)])\n)\n\n\n  \n\n\n\n\n\nInterestingly, the sparcity parameter is larger (\\(\\tau\\) is smaller), and \\(\\alpha\\) is smaller as well. This could be due to fewer data failing to overwhelm the prior to be informative. As a graphical display, let us compare the credible intervals for \\(\\alpha\\) from the full data set and from the training set.\n\nsummaries = [\n    dict(label='full', estimate=float(samples.posterior.alpha.median()),\n    conf_int=samples.posterior.alpha.quantile([0.0275, 0.975]).values),\n    dict(label='train', estimate=float(samples_train.posterior.alpha.median()),\n    conf_int=samples_train.posterior.alpha.quantile([0.0275, 0.975]).values),\n]\n\nbokeh.io.show(bebi103.viz.confints(summaries, x_axis_label='α'))\n\n\n  \n\n\n\n\n\nNow, we will overlay the credible intervals from the model fit with the training data with that of the entire data set.\n\n# Compute credible intervals\ncred_int_beta_train = samples_train.posterior.beta_.quantile(\n    q=[0.0275, 0.5, 0.975], dim=[\"chain\", \"draw\"]\n).values.transpose()\n\n# Overlay with full model\nneuron_labels = [str(i) for i in neuron_inds]\np_cred_ints.segment(\n    x0=cred_int_beta_train[neuron_inds, 0],\n    y0=neuron_labels,\n    x1=cred_int_beta_train[neuron_inds, 2],\n    y1=neuron_labels,\n    color=\"orange\",\n    line_width=3,\n    alpha=0.5,\n)\np_cred_ints.scatter(\n    x=cred_int_beta_train[neuron_inds, 1],\n    y=neuron_labels,\n    color=\"orange\",\n    size=5,\n    alpha=0.5,\n)\n\nbokeh.io.show(p_cred_ints)\n\n\n  \n\n\n\n\n\nWe do see some systematic variation. For inference based on the training set, \\(|\\beta|\\) tends to be smaller than on values inferred from the entire set. This is consistent with \\(\\alpha\\) also being smaller in magnitude.\nPerhaps the predictions of \\(\\theta\\) based on the inference based on the training set will be different than those based on the full data set. Let’s take a look.\n\n# Compute theta\ntheta_train = np.array(\n    [\n        inv_logit_glm(X, alpha, beta)\n        for alpha, beta in zip(\n            samples_train.posterior.alpha.stack({\"sample\": (\"chain\", \"draw\")}).values,\n            samples_train.posterior.beta_.stack(\n                {\"sample\": (\"chain\", \"draw\")}\n            ).values.transpose(),\n        )\n    ]\n)\n\n# Confidence region\ntheta_train = np.percentile(theta_train, [2.5, 50, 97.5], axis=0)\n\n# Add to plot\nbebi103.viz.fill_between(\n    t,\n    theta_train[0, :],\n    t,\n    theta_train[2, :],\n    patch_kwargs=dict(alpha=0.3, color='orange'),\n    show_line=False,\n    p=p_pred,\n)\np_pred.line(t, theta_train[1, :], color='orange')\n\nbokeh.io.show(p_pred)\n\n\n  \n\n\n\n\n\nThe posterior inferred on only half of the data is still predictive, but with larger credible intervals on the probability of attack.\nThe fact that the model is still predictive with only half of the data points and with different values of the parameters and the wide credible intervals implies that the parameters of the model are not identifiable, at least not quantitatively, but their relative values are.\nWe can try plotting the credible intervals of the ratio of \\(\\beta/|\\alpha|\\) instead; checking the relative scale of \\(\\beta\\) and \\(\\alpha\\).\n\n# Compute credible interval of beta / alpha\nsamples.posterior['beta_alpha'] = samples.posterior.beta_ / np.abs(samples.posterior.alpha)\ncred_int_beta_alpha = samples.posterior.beta_alpha.quantile(\n    q=[0.0275, 0.5, 0.975], dim=[\"chain\", \"draw\"]\n).values.transpose()\n\nsummaries = [\n    dict(label=str(i), estimate=c[1], conf_int=[c[0], c[2]])\n    for i, c in zip(neuron_inds, cred_int_beta_alpha[neuron_inds, :])\n]\n\np_cred_ints_ratio = bebi103.viz.confints(\n    summaries, x_axis_label=\"β / |α|\", y_axis_label=\"neuron\", frame_height=1000\n)\n\n# Add results based on training\nsamples_train.posterior['beta_alpha'] = samples_train.posterior.beta_ / np.abs(samples_train.posterior.alpha)\ncred_int_beta_alpha_train = samples_train.posterior.beta_alpha.quantile(\n    q=[0.0275, 0.5, 0.975], dim=[\"chain\", \"draw\"]\n).values.transpose()\n\n# Overlay with full model\nneuron_labels = [str(i) for i in neuron_inds]\np_cred_ints_ratio.segment(\n    x0=cred_int_beta_alpha_train[neuron_inds, 0],\n    y0=neuron_labels,\n    x1=cred_int_beta_alpha_train[neuron_inds, 2],\n    y1=neuron_labels,\n    color=\"orange\",\n    line_width=3,\n    alpha=0.5,\n)\np_cred_ints_ratio.scatter(\n    x=cred_int_beta_alpha_train[neuron_inds, 1],\n    y=neuron_labels,\n    color=\"orange\",\n    size=5,\n    alpha=0.5,\n)\n\nbokeh.io.show(p_cred_ints_ratio)\n\n\n  \n\n\n\n\n\nNow we see more consistency in the parameters. The parameters inferred from the training set all lie within the credible intervals inferred from the full data set, but with wider credible intervals themselves. Perhaps, then, is is only \\(\\boldsymbol{\\beta}\\) relative to background \\(\\alpha\\) that can be inferred.",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>GLMs applied to neurons and aggression</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_in_practice.html#lasso-regularization",
    "href": "lessons/GLM/glm_in_practice.html#lasso-regularization",
    "title": "53  GLMs applied to neurons and aggression",
    "section": "53.4 LASSO regularization",
    "text": "53.4 LASSO regularization\nLet us now consider another set of priors for \\(\\alpha\\) and \\(\\beta\\) instead of the Normal priors. We will instead use a Laplace prior, otherwise known as a double exponential prior. With this choice of prior, the GLM is referred to as LASSO-regularized logistic regression. The model is then\ndata {\n  int N_neurons;\n  int N_time_points;\n  array[N_time_points] int a;\n  matrix[N_time_points, N_neurons] X;\n}\n\n\nparameters {\n  real alpha;\n  vector[N_neurons] beta_;\n\n  // Inverse sparcity parameter\n  real&lt;lower=0&gt; tau;\n}\n\n\ntransformed parameters {\n  // Sparcity parameter as typically defined\n  real lambda = 1.0 / tau^2;\n}\n\n\nmodel {\n  tau ~ normal(0, 100);\n  alpha ~ double_exponential(0, tau);\n  beta_ ~ double_exponential(0, tau);\n  a ~ bernoulli_logit_glm(X, alpha, beta_);\n}\nLet’s grab samples from this one.\n\nwith bebi103.stan.disable_logging():\n    sm_lasso = cmdstanpy.CmdStanModel(stan_file=\"logistic_lasso.stan\")\n\ntry:\n    samples_lasso = az.from_netcdf(\"lasso.nc\")\nexcept:\n    with bebi103.stan.disable_logging():\n        samples_lasso = az.from_cmdstanpy(\n            sm_lasso.sample(\n                data=data,\n                max_treedepth=15,\n                iter_warmup=2000,\n                iter_sampling=2000,\n                thin=2,\n            )\n        )\n    samples_lasso.to_netcdf('lasso.nc')\n\nWe’ll do the same analysis. Check diagnostics, make a corner plot, plot credible intervals, and check prediction.\n\nbebi103.stan.check_all_diagnostics(samples_lasso, max_treedepth=15)\n\nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 15.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\n\n\nNow, the corner plot.\n\nbokeh.io.show(\n    bebi103.viz.corner(samples_lasso, parameters=[\"tau\", \"alpha\"] + [f\"beta_[{i}]\" for i in range(4)])\n)\n\n\n  \n\n\n\n\n\nWe see a similar value of \\(\\tau\\) as we did the ridge regularization. The parameter \\(\\alpha\\) is much smaller though.\nLet’s check out the \\(\\beta\\) parameters. We’ll overlay them on our credible intervals from ridge regularization, with the results from LASSO regularization in red.\n\n# Compute credible intervals\ncred_int_beta_lasso = samples_lasso.posterior.beta_.quantile(\n    q=[0.0275, 0.5, 0.975], dim=[\"chain\", \"draw\"]\n).values.transpose()\n\n# Overlay with full model\nneuron_labels = [str(i) for i in neuron_inds]\np_cred_ints.segment(\n    x0=cred_int_beta_lasso[neuron_inds, 0],\n    y0=neuron_labels,\n    x1=cred_int_beta_lasso[neuron_inds, 2],\n    y1=neuron_labels,\n    color=\"tomato\",\n    line_width=3,\n    alpha=0.5,\n)\np_cred_ints.scatter(\n    x=cred_int_beta_lasso[neuron_inds, 1],\n    y=neuron_labels,\n    color=\"tomato\",\n    size=5,\n    alpha=0.5,\n)\n\nbokeh.io.show(p_cred_ints)\n\n\n  \n\n\n\n\n\nThe LASSO results follow the same trend, but with some broader confidence intervals and with some neurons having extreme values.\nFinally, let’s check prediction.\n\n# Compute theta\ntheta_lasso = np.array(\n    [\n        inv_logit_glm(X, alpha, beta)\n        for alpha, beta in zip(\n            samples_lasso.posterior.alpha.stack({\"sample\": (\"chain\", \"draw\")}).values,\n            samples_lasso.posterior.beta_.stack(\n                {\"sample\": (\"chain\", \"draw\")}\n            ).values.transpose(),\n        )\n    ]\n)\n\n# Confidence region\ntheta_lasso = np.percentile(theta_lasso, [2.5, 50, 97.5], axis=0)\n\n# Add to plot\nbebi103.viz.fill_between(\n    t,\n    theta_lasso[0, :],\n    t,\n    theta_lasso[2, :],\n    patch_kwargs=dict(alpha=0.3, color='tomato'),\n    show_line=False,\n    p=p_pred,\n)\np_pred.line(t, theta_lasso[1, :], color='tomato')\n\nbokeh.io.show(p_pred)\n\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_51710/548793961.py:2: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-alpha - np.dot(X, beta)))\n\n\n\n  \n\n\n\n\n\nThe predictions from the model with LASSO regularization are very similar to those with ridge regularization, which is not surprising since most of the parameter values are similar for the two models.\nAs a final check, let’s perform parameter inference with LASSO regularization using only the training data.\n\ntry:\n    samples_lasso_train = az.from_netcdf(\"train_lasso.nc\")\nexcept:\n    with bebi103.stan.disable_logging():\n        sm_lasso = cmdstanpy.CmdStanModel(stan_file=\"logistic_lasso.stan\")\n        samples_lasso_train = az.from_cmdstanpy(\n            sm_lasso.sample(\n                data=data_train,\n                max_treedepth=15,\n                iter_warmup=2000,\n                iter_sampling=2000,\n                thin=2,\n            )\n        )\n    samples_lasso_train.to_netcdf('train_lasso.nc')\n\nAnd, we’ll proceed with the usual procedures.\n\nbebi103.stan.check_all_diagnostics(samples_lasso_train, max_treedepth=15)\n\nEffective sample size looks reasonable for all parameters.\n\nRhat looks reasonable for all parameters.\n\n0 of 4000 (0.0%) iterations ended with a divergence.\n\n0 of 4000 (0.0%) iterations saturated the maximum tree depth of 15.\n\nE-BFMI indicated no pathological behavior.\n\n\n0\n\n\n\nbokeh.io.show(\n    bebi103.viz.corner(samples_lasso_train, parameters=[\"tau\", \"alpha\"] + [f\"beta_[{i}]\" for i in range(4)])\n)\n\n\n  \n\n\n\n\n\nWe have a smaller \\(\\tau\\) and also a smaller \\(\\alpha\\) compared to when we used the full data set. We also saw this with the analysis with ridge regularization.\n\n# Compute credible intervals\ncred_int_beta_lasso_train = samples_lasso_train.posterior.beta_.quantile(\n    q=[0.0275, 0.5, 0.975], dim=[\"chain\", \"draw\"]\n).values.transpose()\n\n# Overlay with full model\nneuron_labels = [str(i) for i in neuron_inds]\np_cred_ints.segment(\n    x0=cred_int_beta_lasso_train[neuron_inds, 0],\n    y0=neuron_labels,\n    x1=cred_int_beta_lasso_train[neuron_inds, 2],\n    y1=neuron_labels,\n    color=\"purple\",\n    line_width=3,\n    alpha=0.5,\n)\np_cred_ints.scatter(\n    x=cred_int_beta_lasso_train[neuron_inds, 1],\n    y=neuron_labels,\n    color=\"purple\",\n    size=5,\n    alpha=0.5,\n)\n\nbokeh.io.show(p_cred_ints)\n\n\n  \n\n\n\n\n\n\n# Compute theta\ntheta_lasso_train = np.array(\n    [\n        inv_logit_glm(X, alpha, beta)\n        for alpha, beta in zip(\n            samples_lasso_train.posterior.alpha.stack({\"sample\": (\"chain\", \"draw\")}).values,\n            samples_lasso_train.posterior.beta_.stack(\n                {\"sample\": (\"chain\", \"draw\")}\n            ).values.transpose(),\n        )\n    ]\n)\n\n# Confidence region\ntheta_lasso_train = np.percentile(theta_lasso_train, [2.5, 50, 97.5], axis=0)\n\n# Add to plot\nbebi103.viz.fill_between(\n    t,\n    theta_lasso_train[0, :],\n    t,\n    theta_lasso_train[2, :],\n    patch_kwargs=dict(alpha=0.3, color='purple'),\n    show_line=False,\n    p=p_pred,\n)\np_pred.line(t, theta_lasso_train[1, :], color='purple')\n\nbokeh.io.show(p_pred)\n\n\n  \n\n\n\n\n\nApparently, with the performance of the LASSO regularization with the training set is better than that of the ridge regularization.",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>GLMs applied to neurons and aggression</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_in_practice.html#sec-glm-scikit-learn",
    "href": "lessons/GLM/glm_in_practice.html#sec-glm-scikit-learn",
    "title": "53  GLMs applied to neurons and aggression",
    "section": "53.5 GLM with scikit-learn",
    "text": "53.5 GLM with scikit-learn\nScikit-learn offers a convenient interface into a subset of GLMs via its sklearn.linear_model module. You can find the MAP parameter values for \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\). Importantly, though, for some GLMs, only ridge regularization is available. For all functions, the sparcity parameter must be specified (with a default value of 1). That is, the sparcity parameter is not inferred, but conferred.\nLet’s put it to use! We first instantiate a logistic regression instance. We will use ridge regularization, which we specify with the penalty='l2' keyword argument (specifying an L2 norm regularizer, which is equivalent to a Normal prior on the \\(\\alpha\\) and \\(\\boldsymbol{\\beta}\\) values). We will also allow for more iterations when fitting, as I found was necessary for this particular case, using the max_iter keyword argument.\n\nlogreg = sklearn.linear_model.LogisticRegression(penalty='l2', max_iter=1000)\n\nAs usual, to get the MAP estimates of the parameters, we use the fit() method.\n\nlogreg.fit(X, attack_vector)\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: divide by zero encountered in matmul\n  raw_prediction = X @ weights + intercept\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: overflow encountered in matmul\n  raw_prediction = X @ weights + intercept\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in matmul\n  raw_prediction = X @ weights + intercept\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: divide by zero encountered in matmul\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: overflow encountered in matmul\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(max_iter=1000) \n\n\nWe can access the MAP value of \\(\\alpha\\) via the logreg.intercept_ attribute and the MAP value of \\(\\boldsymbol{\\beta}\\) via the logreg.coef_ attribute. Conveniently, we can access the predicted attack probability using logreg.predict_proba(X).\nTo check how well we did with our regression, we will add the attack probability to the above plot, this time with a single green line.\n\np_pred.line(t, logreg.predict_proba(X)[:, 1], color='green')\n\nbokeh.io.show(p_pred)\n\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n/Users/bois/miniconda3/envs/datasai/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n\n\n\n  \n\n\n\n\n\nWe unsurprisingly have good predictions; the data are informative!\nAs a final now, the table below shows which classes in the sklearn.linear_model module offer which named GLMs.\n\n\n\n\n\n\n\nGLM name\nscikit-learn access\n\n\n\n\nLinear regression\nLinearRegression, Ridge, Lasso\n\n\nLogistic regression\nLogisticRegression\n\n\nMultinomial logistic regression\nLogisticRegression (number of categories inferred based on input to fit())\n\n\nBinomial regression\n—\n\n\nMultinomial regression\n—\n\n\nPoisson regression\nPoissonRegressor\n\n\nGamma regression\nGammaRegressor (only uses exponential inverse link function)",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>GLMs applied to neurons and aggression</span>"
    ]
  },
  {
    "objectID": "lessons/GLM/glm_in_practice.html#computing-environment",
    "href": "lessons/GLM/glm_in_practice.html#computing-environment",
    "title": "53  GLMs applied to neurons and aggression",
    "section": "53.6 Computing environment",
    "text": "53.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,sklearn,cmdstanpy,arviz,bebi103,bokeh,jupyterlab\nprint(\"cmdstan   :\", bebi103.stan.cmdstan_version())\n\nPython implementation: CPython\nPython version       : 3.12.11\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.3\nsklearn   : 1.6.1\ncmdstanpy : 1.2.5\narviz     : 0.21.0\nbebi103   : 0.1.27\nbokeh     : 3.6.2\njupyterlab: 4.3.7\n\ncmdstan   : 2.36.0",
    "crumbs": [
      "Generalized linear models",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>GLMs applied to neurons and aggression</span>"
    ]
  },
  {
    "objectID": "exercises/polars_exercises/polars_exercises.html",
    "href": "exercises/polars_exercises/polars_exercises.html",
    "title": "Polars and split-apply-combine exercises",
    "section": "",
    "text": "Brrrrr. It’s time to do some Polars exercises!",
    "crumbs": [
      "Polars and split-apply-combine exercises"
    ]
  },
  {
    "objectID": "exercises/polars_exercises/frog_tongue_1.html",
    "href": "exercises/polars_exercises/frog_tongue_1.html",
    "title": "54  Mastering selection and filtering of data frames",
    "section": "",
    "text": "| Download notebook\nData set download\n\nWe will work with a data set from Kleinteich and Gorb, Sci. Rep., 4, 5355, 2014, and was featured in the New York Times. They measured several properties about the tongue strikes of horned frogs. Let’s take a look at the data set, which is in the file ~/git/data/frog_tongue_adhesion.csv.\n\n!head -20 ../data/frog_tongue_adhesion.csv\n\n# These data are from the paper,\n#   Kleinteich and Gorb, Sci. Rep., 4, 5225, 2014.\n# It was featured in the New York Times.\n#    http://www.nytimes.com/2014/08/25/science/a-frog-thats-a-living-breathing-pac-man.html\n#\n# The authors included the data in their supplemental information.\n#\n# Importantly, the ID refers to the identifites of the frogs they tested.\n#   I:   adult, 63 mm snout-vent-length (SVL) and 63.1 g body weight,\n#        Ceratophrys cranwelli crossed with Ceratophrys cornuta\n#   II:  adult, 70 mm SVL and 72.7 g body weight,\n#        Ceratophrys cranwelli crossed with Ceratophrys cornuta\n#   III: juvenile, 28 mm SVL and 12.7 g body weight, Ceratophrys cranwelli\n#   IV:  juvenile, 31 mm SVL and 12.7 g body weight, Ceratophrys cranwelli\ndate,ID,trial number,impact force (mN),impact time (ms),impact force / body weight,adhesive force (mN),time frog pulls on target (ms),adhesive force / body weight,adhesive impulse (N-s),total contact area (mm2),contact area without mucus (mm2),contact area with mucus / contact area without mucus,contact pressure (Pa),adhesive strength (Pa)\n2013_02_26,I,3,1205,46,1.95,-785,884,1.27,-0.290,387,70,0.82,3117,-2030\n2013_02_26,I,4,2527,44,4.08,-983,248,1.59,-0.181,101,94,0.07,24923,-9695\n2013_03_01,I,1,1745,34,2.82,-850,211,1.37,-0.157,83,79,0.05,21020,-10239\n2013_03_01,I,2,1556,41,2.51,-455,1025,0.74,-0.170,330,158,0.52,4718,-1381\n2013_03_01,I,3,493,36,0.80,-974,499,1.57,-0.423,245,216,0.12,2012,-3975\n\n\nThe first lines all begin with # signs, signifying that they are comments and not data. They do give important information, though, such as the meaning of the ID data. The ID refers to which specific frog was tested.\nImmediately after the comments, we have a row of comma-separated headers. This row sets the number of columns in this data set and labels the meaning of the columns. So, we see that the first column is the date of the experiment, the second column is the ID of the frog, the third is the trial number, and so on.\nAfter this row, each row represents a single experiment where the frog struck the target. So, these data are already in tidy format.\na) Load in the data set into a data frame. Be sure to use the appropriate value for the comment_prefix keyword argument of pl.read_csv().\nb) Extract the impact time of all impacts that had an adhesive strength of magnitude greater than 2000 Pa. Note: The data in the 'adhesive strength (Pa)' column is all negative. This is because the adhesive force is defined to be negative in the measurement. Without changing the data in the data frame, how can you check that the magnitude (the absolute value) is greater than 2000?\nc) Extract the impact force and adhesive force for all of Frog II’s strikes.\nd) Extract the adhesive force and the time the frog pulls on the target for juvenile frogs (Frogs III and IV). Hint: We saw the & operator for Boolean indexing across more than one column. The | operator signifies OR, and works analogously. For technical reasons that we can discuss if you like, the Python operators and and or will not work for Boolean indexing of data frames. You could also approach this using the is_in() method of a Polars Expression.",
    "crumbs": [
      "Polars and split-apply-combine exercises",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Mastering selection and filtering of data frames</span>"
    ]
  },
  {
    "objectID": "exercises/polars_exercises/frog_tongue_2.html",
    "href": "exercises/polars_exercises/frog_tongue_2.html",
    "title": "55  Split-Apply-Combine of the frog data set",
    "section": "",
    "text": "| Download notebook\nData set download\n\nWe will continue working with the frog tongue adhesion data set.\nYou’ll now practice your split-apply-combine skills. First load in the data set. Then,\na) Compute standard deviation of the impact forces for each frog.\nb) Compute the coefficient of variation of the impact forces and adhesive forces for each frog.\nc) Compute a data frame that has the mean, median, standard deviation, and coefficient of variation of the impact forces and adhesive forces for each frog.",
    "crumbs": [
      "Polars and split-apply-combine exercises",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Split-Apply-Combine of the frog data set</span>"
    ]
  },
  {
    "objectID": "exercises/polars_exercises/frog_tongue_3.html",
    "href": "exercises/polars_exercises/frog_tongue_3.html",
    "title": "56  Adding data to a data frame",
    "section": "",
    "text": "| Download notebook\nData set download\n\n\n# Colab setup ------------------\nimport os, sys, subprocess\nif \"google.colab\" in sys.modules:\n    cmd = \"pip install --upgrade polars bebi103 watermark\"\n    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    data_path = \"https://s3.amazonaws.com/bebi103.caltech.edu/data/\"\nelse:\n    data_path = \"../data/\"\n# ------------------------------\n\nimport polars as pl\n\n\nWe continue working with the frog tongue data. Recall that the header comments in the data file contained information about the frogs.\n\n!head -20 ../data/frog_tongue_adhesion.csv\n\n# These data are from the paper,\n#   Kleinteich and Gorb, Sci. Rep., 4, 5225, 2014.\n# It was featured in the New York Times.\n#    http://www.nytimes.com/2014/08/25/science/a-frog-thats-a-living-breathing-pac-man.html\n#\n# The authors included the data in their supplemental information.\n#\n# Importantly, the ID refers to the identifites of the frogs they tested.\n#   I:   adult, 63 mm snout-vent-length (SVL) and 63.1 g body weight,\n#        Ceratophrys cranwelli crossed with Ceratophrys cornuta\n#   II:  adult, 70 mm SVL and 72.7 g body weight,\n#        Ceratophrys cranwelli crossed with Ceratophrys cornuta\n#   III: juvenile, 28 mm SVL and 12.7 g body weight, Ceratophrys cranwelli\n#   IV:  juvenile, 31 mm SVL and 12.7 g body weight, Ceratophrys cranwelli\ndate,ID,trial number,impact force (mN),impact time (ms),impact force / body weight,adhesive force (mN),time frog pulls on target (ms),adhesive force / body weight,adhesive impulse (N-s),total contact area (mm2),contact area without mucus (mm2),contact area with mucus / contact area without mucus,contact pressure (Pa),adhesive strength (Pa)\n2013_02_26,I,3,1205,46,1.95,-785,884,1.27,-0.290,387,70,0.82,3117,-2030\n2013_02_26,I,4,2527,44,4.08,-983,248,1.59,-0.181,101,94,0.07,24923,-9695\n2013_03_01,I,1,1745,34,2.82,-850,211,1.37,-0.157,83,79,0.05,21020,-10239\n2013_03_01,I,2,1556,41,2.51,-455,1025,0.74,-0.170,330,158,0.52,4718,-1381\n2013_03_01,I,3,493,36,0.80,-974,499,1.57,-0.423,245,216,0.12,2012,-3975\n\n\nSo, each frog has associated with it an age (adult or juvenile), snout-vent-length (SVL), body weight, and species (either cross or cranwelli). For a tidy data frame, we should have a column for each of these values. Your task is to load in the data, and then add these columns to the data frame. For convenience, here is a data frame with data about each frog.\n\ndf_frog = pl.DataFrame(\n    data={\n        \"ID\": [\"I\", \"II\", \"III\", \"IV\"],\n        \"age\": [\"adult\", \"adult\", \"juvenile\", \"juvenile\"],\n        \"SVL (mm)\": [63, 70, 28, 31],\n        \"weight (g)\": [63.1, 72.7, 12.7, 12.7],\n        \"species\": [\"cross\", \"cross\", \"cranwelli\", \"cranwelli\"],\n    }\n)\n\nNote: There are lots of ways to solve this problem. This is a good exercise in searching through the Polars documentation and other online resources, such as Stack Overflow. Remember, much of your programming efforts are spent searching through documentation and the internet.\nFinally, as a fun challenge, see if you can highlight the strike with the highest impact force for each frog in the data frame.",
    "crumbs": [
      "Polars and split-apply-combine exercises",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Adding data to a data frame</span>"
    ]
  },
  {
    "objectID": "exercises/polars_exercises/palmer_penguins.html",
    "href": "exercises/polars_exercises/palmer_penguins.html",
    "title": "57  Palmer penguins and split-apply-combine",
    "section": "",
    "text": "| Download notebook\nData set download\n\nThe Palmer penguins data set is a nice data set with which to practice various data science skills. For this exercise, we will use as subset of it, which you can download here: https://s3.amazonaws.com/bebi103.caltech.edu/data/penguins_subset.csv. The data set consists of measurements of three different species of penguins acquired at the Palmer Station in Antarctica. The measurements were made between 2007 and 2009 by Kristen Gorman.\na) Take a look at the CSV file containing the data set. Is it in tidy format? Why or why not?\nb) You can convert the CSV file to a “tall” format using the bebi103.utils.unpivot_csv() function. You can do that with the following function call, where path_to_penguins is a string containing the path to the penguin_subset.csv file.\nbebi103.utils.unpivot_csv(\n    path_to_penguins,\n    \"penguins_tall.csv\",\n    n_header_rows=2,\n    header_names=[\"species\", \"quantity\"],\n    comment_prefix=\"#\",\n    retain_row_index=True,\n    row_index_name='penguin_id',\n)    \nAfter running that function, load in the data set stored in the penguins_tall.csv file and store it in a variable named df_tall. Is this a tidy data set?\nc) Perform the following operations to make a new DataFrame from the one you loaded in to generate a new DataFrame. Explain what the operations do.\ndf = (\n    df_tall\n    .pivot(\n        index=['penguin_id', 'species'], columns='quantity', values='value'\n    )\n    .select(pl.exclude('penguin_id'))\n)\nIs the resulting data frame df tidy? Why or why not?\nd) Using the data frame you created in part (c), slice out all of the bill lengths for Gentoo penguins.\ne) Make a new data frame, df_tidy, containing the mean measured bill depth, bill length, body mass in kg, and flipper length for each species. You can use millimeters for all length measurements.\nf) Save the data frame you made in part (c) in a file named penguins_subset_tidy.csv.",
    "crumbs": [
      "Polars and split-apply-combine exercises",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Palmer penguins and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "exercises/viz_exercises/viz_exercises.html",
    "href": "exercises/viz_exercises/viz_exercises.html",
    "title": "Data visualization exercises",
    "section": "",
    "text": "It’s time to make some pretty, informative (or pretty informative?) graphics in some exercises!",
    "crumbs": [
      "Data visualization exercises"
    ]
  },
  {
    "objectID": "exercises/viz_exercises/palmer_penguins_plot.html",
    "href": "exercises/viz_exercises/palmer_penguins_plot.html",
    "title": "58  Plotting with Palmer penguins",
    "section": "",
    "text": "| Download notebook\nData set download\n\nIn a ?exr-palmer-penguins-split-apply-combine, you tidied a subset of the Palmer penguins data set and saved the result in a file name penguins_subset_tidy.csv. Use that tidied data set to make a scatter plot of bill length versus flipper length with the glyphs colored by species.",
    "crumbs": [
      "Data visualization exercises",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Plotting with Palmer penguins</span>"
    ]
  },
  {
    "objectID": "exercises/viz_exercises/mt_catastrophe_ecdfs.html",
    "href": "exercises/viz_exercises/mt_catastrophe_ecdfs.html",
    "title": "59  Microtubule catastrophe and ECDFs",
    "section": "",
    "text": "| Download notebook\nData set download\n\nAs a reminder, the empirical cumulative distribution function for a set of data point evaluated at x is\n\nECDF(x) = fraction of data points ≤ x.\n\nThe ECDF is defined on the entire real number line, with \\(\\mathrm{ECDF}(x\\to-\\infty) = 0\\) and \\(\\mathrm{ECDF}(x\\to\\infty) = 1\\). However, the ECDF is often plotted as discrete points, \\(\\{(x_i, y_i)\\}\\), where for point \\(i\\), \\(x_i\\) is the value of the measured quantity and \\(y_i\\) is \\(\\mathrm{ECDF}(x_i)\\). For example, if I have a set of measured data with values (1.1, –6.7, 2.3, 9.8, 2.3), the points on the ECDF plot are\n\n\n\nx\ny\n\n\n\n\n–6.7\n0.2\n\n\n1.1\n0.4\n\n\n2.3\n0.6\n\n\n2.3\n0.8\n\n\n9.8\n1.0\n\n\n\nIn this exercise, you will use a data set we will explore throughout the workshop. Gardner, Zanic, and coworkers investigated the dynamics of microtubule catastrophe, the switching of a microtubule from a growing to a shrinking state. In particular, they were interested in the time between the start of growth of a microtubule and the catastrophe event. They monitored microtubules by using tubulin (the monomer that comprises a microtubule) that was labeled with a fluorescent marker. As a control to make sure that fluorescent labels and exposure to laser light did not affect the microtubule dynamics, they performed a similar experiment using differential interference contrast (DIC) microscopy. They measured the time until catastrophe with labeled and unlabeled tubulin.\nWe will look at the data used to generate Fig. 2a of their paper. In the end, you will generate a plot similar to that figure.\na) Write a function with the call signature ecdfvals(data), which takes a one-dimensional Numpy array (or Polars Series; the same construction of your function will work for both) of data and returns the x and y values as Numpy arrays for plotting the ECDF in the “dots” style, as in Fig. 2a of the Gardner, Zanic, et al. paper. As a reminder,\n\nECDF(x) = fraction of data points ≤ x.\n\nAssume that there are no NaNs in the input. When you write this function, you may only use base Python and the standard library, in addition to Numpy and Polars. (iqplot has this functionality built-in, but the point here is to build a more concrete understanding of what an ECDF is.)\nb) Write a function, ecdfvals_expr(col), that returns a Polars Expression that will compute the y values of an ECDF for a given column, col. Again, assume there are no NaNs in the column.\nc) Use either the ecdfvals() function or the ecdfvals_expr() function that you wrote to plot the ECDFs shown in Fig. 2a of the Gardner, Zanic, et al. paper. By looking this plot, do you think that the fluorescent labeling makes a difference in the onset of catastrophe? (We will do a more careful statistical inference later in the workshop, but for now, does it pass the eye test? Eye tests are an important part of EDA.) You can access the data set here: https://s3.amazonaws.com/bebi103.caltech.edu/data/gardner_time_to_catastrophe_dic_tidy.csv",
    "crumbs": [
      "Data visualization exercises",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Microtubule catastrophe and ECDFs</span>"
    ]
  },
  {
    "objectID": "exercises/viz_exercises/finches_plotting.html",
    "href": "exercises/viz_exercises/finches_plotting.html",
    "title": "60  Long-term trends in hybridization of Darwin finches",
    "section": "",
    "text": "| Download notebook\n\nPeter and Rosemary Grant have been working on the Galápagos island of Daphne Major for over forty years. During this time, they have collected lots and lots of data about physiological features of finches. In 2014, they published a book with a summary of some of their major results (Grant P. R., Grant B. R., 40 years of evolution. Darwin’s finches on Daphne Major Island, Princeton University Press, 2014). They made their data from the book publicly available via the Dryad Digital Repository.\nWe will investigate their measurements of beak depth (the distance, top to bottom, of a closed beak) and beak length (base to tip on the top) of Darwin’s finches. We will look at data from two species, Geospiza fortis and Geospiza scandens. The Grants provided data on the finches of Daphne for the years 1973, 1975, 1987, 1991, and 2012. I have included the data in the files grant_1973.csv, grant_1975.csv, grant_1987.csv, grant_1991.csv, and grant_2012.csv. They are in almost exactly the same format is in the Dryad repository; I have only deleted blank entries at the end of the files.\nNote: If you want to skip the wrangling (which is very valuable experience), you can go directly to part (d). You can load in the data frame you generate in parts (a) through (c) from the file ~/git/bootcamp/data/grant_complete.csv.\na) Load each of the files into separate Polars data frames. You might want to inspect the file first to make sure you know what character the comments start with and if there is a header row.\nb) We would like to merge these all into one data frame. The problem is that they have different header names, and only the 1973 file has a year entry (called yearband). This is common with real data. It is often a bit messy and requires some wrangling.\n\nFirst, change the name of the yearband column of the 1973 data to year. Also, make sure the year format is four digits, not two!\n\nNext, add a year column to the other four data frames. You want tidy data, so each row in the data frame should have an entry for the year.\nChange the column names so that all the data frames have the same column names. I would choose column names\n['band', 'species', 'beak length (mm)', 'beak depth (mm)', 'year']\nConcatenate the data frames into a single data frame. Hint: You might want to use the how='diagonal' or how='diagonal_relaxed' kwargs.\n\nc) The band field gives the number of the band on the bird’s leg that was used to tag it. Are some birds counted twice? Are they counted twice in the same year? Do you think you should drop duplicate birds from the same year? How about different years? My opinion is that you should drop duplicate birds from the same year and keep the others, but I would be open to discussion on that. To practice your Pandas skills, though, let’s delete only duplicate birds from the same year from the data frame. When you have made this data frame, save it as a CSV file.\nHint: The methods is_duplicated() and unique() will be useful.\nAfter doing this work, it is worth saving your tidy data frame in a CSV document. To this using the to_csv() method of your data frame. (I have already done this and saved it as ~/git/bootcamp/data/grant_complete.csv, which will help you do the rest of the exercise if you have problems with this part.)\nd) Make a plots exploring how beak depth changes over time for each species. Think about what might be effective ways to display the data.\ne) It is informative to plot the measurement of each bird’s beak as a point in the beak depth-beak length plane. For the 1987 data, plot beak depth vs. beak width for Geospiza fortis and for Geospiza scandens.\nf) Do part (d) again for all years. Hint: To display all of the plots, check out the Bokeh documentation for layouts. In your plots, make sure all plots have the same range on the axes. If you want to set two plots, say p1 and p2 to have the same axis ranges, you can do the following.\np1.x_range = p2.x_range\np1.y_range = p2.y_range",
    "crumbs": [
      "Data visualization exercises",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Long-term trends in hybridization of Darwin finches</span>"
    ]
  },
  {
    "objectID": "exercises/probability_exercises/probability_exercises.html",
    "href": "exercises/probability_exercises/probability_exercises.html",
    "title": "Probability exercises",
    "section": "",
    "text": "You probably want to do some probability exercises.",
    "crumbs": [
      "Probability exercises"
    ]
  },
  {
    "objectID": "exercises/probability_exercises/distribution_stories.html",
    "href": "exercises/probability_exercises/distribution_stories.html",
    "title": "61  Distribution stories",
    "section": "",
    "text": "| Download notebook\n\nHow would you expect each of the following to be distributed?\na) The amount of time between repressor-operator binding events.\nb) The number of times a repressor binds its operator in a given hour.\nc) The amount of time (in total minutes of baseball played) between no-hitters in Major League Baseball.\nd) The number of no-hitters in a Major League Baseball season.\ne) The winning times of the Belmont Stakes.\nTo answer this question, try to match these stories to the stories of named distributions. For those of you not familiar with baseball, a no-hitter is a game in which a team concedes no hits to the opposing team. There have only been a few hundred no-hitters in over 200,000 MLB games. The Belmont Stakes is a major horse race that has been run each year for over 150 years.",
    "crumbs": [
      "Probability exercises",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Distribution stories</span>"
    ]
  },
  {
    "objectID": "exercises/probability_exercises/two_step_mt_catastrophe.html",
    "href": "exercises/probability_exercises/two_step_mt_catastrophe.html",
    "title": "62  Models for microtubule catastrophe",
    "section": "",
    "text": "| Download notebook\n\nIn a previous problem, you worked with a data set of measured times for microtubule catastrophe. In this problem, we will develop a model for microtubule catastrophe.\na) In the Gardner, Zanic, et al. paper, the authors assumed that the microtubule catatrophe times are Gamma distributed. Discuss how the story behind the Gamma distribution might work for modeling microtubule catastrophe.\nb) As an alternative model, we assert that two biochemical processes have to happen in succession to trigger catastrophe. That is, the first process happens, and only after the first process happens can the second one happen. We model each of the two process as a Poisson process (as is very often done with (bio)chemical dynamics). The rate of arrivals for the first one is \\(\\beta_1\\) and the rate of arrivals for the second one is \\(\\beta_2\\). Show that the PDF of the distribution matching this story is\n\\[\\begin{align}\nf(t;\\beta_1, \\beta_2) = \\frac{\\beta_1 \\beta_2}{\\beta_2 - \\beta_1}\\left(\\mathrm{e}^{-\\beta_1 t} - \\mathrm{e}^{-\\beta_2 t}\\right)\n\\end{align}\\]\nfor \\(\\beta_1 \\ne \\beta_2\\). Consequently, the CDF for this distribution is\n\\[\\begin{align}\nF(t; \\beta_1, \\beta_2) =\n\\frac{\\beta_1 \\beta_2}{\\beta_2-\\beta_1}\\left[\n\\frac{1}{\\beta_1}\\left(1-\\mathrm{e}^{- \\beta_1 t}\\right)- \\frac{1}{\\beta_2}\\left(1-\\mathrm{e}^{-\\beta_2 t}\\right)\n\\right].\n\\end{align}\\]\nc) Without formally doing any integrals, computing any derivatives, or taking limits, show that the PDF of the distribution for \\(\\beta_1 = \\beta_2 \\equiv \\beta\\) is\n\\[\\begin{align}\nf(t;\\beta) = \\beta^2\\,t\\,\\mathrm{e}^{-\\beta t}.\n\\end{align}\\]",
    "crumbs": [
      "Probability exercises",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Models for microtubule catastrophe</span>"
    ]
  },
  {
    "objectID": "exercises/probability_exercises/censored_and_truncated.html",
    "href": "exercises/probability_exercises/censored_and_truncated.html",
    "title": "63  Censored and truncated distributions",
    "section": "",
    "text": "| Download notebook\nSay experimenters A and B have a behavioral assay in which they time how long it takes for a mouse to descend a pole. Experimenter A records the descent times as follows.\n\nIf the mouse descends the pole in less than \\(y_\\mathrm{max}\\) seconds (typically 60 seconds), he records the descent time.\nIf the mouse does not descend the pole in less than \\(y_\\mathrm{max}\\) seconds, he does not record anything. He simply ignores that trial.\n\nExperimenter B takes a different approach. She records the descent times as follows.\n\nIf the mouse descends the pole in less than \\(y_\\mathrm{max}\\) seconds, she records the descent time.\nIf the mouse does not descend the pole in less than \\(y_\\mathrm{max}\\) seconds, she records the descent time as \\(y_\\mathrm{max}\\) seconds.\n\nLet \\(y\\) be the time to descend the pole for patient experimenters. That is, \\(y\\) can be greater than \\(y_\\mathrm{max}\\) seconds. Let \\(f(y)\\) by the probability density function of descent time and \\(F(y)\\) be the corresponding CDF.\na) Write down an expression for \\(f_\\mathrm{trunc}(y)\\), the probability density function for experimenter A’s observations in terms of \\(f(y)\\) and \\(F(y)\\). This is called a truncated distribution.\nb) Write down an expression for \\(f_\\mathrm{cens}(y)\\), the probability density function for experimenter B’s observations that are not \\(y_\\mathrm{max}\\). Also write an expression for \\(\\pi(y_\\mathrm{max})\\), which is the probability that experimenter B records \\(y_\\mathrm{max}\\). This is called a censored distribution.",
    "crumbs": [
      "Probability exercises",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Censored and truncated distributions</span>"
    ]
  },
  {
    "objectID": "exercises/probability_exercises/isi_distributions.html",
    "href": "exercises/probability_exercises/isi_distributions.html",
    "title": "64  Distributions of interspike intervals",
    "section": "",
    "text": "| Download notebook\nThere are several models for interspike intervals, and we will explore many of them. For the following descriptions of interspike intervals, what distribution would we use to model them?\na) When a previous spike arrived has no bearing on when the next one arrives.\nb) Several Poisson processes need to arrive in order for a spike to fire.\nc) A spike fires as a result of a drifting, diffusing signal reaching a threshold.",
    "crumbs": [
      "Probability exercises",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Distributions of interspike intervals</span>"
    ]
  },
  {
    "objectID": "exercises/probability_exercises/normal_approx.html",
    "href": "exercises/probability_exercises/normal_approx.html",
    "title": "65  Normal approximations",
    "section": "",
    "text": "| Download notebook\n\na) Imagine I have a univariate continuous distribution with PDF \\(f(y)\\) that has a maximum at \\(y^*\\). Assume that the first and second derivatives of \\(f(y)\\) are defined and continuous near \\(y^*\\). Show by expanding the log PDF of this distribution in a Taylor series about \\(y^*\\) that the distribution is locally Normal near the maximum.\nIn performing the Taylor series, how is the scale parameter \\(\\sigma\\) of the Normal approximation of the distribution related to the log PDF of the distribution it is approximating?\nb) Another way you can approximate a distribution as Normal is to use its mean and variance as the parameters as the approximate Normal. We will call this technique “equating moments.” Can you do this if the distribution you are approximating has heavy tails, say like a Cauchy distribution? Why or why not?\nc) Make plots of the PDF and CDF of the following distributions with their Normal approximations as derived from the Taylor series and by equating moments. Do you have any comments about the approximations?\n\n\nBeta with α = β = 10\nGamma with α = 5 and β = 2\n\nd) Discrete distributions are also often approximated as Normal. In fact, early studies of the Normal distributions arose from it being used to approximate a Binomial distribution. Use the method of equating moments to make a plot of the PMF of the Binomial distribution and the PDF of the Normal approximation of the Binomial distribution for:\n\nBinomial with N = 100 and θ = 0.1.\nBinomial with N = 10 and θ = 0.1.\n\nComment on what you see.",
    "crumbs": [
      "Probability exercises",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Normal approximations</span>"
    ]
  },
  {
    "objectID": "exercises/sampling_exercises/sampling_exercises.html",
    "href": "exercises/sampling_exercises/sampling_exercises.html",
    "title": "Sampling and simulation exercises",
    "section": "",
    "text": "Following is a sample of sampling exercises you can do.",
    "crumbs": [
      "Sampling and simulation exercises"
    ]
  },
  {
    "objectID": "exercises/sampling_exercises/practice_sampling.html",
    "href": "exercises/sampling_exercises/practice_sampling.html",
    "title": "66  Exploring and sampling probability distributions",
    "section": "",
    "text": "| Download notebook\n\n\nimport numpy as np\nimport scipy.stats as st\n\nimport iqplot\n\nimport bokeh.io\nimport bokeh.plotting\nbokeh.io.output_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nNumPy offers tools for drawing random number out of distributions. The scipy.stats module allows convenient calculation of probability density functions, cumulative distribution functions, etc., of many named probability distributions. For example, we can draw samples out of a standard Normal distribution using NumPy and can then plot a histogram of the results to get that familiar, beautifully shaped curve.\n\n# Instantiate random number generator\nrng = np.random.default_rng()\n\n# Draw 100,000 Normal samples\nsamples = rng.normal(0, 1, size=100000)\n\n# Plot the histogram\np = iqplot.histogram(samples, bins=50, rug=False, style='step', density=True)\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe could use SciPy to evaluate the PDF, which can be a pain to code up. It’s formula is\n\\[\\begin{align}\nf(x;\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\mathrm{e}^{-(x-\\mu)^2/2\\sigma^2}.\n\\end{align}\\]\nInstead of coding this up, we can use SciPy!\n\n# x-values for plotting.\nx = np.linspace(-5, 5, 200)\n\n# Compute PDF using scipy.stats\npdf = st.norm.pdf(x, loc=0, scale=1)\n\n# Add to the plot\np.line(x, pdf, line_color='orange', line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe usage of NumPy and SciPy for these distributions can get tricky because distributions can be parametrized in different ways. As an example, instead of parametrizing the Normal distribution with \\(\\sigma\\), we could parametrize it with \\(\\tau \\equiv 1/\\sigma\\). In this case, the PDF is\n\\[\\begin{align}\nf(x;\\mu, \\tau) = \\frac{\\tau}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-\\tau^2(x-\\mu)^2/2}.\n\\end{align}\\]\nIt is important to keep everything straight. To do this, I generally use the parametrizations used by the Stan software package. To help me keep things straight, I built the Probability Distribution Explorer. I encourage you to spend some time with the Distribution Explorer to familiarize yourself with distributions and how to sample out of them with NumPy and compute PDFs, PMFs, and CDFs with scipy.stats.\na) Draw 100,000 samples out of a Gamma distribution with parameters \\(\\alpha = 5\\) and \\(\\beta = 2\\), and plot a histogram. Then, plot the PDF of the Gamma distribution with these parameters.\nb) Draw 100,000 samples out of a Negative Bionomial distribution with parameters \\(\\alpha = 20\\) and \\(\\beta = 2\\) and plot a spike plot (You should use the fraction=True kwarg with iqplot.spike()). Then, plot the PMF of the Negative Binomial distribution with these parameters.",
    "crumbs": [
      "Sampling and simulation exercises",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Exploring and sampling probability distributions</span>"
    ]
  },
  {
    "objectID": "exercises/sampling_exercises/heavy_tails.html",
    "href": "exercises/sampling_exercises/heavy_tails.html",
    "title": "67  Exploring tails of distributions",
    "section": "",
    "text": "| Download notebook\n\na) Say I have three distributions:\n\nExponential, β=1\nNormal, μ=1, σ=1\nCauchy, µ=1, σ=1\n\nSay I draw numbers out of each of these distributions. Rank order the distributions, lowest to highest, in terms of how likely I am do draw a number greater than 10. You do not need to calculate anything to answer this question.\nb) Now draw a million numbers out of each of the three distributions. Which, if any, had numbers greater than ten? Was your intuition from part (a) correct?",
    "crumbs": [
      "Sampling and simulation exercises",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Exploring tails of distributions</span>"
    ]
  },
  {
    "objectID": "exercises/sampling_exercises/two_step_mt_catastrophe_sampling.html",
    "href": "exercises/sampling_exercises/two_step_mt_catastrophe_sampling.html",
    "title": "68  Simulating distributions MT catastrophe in a two-step model",
    "section": "",
    "text": "| Download notebook\n\nIn ?exr-two-step-mt-model, you worked out the PDF for a model where two biochemical processes have to happen in succession to trigger microtubule catastrophe. The result was\n\\[\\begin{align}\n&f(t;\\beta_1, \\beta_2) = \\frac{\\beta_1 \\beta_2}{\\beta_2 - \\beta_1}\\left(\\mathrm{e}^{-\\beta_1 t} - \\mathrm{e}^{-\\beta_2 t}\\right), \\\\[1em]\n&F(t; \\beta_1, \\beta_2) =\n\\frac{\\beta_1 \\beta_2}{\\beta_2-\\beta_1}\\left[\n\\frac{1}{\\beta_1}\\left(1-\\mathrm{e}^{- \\beta_1 t}\\right)- \\frac{1}{\\beta_2}\\left(1-\\mathrm{e}^{-\\beta_2 t}\\right)\n\\right].\n\\end{align}\n\\]\nIn a typical experiment, Gardner and Zanic measured about 150 catastrophe events. Use random number generation to simulate one of these experiments with this successive Poisson process model and plot the ECDF of times to catastrophe. That is, generate 150 random numbers that are distributed according to the story of the model. You can plot the time axis of the ECDF in units of \\(\\beta_1^{-1}\\). Do this for several values of \\(\\beta_2/\\beta_1\\). Overlay the analytical CDF with an ECDF from your simulation to verify that they match.",
    "crumbs": [
      "Sampling and simulation exercises",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Simulating distributions MT catastrophe in a two-step model</span>"
    ]
  },
  {
    "objectID": "exercises/sampling_exercises/refractory_isi.html",
    "href": "exercises/sampling_exercises/refractory_isi.html",
    "title": "69  Spike timing with a refractory period",
    "section": "",
    "text": "| Download notebook\n\nAfter a neuron spikes, there is a refractory period, typically a few milliseconds, for the membrane potential to return to its rest state. A neuron cannot fire during a refractory period. Draw samples of interspike intervals for neuronal firing modeled as a Poisson process with a refractory period. Think carefully about what distribution you want use to model the refractory period.",
    "crumbs": [
      "Sampling and simulation exercises",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Spike timing with a refractory period</span>"
    ]
  },
  {
    "objectID": "exercises/intro_mcmc_exercises/intro_mcmc_exercises.html",
    "href": "exercises/intro_mcmc_exercises/intro_mcmc_exercises.html",
    "title": "Introductory MCMC exercises",
    "section": "",
    "text": "Pass the mic to the MCMC! It’s time to do some exercises!",
    "crumbs": [
      "Introductory MCMC exercises"
    ]
  },
  {
    "objectID": "exercises/intro_mcmc_exercises/sample_bivariate_normal.html",
    "href": "exercises/intro_mcmc_exercises/sample_bivariate_normal.html",
    "title": "70  Sampling out of a bivariate Normal distribution",
    "section": "",
    "text": "| Download notebook\n\nDraw 4000 samples out of a bivariate Normal distribution with mean \\(\\boldsymbol{\\mu} = (10, 20)\\) and a covariance matrix of\n\\[\\begin{align}\n\\mathsf{\\Sigma} = \\begin{pmatrix}\n4 & -2 \\\\\n-2 & 6\n\\end{pmatrix}\n\\end{align}\\]\nusing each of the following three methods.\na) Using Numpy.\nb) Using Stan’s build-in random number generator (that is, in the generated quantities block).\nc) Using Stan’s MCMC sampler.\nMake plots of the samples to show they are consistent.",
    "crumbs": [
      "Introductory MCMC exercises",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Sampling out of a bivariate Normal distribution</span>"
    ]
  },
  {
    "objectID": "exercises/intro_mcmc_exercises/funnel_of_hell.html",
    "href": "exercises/intro_mcmc_exercises/funnel_of_hell.html",
    "title": "71  Funnel of hell",
    "section": "",
    "text": "| Download notebook\n\n\n\nCode\n# Colab setup ------------------\nimport os, shutil, sys, subprocess, urllib.request\nif \"google.colab\" in sys.modules:\n    cmd = \"pip install --upgrade iqplot colorcet bebi103 arviz cmdstanpy watermark\"\n    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    from cmdstanpy.install_cmdstan import latest_version\n    cmdstan_version = latest_version()\n    cmdstan_url = f\"https://github.com/stan-dev/cmdstan/releases/download/v{cmdstan_version}/\"\n    fname = f\"colab-cmdstan-{cmdstan_version}.tgz\"\n    urllib.request.urlretrieve(cmdstan_url + fname, fname)\n    shutil.unpack_archive(fname)\n    os.environ[\"CMDSTAN\"] = f\"./cmdstan-{cmdstan_version}\"\n    data_path = \"https://s3.amazonaws.com/bebi103.caltech.edu/data/\"\nelse:\n    data_path = \"../data/\"\n# ------------------------------\n\n\n\nimport numpy as np\n\nimport bebi103\n\nimport bokeh.io\nbokeh.io.output_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nIn his 2003 paper, Radford Neal pointed out that sampling out of relatively simply distributions using MCMC can be challenging. He proposed the following model, a kind of distribution that Thomas Wiecki has referred to as the “funnel of hell,” as a demonstration.\n\\[\\begin{align}\n& v \\sim \\text{Norm}(0, 3),\\\\[1em]\n& \\theta \\sim \\text{Norm}(0, \\mathrm{e}^{v/2}),\n\\end{align}\n\\tag{71.1}\\]\nThat is, \\(v\\) is Normally distribution with mean zero and variance 9, and \\(\\theta\\) is Normally distributed with mean zero and variance \\(\\mathrm{e}^v\\). The joint distribution is then\n\\[\\begin{align}\nP(\\theta, v) = P(\\theta\\mid v) \\,P(v) = \\frac{\\mathrm{e}^{-v/2}}{6\\pi}\\,\\exp\\left[-\\frac{1}{2}\\left(\\frac{v^2}{9} + \\frac{\\theta^2}{\\mathrm{e}^v}\\right)\\right]\n\\end{align}\n\\tag{71.2}\\]\nWe can compute the PDF analytically, so let’s make a plot of it so we know what we’re sampling out of.\n\ntheta = np.linspace(-4, 4, 400)\nv = np.linspace(-15, 5, 400)\n\nTHETA, V = np.meshgrid(theta, v)\nP = np.exp(-V / 2) / 6 / np.pi * np.exp(-(V ** 2 / 9 + THETA ** 2 / np.exp(V)) / 2)\n\n# Show it by hacking contour to show image, but no contours\nbokeh.io.show(\n    bebi103.viz.contour(\n        THETA,\n        V,\n        P,\n        overlaid=True,\n        line_kwargs=dict(alpha=0),\n        x_axis_label=\"θ\",\n        y_axis_label=\"v\",\n    )\n)\n\n\n  \n\n\n\n\n\nThis probability density function is funnel shaped, named “the Funnel of Hell” by Thomas Wiecki.\nNote that much of the probability density lies deep in the needle, which is a region of high curvature.\nThis simple distribution allows for independent sampling without MCMC. First, let’s generate some of these samples so we can see what effective sampling should look like.\n\n# Sample out of distribution\nnp.random.seed(3252)\nv = np.random.normal(0, 3, size=4000)\ntheta = np.random.normal(0, np.exp(v / 2))\n\np = bokeh.plotting.figure(\n    height=400, width=450, x_range=[-100, 100], x_axis_label=\"θ\", y_axis_label=\"v\"\n)\np.scatter(theta, v, alpha=0.3, color=\"#66c2a5\", legend_label=\"indep. samples\")\np.legend.location = \"bottom_left\"\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYour task is to code up a Stan model for this distribution and use MCMC to get samples out of it. Then, overlay those samples on the plot of the independent samples. What do you observe? (There will be obvious problems with the sampling, and we will address these in coming lessons.)",
    "crumbs": [
      "Introductory MCMC exercises",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Funnel of hell</span>"
    ]
  },
  {
    "objectID": "appendices/notation.html",
    "href": "appendices/notation.html",
    "title": "Appendix A — Notation",
    "section": "",
    "text": "Below are mathematical notational rules used throughout the course.\n\nScalar quantities as denoted as italicized symbols, such as \\(x\\), \\(y\\), \\(\\mu\\), and \\(\\sigma\\).\nVector quantities (first-rank tensors) are denoted in bold, such as \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\), and \\(\\boldsymbol{\\sigma}\\).\nMatrix quantities (second-rank tensors) are denoted with sans serif capital letters, such as \\(\\mathsf{A}\\), \\(\\mathsf{W}\\), and \\(\\mathsf{\\sigma}\\).\nThe one exception to the boldface and sans serif convention is when we denote a generic set of data or parameters. In that case, we use standard italicized symbols like \\(\\theta\\) (typically for a set of parameters) or \\(z\\) (typically for a set of latent variables).\nSubscripts typically denote an element of a vector, such as \\(x_i\\), or an element of a matrix, such as \\(A_{ij}\\). They can also denote an entry in a non-ordered collection, such as \\(M_i\\).\nTransposes are denoted with a superscript \\(\\mathsf{T}\\).\nVector dot products result in a scalar and are denoted with a dot, such as \\(\\mathbf{x}\\cdot\\mathbf{y}\\). Note that this is denoted as \\(\\mathbf{x}^\\mathsf{T}\\mathbf{x}\\) in some texts, but we will not use that notation. Writing out the sum, this is\n\n\\[\\begin{aligned}\n\\mathbf{x}\\cdot\\mathbf{y} = \\sum_{i}x_i\\, y_i.\n\\end{aligned}\n\\tag{A.1}\\]\n\nMatrix-vector products result in a vector are also denoted with a dot, such as \\(\\mathsf{A}\\cdot\\mathbf{x}\\). Writing out the sum, this is\n\n\\[\\begin{aligned}\n\\mathsf{A}\\cdot\\mathbf{x} = \\begin{pmatrix}\\sum_{i}A_{i1} x_i \\\\ \\sum_{i}A_{i2} x_i \\\\ \\vdots \\end{pmatrix}\n\\end{aligned}\n\\tag{A.2}\\]\n\nMatrix-matrix multiplication results in a matrix and is also denoted with a dot, such as \\(\\mathsf{A}\\cdot\\mathsf{B}\\).\nWe denote probability mass functions or probability density functions of measured quantities with \\(f\\), such as \\(f(y\\mid \\theta)\\). We denote probability mass functions or probability density functions of parameters or unmeasured quantities with \\(g\\), such as \\(g(\\theta \\mid y)\\). We denotes PMFs or PDFs of mixed or unknown variables with \\(\\pi\\), such as \\(\\pi(y, \\theta)\\) or \\(\\pi(z)\\).",
    "crumbs": [
      "Appendices",
      "Notation",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html",
    "href": "appendices/setting_up_python_computing_environment.html",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "",
    "text": "B.1 Why Python?\n| Download notebook\nThere are plenty of programming languages that are widely used in data science and in scientific computing more generally. Some of these, in addition to Python, are Matlab/Octave, Mathematica, R, Julia, Java, JavaScript, Rust, and C++.\nI have chosen to use Python. I believe language wars are counterproductive and welcome anyone to port the code we use to any language of their choice, I nonetheless feel we should explain this choice.\nPython is a flexible programming language that is widely used in many applications. This is in contrast to more domain-specific languages like R and Julia. It is easily extendable, which is in many ways responsible for its breadth of use. We find that there is a decent Python-based tool for many applications we can dream up, certainly in data science. However, the Python-based tool is often not the very best for the particular task at hand, but it is almost always pretty good. Thus, knowing Python is like having a Swiss Army knife; you can wield it to effectively accomplish myriad tasks. Finally, we also find that it has a shallow learning curve with most students.\nPerhaps most importantly, specifically for neuroscience applications, is that Python is widely used in machine learning and AI. The development of packages like TensorFlow, PyTorch, JAX, Keras, and scikit-learn have led to very widespread adoption of Python.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#jupyter-notebooks",
    "href": "appendices/setting_up_python_computing_environment.html#jupyter-notebooks",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.2 Jupyter notebooks",
    "text": "B.2 Jupyter notebooks\nThe materials of this course are constructed from Jupyter notebooks. To quote Jupyter’s documentation,\n\nJupyter Notebook and its flexible interface extends the notebook beyond code to visualization, multimedia, collaboration, and more. In addition to running your code, it stores code and output, together with markdown notes, in an editable document called a notebook.\n\nThis allows for executable documents that have code, but also richly formatted text and graphics, enabling the reader to interact with the material as they read it.\nSpecifically, notebooks are comprised of cells, where each cell contains either executable Python code or text.\nWhile you read the materials, you can read the HTML-rendered versions of the notebooks. To execute (and even edit!) code in the notebooks, you will need to run them. There are many options available to run Jupyter notebooks. Here are a few we have found useful.\n\nJupyterLab: This is a browser-based interface to Jupyter notebooks and more (including a terminal application, text editor, file manager, etc.). As of March 2025, Chrome, Firefox, Safari, and Edge are supported.\nVSCode: This is an excellent source code editor that supports Jupyter notebooks. Be sure to read the documentation on how to use Jupyter notebooks in VSCode. This may be an especially good option for Windows users.\nGoogle Colab: Google offers this service to run notebooks in the cloud on their machines. There are a few caveats, though. First, not all packages and updates are available in Colab. Furthermore, not all interactivity that will work natively in Jupyter notebooks works with Colab. If a notebook sits idle for too long, you will be disconnected from Colab. Finally, there is a limit to resources that are available for free, and as of March 2025, that limit is unpublished and can vary. All of the notebooks in the HTML rendering of this book have an “Open in Colab” button at the upper right that allows you to launch the notebook in Colab. This is a quick-and-easy way to execute the book’s contents.\n\nFor our work in this programming bootcamp, I encourage you to use either JupyterLab in the browser or VSCode, with Colab as a backup if you’re having trouble.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#marimo",
    "href": "appendices/setting_up_python_computing_environment.html#marimo",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.3 Marimo",
    "text": "B.3 Marimo\nMarimo offers a very nice notebook interface that is a departure from Jupyter notebooks in its structure. The biggest departure is that Marimo notebooks are specifically for Python, as opposed to being language agnostic like Jupyter. As a result, Marimo notebooks can offer many features not seen in Jupyter notebooks (without add-ons). The two most compelling, at least to me, are\n\nMarimo notebooks are simple .py files which allow for easier version control and simple execution as scripts.\nMarimo notebooks are reactive, meaning that the ordering of the cells is irrelevant and the notebook runs all cells that need to be rerun as a result of a change of value of a variable in any given cell.\n\nIn the course, we will use Jupyter notebooks, but you are welcome to play with Marimo notebooks. Upon completing the installation instructions in this notebook, Marimo will be installed.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#ensuring-you-have-a-c-toolchain",
    "href": "appendices/setting_up_python_computing_environment.html#ensuring-you-have-a-c-toolchain",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.4 Ensuring you have a C++ toolchain",
    "text": "B.4 Ensuring you have a C++ toolchain\nWe will be using Stan for some of our modeling. Stan has a probabilistic programming language. Programs written in this language, called Stan programs, are translated into C++ by the Stan parser, and then the C++ code is compiled. As you will see throughout the class, there are many advantages to this approach.\nThere are many interfaces for Stan, including the two most widely used RStan and PyStan, which are R and Python interfaces, respectively. We will use a simpler interface, CmdStanPy, which has several advantages that will become apparent when you start using it.\nWhichever interface you use needs to have Stan installed and functional, which means you have to have an installed C++ toolchain. Installation and compilation can be tricky and varies from operating system to operating system. The instructions below are not guaranteed to work; you may have to do some troubleshooting on your own. Note that you can use Google Colab (or other cloud computing resources) for computing as well, so you do not need to worry if you have trouble installing Stan locally.\nYou can read the CmdStanPy documentation about setting up the necessary tooling for your operatinve system. The long and short of it is that you do not need to do anything if you are using Windows. Likewise, if you are using Linux, a suitable C++ toolchain is typically preinstalled. For MacOS, you need to install Xcode command line tools by running the following on the command line.\nxcode-select --install",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#installing-python-tools",
    "href": "appendices/setting_up_python_computing_environment.html#installing-python-tools",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.5 Installing Python tools",
    "text": "B.5 Installing Python tools\nPrior to embarking on your journey into data analysis, you need to have a functioning Python distribution installed on your computer. We will use pixi, a relatively new package manager that I have found very effective.\nImportantly, it does so in a project-based way. That is, for each project, you use Pixi to create and manage the packages needed for that project. Our “project” here is our course!\nPixi is a package management tool that allows installation of packages. Importantly, it does so in a project-based way. That is, for each project, you use Pixi to create and manage the packages needed for that project. Our “project” here is our data analysis/statistical inference course.\nStep 1: Install Pixi. To install Pixi, you need access to the command line. For macOS users, hit Command-space, type in “terminal” and open the Terminal app. In Windows, open PowerShell by opening the Start Menu, typing “PowerShell” in the search bar, and selecting “Windows PowerShell.” I assume you know how to get access to the command line if you are using Linux.\nOn the command line, do the following.\nmacOS or Linux\ncurl -fsSL https://pixi.sh/install.sh | sh\nWindows\npowershell -ExecutionPolicy ByPass -c \"irm -useb https://pixi.sh/install.ps1 | iex\"\nStep 2: Create a directory for your work in the course. You might want to name the directory wis-stats/, which is what I have named it. You can do this either with the command line of your graphical file management program (e.g., Finder for macOS).\nStep 3 Navigate to the directory you created on the command line. For example, if the directory is wis_stats/ in your home directory and you are in your home directory, you can do\ncd wis_stats\non the command line.\nStep 4 Download the requisite Pixi files: pixi.toml, pixi.lock. These files need to be stored in the directory you created in step 3. You may download them by right-clicking those links, or by doing the following on the command line.\nmacOS or Linux\ncurl -fsSL https://raw.githubusercontent.com/wis-stats/wis-stats.github.io/refs/heads/main/pixi.toml\ncurl -fsSL https://raw.githubusercontent.com/wis-stats/wis-stats.github.io/refs/heads/main/pixi.lock\nWindows\nirm -useb https://raw.githubusercontent.com/wis-stats/wis-stats.github.io/refs/heads/main/pixi.toml -OutFile pixi.toml\n\nirm -useb https://raw.githubusercontent.com/wis-stats/wis-stats.github.io/refs/heads/main/pixi.lock -OutFile pixi.lock\nStep 5 Install CmdStan. Do the following on the command line (it may take a while to execute).\npixi run install_cmdstan\nStep 6 Install the environment! Do the following on the command line.\npixi install\nStep 6 To be able to use all of the packages, you need to invoke a Pixi shell. To do so, execute the following on the command line.\npixi shell\nYou are now good to go! After you are done working, to exit the Pixi shell, hit Control-D.\nFor doing work for this class, you will need to cd into the directory you created in step 2 and execute pixi shell every time you open a new terminal (or PowerShell) window.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#launching-jupyterlab",
    "href": "appendices/setting_up_python_computing_environment.html#launching-jupyterlab",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.6 Launching JupyterLab",
    "text": "B.6 Launching JupyterLab\nOnce you have invoked a Pixi shell, you can launch JupyterLab via your operating system’s terminal program (Terminal on macOS and PowerShell on Windows). To do so, enter the following on the command line (after having run pixi shell).\njupyter lab\nYou will have an instance of JupyterLab running in your default browser. If you want to specify the browser, you can, for example, type\njupyter lab --browser=firefox\non the command line.\nAlternatively, if you are using VSCode, you can use its menu system to open .ipynb files. Make sure you select the Python kernel corresponding to your environment. You can read the documentation here. Hint: You may need to restart VSCode after doing the above installations so it is aware of your pixi environment.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#checking-your-distribution",
    "href": "appendices/setting_up_python_computing_environment.html#checking-your-distribution",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.7 Checking your distribution",
    "text": "B.7 Checking your distribution\nLet’s now run a quick test to make sure things are working properly. We will make a quick plot that requires some of the scientific libraries we will use.\nLaunch a Jupyter notebook in JupyterLab. In the first cell (the box next to the [ ]: prompt), paste the code below. To run the code, press Shift+Enter while the cursor is active inside the cell. You should see a plot that looks like the one below. If you do, you have a functioning Python environment for scientific computing!\n\nimport os, glob\n\nimport numpy as np\n\nimport bebi103\nimport cmdstanpy\nimport arviz as az\n\nimport bokeh.plotting\nimport bokeh.io\nbokeh.io.output_notebook()\n\nschools_data = {\n    \"J\": 8,\n    \"y\": [28, 8, -3, 7, -1, 1, 18, 12],\n    \"sigma\": [15, 10, 16, 11, 9, 11, 10, 18],\n}\n\nschools_code = \"\"\"\ndata {\n  int&lt;lower=0&gt; J; // number of schools\n  vector[J] y; // estimated treatment effects\n  vector&lt;lower=0&gt;[J] sigma; // s.e. of effect estimates\n}\n\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] eta;\n}\n\ntransformed parameters {\n  vector[J] theta = mu + tau * eta;\n}\n\nmodel {\n  eta ~ normal(0, 1);\n  y ~ normal(theta, sigma);\n}\n\"\"\"\n\nwith open(\"schools_code.stan\", \"w\") as f:\n    f.write(schools_code)\n\nwith bebi103.stan.disable_logging():\n    sm = cmdstanpy.CmdStanModel(stan_file=\"schools_code.stan\")\n    samples = sm.sample(data=schools_data, output_dir=\"./\", show_progress=False)\n\nsamples = az.from_cmdstanpy(samples)\n\n# Clean up\nbebi103.stan.clean_cmdstan()\nfor fname in glob.glob(\"schools_code*\"):\n    os.remove(fname) \n\n# Make a plot of samples\np = bokeh.plotting.figure(\n    frame_height=250, frame_width=250, x_axis_label=\"μ\", y_axis_label=\"τ\"\n)\np.scatter(\n    np.ravel(samples.posterior[\"mu\"]), \n    np.ravel(samples.posterior[\"tau\"]), \n    alpha=0.1\n)\n\nbokeh.io.show(p)\n\n    \n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/setting_up_python_computing_environment.html#computing-environment",
    "href": "appendices/setting_up_python_computing_environment.html#computing-environment",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,cmdstanpy,arviz,bebi103,bokeh,jupyterlab\nprint(\"CmdStan : {0:d}.{1:d}\".format(*cmdstanpy.cmdstan_version()))\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\ncmdstanpy : 1.2.5\narviz     : 0.22.0\nbebi103   : 0.1.28\nbokeh     : 3.7.3\njupyterlab: 4.4.5\n\nCmdStan : 2.36",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html",
    "href": "appendices/python/hello_world.html",
    "title": "Appendix C — Hello, world.",
    "section": "",
    "text": "C.1 The Python interpreter\n| Download notebook\nIn this appendix, we introduce some of the basic syntax and ideas behind the Python programming language and the Jupyter interface.\nPython is an interpreted language, which means that each line of code you write is translated, or interpreted, into a set of instructions that your machine can understand by the Python interpreter. This stands in contrast to compiled languages. For these languages (the dominant ones being Fortran, C, and C++), your entire code is translated into machine language before you ever run it. When you execute your program, it is already in machine language.\nSo, whenever you want your Python code to run, you give it to the Python interpreter.\nThere are many ways to launch the Python interpreter. One way is to type\non the command line of a terminal. This launches the vanilla Python interpreter. Because we are using Python code to explore biological circuit design, we will never really use this. Rather, we will have a greatly enhanced Python experience using Jupyter notebooks. Nevertheless, as you go beyond notebooks and do more sophisticated computing in your adventures with biological circuits, it is good to know about running Python outside of notebooks, so we will do that now.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#the-python-interpreter",
    "href": "appendices/python/hello_world.html#the-python-interpreter",
    "title": "Appendix C — Hello, world.",
    "section": "",
    "text": "python",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#hello-world.-and-the-print-function",
    "href": "appendices/python/hello_world.html#hello-world.-and-the-print-function",
    "title": "Appendix C — Hello, world.",
    "section": "C.2 Hello, world. and the print() function",
    "text": "C.2 Hello, world. and the print() function\nTraditionally, the first program anyone writes when learning a new language is called “Hello, world.” In this program, the words “Hello, world.” are printed on the screen. The original Hello, world. was likely written by Brian Kernighan, one of the inventors of Unix, and the author of the classic and authoritative book on the C programming language. In his original, the printed text was “hello, world” (no period or capital H), but people use lots of variants.\nWe will first write and run this little program using a JupyterLab console. After launching JupyterLab, you probably already have the Launcher in your JupyterLab window. If you do not, you can expand the Files tab at the left of your JupyterLab window (if it is not already expanded) by clicking on that tab, or alternatively hit ctrl+b (or cmd+b on macOS). At the top of the Files tab is a + sign, which gives you a Jupyter Launcher.\nIn the Jupyter Launcher, click the Python 3 icon under Console. This will launch a console, which has a large white space above a prompt that says In []:. You can enter Python code in this prompt, and it will be executed.\nTo print Hello, world., enter the code below. To execute the code, hit shift+enter.\n\nprint('Hello, world.')\n\nHello, world.\n\n\nHooray! We just printed Hello, world. to the screen. To do this, we used Python’s built-in print() function. The print() function takes a string as an argument. It then prints that string to the screen. We will learn more about function syntax later, but we can already see the rough syntax with the print() function.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#py-files",
    "href": "appendices/python/hello_world.html#py-files",
    "title": "Appendix C — Hello, world.",
    "section": "C.3 .py files",
    "text": "C.3 .py files\nNow let’s use our new knowledge of the print() function to have our computer say a bit more than just Hello, world. Type these lines in at the prompt, hitting enter each time you need a new line. After you’ve typed them all in, hit shift+enter to run them.\n\n# The first few lines from The Zen of Python by Tim Peters\nprint('Beautiful is better than ugly.')\nprint('Explicit is better than implicit.')\nprint('Simple is better than complex.')\nprint('Complex is better than complicated.')\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nNote that the first line is preceded with a # sign, and the Python interpreter ignored it. The # sign denotes a comment, which is ignored by the interpreter, but very very important for the human!\nWhile the console prompt was nice for entering all of this, a better option is to store them in a file, and then have the Python interpreter run the lines in the file. This is how you typically store Python code, and the suffix of such files is .py.\nSo, let’s create a .py file. To do this, use the JupyterLab Launcher to launch a text editor. Once it is launched, you can right click on the tab of the text editor window to change the name. We will call this file zen.py. Within this file, enter the four lines of code you previously entered in the console prompt. Be sure to save it.\nTo run the code in this file, you can invoke the Python interpreter at the command line, followed by the file name. I.e., enter\npython zen.py\nat the command line. Note that when you run code this way, the interpreter exits after completion of running the code, and you do not get a prompt.\nTo run the code in this file using the Jupyter console, you can use the %run magic function.\n\n%run zen.py\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nTo shut down the console, you can click on the Running tab at the left of the JupyterLab window and click on SHUTDOWN next to the console.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#jupyter",
    "href": "appendices/python/hello_world.html#jupyter",
    "title": "Appendix C — Hello, world.",
    "section": "C.4 Jupyter",
    "text": "C.4 Jupyter\nAt this point, we have introduced JupyterLab, its text editor, and the console, as well as the Python interpreter itself. You might be asking….\n\nC.4.1 What is Jupyter?\nFrom the Project Jupyter website: &gt;Project Jupyter is an open source project was born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages.\nSo, Jupyter is an extension of IPython that pushes interactive computing further. It is language agnostic as its name suggests. The name “Jupyter” is a combination of Julia (a newer language for scientific computing), Python (which you know and love), and R (the dominant tool for statistical computation). However, you can run over 40 different languages in a JupyterLab, not just Julia, Python, and R.\nCentral to Jupyter/JupyterLab are Jupyter notebooks. In fact, the document you are reading right now was generated from a Jupyter notebook.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#why-jupyter-notebooks",
    "href": "appendices/python/hello_world.html#why-jupyter-notebooks",
    "title": "Appendix C — Hello, world.",
    "section": "C.5 Why Jupyter notebooks?",
    "text": "C.5 Why Jupyter notebooks?\nWhen writing code you will reuse, you should develop fully tested modules using .py files. You can always import those modules when you are using a Jupyter notebook (more on modules and importing them later in this appendix). So, a Jupyter notebook is not good for an application where you are building reusable code or scripts. However, Jupyter notebooks are very useful in the following applications.\n\nExploring data/analysis. Jupyter notebooks are great for trying things out with code, or exploring a data set. This is an important part of the research process. The layout of Jupyter notebooks is great for organizing thoughts as you synthesize them.\nSharing your thinking in your analysis. Because you can combine nicely formatted text and executable code, Jupyter notebooks are great for sharing how you go about doing your calculations with collaborators and with readers of your publications. Famously, LIGO used a Jupyter notebook to explain the signal processing involved in their first discovery of a gravitational wave.\nPedagogy. All of the content in this book, including this appendix, was developed using Jupyter notebooks!\n\nNow that we know what Jupyter notebooks are and what the motivation is for using them, let’s start!\n\nC.5.1 Launching a Jupyter notebook\nYou can launch JupyterLab via your operating system’s terminal program (Terminal on macOS and PowerShell on Windows). If you are on a Mac, open the Terminal program. You can do this hitting Command + space bar and searching for “terminal.” Using Windows, you should launch PowerShell. You can do this by hitting Windows + R and typing “powershell” in the text box.\nYou need to make sure you are using the caltech_datasai environment whenever you launch JupyterLab, so you should do conda activate caltech_datasai each time you open a terminal.\nNow that you have activated the caltech_datasai environment, you can launch JupyterLab by typing\njupyter lab\non the command line. You will have an instance of JupyterLab running in your default browser. If you want to specify the browser, you can, for example, type\njupyter lab --browser=firefox\non the command line.\nAlternatively, if you are using VSCode, you can use its menu system to open .ipynb files. Within the JupyterLab window, you will have the option to launch a notebook, a console, a terminal, or a text editor.\n\n\nC.5.2 Cells\nA Jupyter notebook consists of cells. The two main types of cells you will use are code cells and markdown cells, and we will go into their properties in depth momentarily. First, an overview.\nA code cell contains actual code that you want to run. You can specify a cell as a code cell using the pulldown menu in the toolbar of your Jupyter notebook. Otherwise, you can can press Esc and then y (denoted Esc - y“) while a cell is selected to specify that it is a code cell. Note that you will have to hit enter after doing this to start editing it.\nIf you want to execute the code in a code cell, hit Enter while holding down the Shift key (denoted Shift + Enter). Note that code cells are executed in the order you shift-enter them. That is to say, the ordering of the cells for which you hit Shift + Enter is the order in which the code is executed. If you did not explicitly execute a cell early in the document, its results are not known to the Python interpreter. This is a very important point and is often a source of confusion and frustration for students.\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. You can read about its syntax here. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting Shift + Enter renders the text in the formatting you specify.\nMarkdown cells can also render LaTeX for mathematics. For example,\nThe FitzHugh-Nagumo model is a simplification of the Hodgkin-Huxley model with two ordinaray differential equations,\n\n\\begin{align}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} &= v - \\frac{v^3}{3} - w + RI,\\\\[1em]\n\\tau\\,\\frac{\\mathrm{d}w}{\\mathrm{d}t} &= v + a + b w.\n\\end{align}\nrenders as:\nThe FitzHugh-Nagumo model is a simplification of the Hodgkin-Huxley model with two ordinaray differential equations,\n\\[\\begin{align}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} &= v - \\frac{v^3}{3} - w + RI,\\\\[1em]\n\\tau\\,\\frac{\\mathrm{d}w}{\\mathrm{d}t} &= v + a + b w.\n\\end{align}\\]\nYou can specify a cell as being a markdown cell in the Jupyter toolbar, or by hitting Esc - m in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn general, when you want to add a new cell, you can click the + icon on the notebook toolbar. The shortcut to insert a cell below is Esc - b and to insert a cell above is Esc - a. Alternatively, you can execute a cell and automatically add a new one below it by hitting Alt + Enter.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#code-cells",
    "href": "appendices/python/hello_world.html#code-cells",
    "title": "Appendix C — Hello, world.",
    "section": "C.6 Code cells",
    "text": "C.6 Code cells\nBelow is an example of a code cell printing hello, world. Notice that the output of the print statement appears in the same cell, though separate from the code block.\n\n# Say hello to the world.\nprint('hello, world.')\n\nhello, world.\n\n\nIf you evaluate a Python expression that returns a value, that value is displayed as output of the code cell. This only happens for the last line of the code cell.\n\n# Would show 9 if this were the last line, but it is not, so shows nothing\n4 + 5\n\n# I hope we see 11.\n5 + 6\n\n11\n\n\nNote, however, if the last line does not return a value, such as if we assigned value to a variable, there is no visible output from the code cell.\n\n# Variable assignment, so no visible output.\na = 5 + 6\n\n\n# However, now if we ask for a, its value will be displayed\na\n\n11\n\n\n\nC.6.1 Display of graphics\nWhen we discuss plotting with Bokeh, you will learn about displaying graphics in Jupyter notebooks.\n\n\nC.6.2 Quick keys\nThere are some keyboard shortcuts that are convenient to use in JupyterLab. We already encountered many of them. Importantly, pressing Esc brings you into command mode in which you are not editing the contents of a single cell, but are doing things like adding cells. Below are some useful quick keys. If two keys are separated by a + sign, they are pressed simultaneously, and if they are separated by a - sign, they are pressed in succession.\n\n\n\nQuick keys\nmode\naction\n\n\n\n\nEsc - m\ncommand\nswitch cell to Markdown cell\n\n\nEsc - y\ncommand\nswitch cell to code cell\n\n\nEsc - a\ncommand\ninsert cell above\n\n\nEsc - b\ncommand\ninsert cell below\n\n\nEsc - d - d\ncommand\ndelete cell\n\n\nAlt + Enter\nedit\nexecute cell and insert a cell below\n\n\n\nThere are many others (and they are shown in the pulldown menus within JupyterLab), but these are the ones I seem to encounter most often.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/hello_world.html#computing-environment",
    "href": "appendices/python/hello_world.html#computing-environment",
    "title": "Appendix C — Hello, world.",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 8.30.0\n\njupyterlab: 4.3.6",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html",
    "href": "appendices/python/variables_operators_types.html",
    "title": "Appendix D — Variables, operators, and types",
    "section": "",
    "text": "D.1 Determining the type\n| Download notebook\nWhether you are programming in Python or pretty much any other language, you will be working with variables. While the precise definition of a variable will vary from language to language, we’ll focus on Python variables here.\nWe will talk more about objects later, but a variable, like everything in Python, is an object. For now, you can think of it in the following way. The following can be properties of a variable: 1. The type of variable. E.g., is it an integer, like 2, or a string, like 'Hello, world.'? 2. The value of the variable.\nDepending on the type of the variable, you can do different things to it and other variables of similar type. This, as with most things, is best explored by example. We will go through some of the properties of variables and things you can do to them.\nFirst, we will use Python’s built-in type() function to determine the type of some variables.\ntype(2)\n\nint\ntype(2.3)\n\nfloat\ntype('Hello, world.')\n\nstr\nThe type function told us that 2 is an int (short for integer), 2.3 is a float (short for floating point number, basically a real number that is not an integer), and 'Hello, world.' is a str (short for string). Note that the single quotes around the characters indicate that it is a string. So, '1' is a string, but 1 is an integer.\nNote that we can also express floats using scientific notation; \\(4.5\\times 10^{-7}\\) is expressed as 4.5e-7.\ntype(4.5e-7)\n\nfloat",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#determining-the-type",
    "href": "appendices/python/variables_operators_types.html#determining-the-type",
    "title": "Appendix D — Variables, operators, and types",
    "section": "",
    "text": "D.1.1 A note on strings\nWe just saw that strings can be enclosed in single quotes. In Python, we can equivalently enclose them in double quotes. E.g.,\n'my string'\nand\n\"my string\"\nare the same thing. We can also denote a string with triple quotes. So,\n\"\"\"my string\"\"\"\n'''my string'''\n\"my string\"\n'my string'\nare all the same thing. The difference with triple quotes is that it allows a string to extend over multiple lines.\n\n# A multi-line string\nmy_str = \"\"\"It was the best of times,\nit was the worst of times...\"\"\"\n\nprint(my_str)\n\nIt was the best of times,\nit was the worst of times...\n\n\nNote, though, we cannot do this with single quotes.\n\n# This is a SyntaxError\nmy_str = 'It was the best of times,\nit was the worst of times...'\n\n\n  Cell In[6], line 2\n    my_str = 'It was the best of times,\n             ^\nSyntaxError: unterminated string literal (detected at line 2)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#operators",
    "href": "appendices/python/variables_operators_types.html#operators",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.2 Operators",
    "text": "D.2 Operators\nOperators allow you to do things with variables, like add them. They are represented by special symbols, like + and *. For now, we will focus on arithmetic operators. Python’s arithmetic operators are\n\n\n\naction\noperator\n\n\n\n\naddition\n+\n\n\nsubtraction\n-\n\n\nmultiplication\n*\n\n\ndivision\n/\n\n\nraise to power\n**\n\n\nmodulo\n%\n\n\nfloor division\n//\n\n\n\nWarning: Do not use the ^ operator to raise to a power. That is actually the operator for bitwise XOR, which we will not really use. Observe firey death if you use these inappropriately:\n\n10^200\n\n194\n\n\nInstead of raising 10 to the 200th power, Python performed a bitwise XOR as illustrated below:\n\n\n\na\nBinary\nDecimal\n\n\n\n\nInput 1\n00001010\n10\n\n\nInput 2\n11001000\n200\n\n\nOutput\n11000010\n194\n\n\n\nNote: if you want to see how a decimal number is represented in binary, you can use the following:\n\n'{0:b}'.format(194)\n\n'11000010'\n\n\n\nD.2.1 Operations on integers\nLet’s see how these operators work on integers.\n\n2 + 3\n\n5\n\n\n\n2 - 3\n\n-1\n\n\n\n2 * 3\n\n6\n\n\n\n2 / 3\n\n0.6666666666666666\n\n\n\n2 ** 3\n\n8\n\n\n\n2 % 3\n\n2\n\n\n\n2 // 3\n\n0\n\n\nThis is what we would expect. An import note, though. If you are using Python 2, division of integers defaults to floor division. Some legacy code is written in Python 2, though it officially sunset on New Years Day 2020.\n\n\nD.2.2 Operations on floats\nLet’s try floats.\n\n2.1 + 3.2\n\n5.300000000000001\n\n\nWait a minute! We know 2.1 + 3.2 = 5.3, but Python gives 5.300000000000001. This is due to the fact that floating point numbers are stored with a finite number of binary bits. There will always be some rounding errors. This means that as far as the computer is concerned, it cannot tell you that 2.1 + 3.2 and 5.3 are equal. This is important to remember when dealing with floats, as we will see in the next lesson.\n\n2.1 - 3.2\n\n-1.1\n\n\n\n# Very very close to zero because of finite precision\n5.3 - (2.1 + 3.2)\n\n-8.881784197001252e-16\n\n\n\n2.1 * 3.2\n\n6.720000000000001\n\n\n\n2.1 / 3.2\n\n0.65625\n\n\n\n2.1 ** 3.2\n\n10.74241047739471\n\n\n\n2.1 % 3.2\n\n2.1\n\n\n\n2.1 // 3.2\n\n0.0\n\n\nAside from the floating point precision issue I already pointed out, everything is like we would expect. Note, though, that we cannot divide by zero.\n\n2.1 / 0.0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 2.1 / 0.0\n\nZeroDivisionError: float division by zero\n\n\n\nWe cannot do it with ints, either.\n\n2 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 2 / 0\n\nZeroDivisionError: division by zero\n\n\n\n\n\nD.2.3 Operations on integers and floats\nThis proceeds as we think it should.\n\n2.1 + 3\n\n5.1\n\n\n\n2.1 - 3\n\n-0.8999999999999999\n\n\n\n2.1 * 3\n\n6.300000000000001\n\n\n\n2.1 / 3\n\n0.7000000000000001\n\n\n\n2.1 ** 3\n\n9.261000000000001\n\n\n\n2.1 % 3\n\n2.1\n\n\n\n2.1 ** 3\n\n9.261000000000001\n\n\nAnd again we have the rounding errors, but everything is otherwise intuitive.\n\n\nD.2.4 Operations on strings\nNow let’s try some of these operations on strings. This idea of applying mathematical operations to strings seems strange, but let’s just mess around and see what we get.\n\n'Hello, ' + 'world.'\n\n'Hello, world.'\n\n\nAdding strings together concatenates them! This is also intuitive. How about subtracting strings?\n\n'Hello, ' - 'world'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 'Hello, ' - 'world'\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\n\nThat stands to reason. Subtracting strings does not make sense. The Python interpreter was kind enough to give us a nice error message saying that we cannot have a str and a str operand type for the subtraction operation. It also makes sense that we can’t do multiplication, raising of power, etc., with two strings. How about multiplying a string by an integer?\n\n'Hello, world.' * 3\n\n'Hello, world.Hello, world.Hello, world.'\n\n\nYes, this makes sense! Multiplication by an integer is the same thing as just adding multiple times, so the Python interpreter concatenates the string several times.\nAs a final note on operators with strings, watch out for this:\n\n'4' + '2'\n\n'42'\n\n\nThe result is not 6, but it is a string containing the characters '4' and '2'.\n\n\nD.2.5 Order of operations\nThe order of operations is also as we would expect. Exponentiation comes first, followed by multiplication and division, floor division, and modulo. Next comes addition and subtraction. In order of precedence, our arithmetic operator table is\n\n\n\nprecedence\noperators\n\n\n\n\n1\n**\n\n\n2\n*, /, //, %\n\n\n3\n+, -\n\n\n\nYou can also group operations with parentheses. Operations within parentheses is are always evaluated first. As a watchout, do not use excessive parentheses. So often, we see students not trusting the order of operations and polluting their code with lots of parentheses, making it unreadable. This has been the source of countless bugs we have encountered in student code through the years.\nLet’s practice order-of-operations.\n\n1 + 4**2\n\n17\n\n\n\n1 + 4/2\n\n3.0\n\n\n\n1**3 + 2**3 + 3**3 + 4**3\n\n100\n\n\n\n(1 + 2 + 3 + 4)**2\n\n100\n\n\nInterestingly, we also demonstrated that the sum of the first \\(n\\) cubes is equal to the sum of the first \\(n\\) integers squared. Fun!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#variables-and-assignment-operators",
    "href": "appendices/python/variables_operators_types.html#variables-and-assignment-operators",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.3 Variables and assignment operators",
    "text": "D.3 Variables and assignment operators\nSo far, we have essentially just used Python as an oversized desktop calculator. We would really like to be able to think about our computational problems symbolically. We mentioned variables at the beginning of the tutorial, but in practice we were just using numbers and strings directly. We would like to say that a variable, a, represents an integer and another variable b represents another integer. Then, we could do things like add a and b. So, we see immediately that the variables have to have a type associated with them so the Python interpreter knows what to do when we use operators with them. A variable should also have a value associated with it, so the interpreter knows, e.g., what to add.\nIn order to create, or instantiate, a variable, we can use an assignment operator. This operator is the equals sign. So, let’s make variables a and b and add them.\n\na = 2\nb = 3\na + b\n\n5\n\n\nGreat! We get what we expect! And we still have a and b.\n\na, b\n\n(2, 3)\n\n\nNow, we might be tempted to say, “a is two.” No. a is not two. a is a variable that has a value of 2. A variable in Python is not just its value. A variable also carries with it a type. It also has more associated with it under the hood of the interpreter that we will not get into. So, you can think about a variable as a map to an address in RAM (called a pointer in computer-speak) that stores information, including a type and a value.\n\nD.3.1 Assignment/increment operators\nNow, let’s say we wanted to update the value of a by adding 4.1 to it. Python will do some magic for us.\n\nprint(type(a), a)\n\na = a + 4.1\n\nprint(type(a), a)\n\n&lt;class 'int'&gt; 2\n&lt;class 'float'&gt; 6.1\n\n\nWe see that a was initially an integer with a value of 2. But we added 4.1 to it, so the Python interpreter knew to change its type to a float and update its value.\nThis operation of updating a value can also be accomplished with an increment operator.\n\na = 2\na += 4.1\na\n\n6.1\n\n\nThe += operator told the interpreter to take the value of a and add 4.1 to it, changing the type of a in the intuitive way if need be. The other six arithmetic operators have similar constructions, -=, *=, /=, //=, %=, and **=.\n\na = 2\na **= 3\na\n\n8",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#type-conversion",
    "href": "appendices/python/variables_operators_types.html#type-conversion",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.4 Type conversion",
    "text": "D.4 Type conversion\nSuppose you have a variable of one type, and you want to convert it to another. For example, say you have a string, '42', and you want to convert it to an integer. This would happen if you were reading information from a text file, which by definition is full of strings, and you wanted to convert some string to a number. This is done as follows.\n\nmy_str = '42'\nmy_int = int(my_str)\nprint(my_int, type(my_int))\n\n42 &lt;class 'int'&gt;\n\n\nConversely, we can convert an int back to a str.\n\nstr(my_int)\n\n'42'\n\n\nWhen converting a float to an int, the interpreter does not round the result, but gives the floor.\n\nint(2.9)\n\n2\n\n\nAlso consider our string concatenation warning/example from above:\n\nprint('4' + '2')\nprint(int('4') + int('2'))\n\n42\n6",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#relational-operators",
    "href": "appendices/python/variables_operators_types.html#relational-operators",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.5 Relational operators",
    "text": "D.5 Relational operators\nSuppose we want to compare the values of two numbers. We may want to know if they are equal for example. The operator used to test for equality is ==, an example of a relational operator (also called a comparison operator).\n\nD.5.1 The equality relational operator\nLet’s test out the == to see how it works.\n\n5 == 5\n\nTrue\n\n\n\n5 == 4\n\nFalse\n\n\nNotice that using the operator gives either True or False. These are important keywords in Python that indicate truth. True and False have a special type, called bool, short for Boolean.\n\ntype(True)\n\nbool\n\n\n\ntype(False)\n\nbool\n\n\nThe equality operator, like all relational operators in Python, also works with variables, testing for equality of their values. Equality of the variables themselves uses identity operators, described below.\n\na = 4\nb = 5\nc = 4\n\na == b\n\nFalse\n\n\n\na == c\n\nTrue\n\n\nNow, let’s try it out with some floats.\n\n5.3 == 5.3\n\nTrue\n\n\n\n2.1 + 3.2 == 5.3\n\nFalse\n\n\nYikes! Python is telling us that 2.1 + 3.2 is not 5.3. This is floating point arithmetic haunting us. Note that floating point numbers that can be exactly represented with binary numbers do not have this problem.\n\n2.2 + 3.2 == 5.4\n\nTrue\n\n\nThis behavior is unpredictable, so here is a rule.\n\nNever use the == operator with floats.\n\n\n\nD.5.2 Other relational operators\nAs you might expect, there are other relational operators. The relational operators are\n\n\n\nEnglish\nPython\n\n\n\n\nis equal to\n==\n\n\nis not equal to\n!=\n\n\nis greater than\n&gt;\n\n\nis less than\n&lt;\n\n\nis greater than or equal to\n&gt;=\n\n\nis less than or equal to\n&lt;=\n\n\n\nWe can try some of them out!\n\n4 &lt; 5\n\nTrue\n\n\n\n5.7 &lt;= 3\n\nFalse\n\n\n\n'michael jordan' &gt; 'lebron james'\n\nTrue\n\n\nWhoa. What happened on that last one? The Python interpreter has weighed in on the debate about the greatest basketball player of all time. It clearly thinks Michael Jordan is better than LeBron James, but that seems kind of subjective. To understand what the interpreter is doing, we need to understand how it compares strings.\n\n\nD.5.3 A brief aside on Unicode\nIn Python, characters are encoded with Unicode. This is a standardized library of characters from many languages around the world that contains over 100,000 characters. Each character has a unique number associated with it. We can access what number is assigned to a character using Python’s built-in ord() function.\n\nord('a')\n\n97\n\n\n\nord('λ')\n\n955\n\n\nThe relational operators on characters compare the values that the ord function returns. So, using a relational operator on 'a' and 'b' means you are comparing ord('a') and ord('b'). When comparing strings, the interpreter first compares the first character of each string. If they are equal, it compares the second character, and so on. So, the reason that 'michael jordan' &gt; 'lebron james' gives a value of True is because ord('m') &gt; ord('l').\nNote that a result of this scheme is that testing for equality of strings means that all characters must be equal. This is the most common use case for relational operators with strings.\n\n'lebron' == 'lebron james'\n\nFalse\n\n\n\n'lebron' == 'LeBron'\n\nFalse\n\n\n\n'LeBron James' == 'LeBron James'\n\nTrue\n\n\n\n'AGTCACAGTA' == 'AGTCACAGCA'\n\nFalse\n\n\n\n\nD.5.4 Chaining relational operators\nPython allow chaining of relational operators.\n\n4 &lt; 6 &lt; 6.1 &lt; 9.3\n\nTrue\n\n\n\n4 &lt; 6.1 &lt; 6 &lt; 9.3\n\nFalse\n\n\nThis is convenient do to. However, it is important not to do the following, even though it is legal.\n\n4 &lt; 6.1 &gt; 5\n\nTrue\n\n\nIn other words, do not mix the direction of the relational operators. You could run into trouble because, in this case, 5 and 4 are never compared. An expression with different relations among all three numbers also returns True.\n\n4 &lt; 6.1 &gt; 3\n\nTrue\n\n\nSo, I issue a warning.\n\nDo not mix the directions of chained relational operators.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#identity-operators",
    "href": "appendices/python/variables_operators_types.html#identity-operators",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.6 Identity operators",
    "text": "D.6 Identity operators\nIdentity operators check to see if two variables occupy the same space in memory; i.e., they are the same object (we’ll learn more about objects as we go along). This is different that the equality relational operator, ==, which checks to see if two variables have the same value. The two identity operators are in the table below.\n\n\n\nEnglish\nPython\n\n\n\n\nis the same object\nis\n\n\nis not the same object\nis not\n\n\n\nThat’s right. The operators are pretty much the same as English! Let’s see these operators in action and get at the difference between == and is. Let’s use the is operator to investigate how Python stored variables in memory, starting with floats.\n\na = 5.6\nb = 5.6\n\na == b, a is b\n\n(True, False)\n\n\nEven though a and b have the same value, they are stored in different places in memory. They can occupy the same place in memory if we do a b = a assignment.\n\na = 5.6\nb = a\n\na == b, a is b\n\n(True, True)\n\n\nBecause we assigned b = a, they necessarily have the same (immutable) value. So, the two variables occupy the same place in memory for efficiency.\n\na = 5.6\nb = a\na = 6.1\n\na == b, a is b\n\n(False, False)\n\n\nIn the last two examples, we see that assigning b = a, where a is a float in this case, means that a and b occupy the same memory. However, reassigning the value of a resulted in the interpreter placing a in a new space in memory. We can double check the values.\nIntegers sometimes do not behave the same way, however.\n\na = 5\nb = 5\n\na == b, a is b\n\n(True, True)\n\n\nEven though we assigned a and b separately, they occupy the same place in memory. This is because Python employs integer caching for all integers between -5 and 256. This caching does not happen for more negative or larger integers.\n\na = 350\nb = 350\n\na is b\n\nFalse\n\n\nNow, let’s look at strings.\n\na = 'Hello, world.'\nb = 'Hello, world.'\n\na == b, a is b\n\n(True, False)\n\n\nSo, even though a and b have the same value, they do not occupy the same place in memory. If we do a b = a assignment, we get similar results as with floats.\n\na = 'Hello, world.'\nb = a\n\na == b, a is b\n\n(True, True)\n\n\nLet’s try string assignment again with a different string.\n\na = 'python'\nb = 'python'\n\na == b, a is b\n\n(True, True)\n\n\nWait a minute! If we choose a string 'python', it occupies the same place in memory as another variable with the same value, but that was not the case for 'Hello, world.'. This is a result of Python also doing string interning which allows for (sometimes very) efficient string processing. Whether two strings occupy the same place in memory depends on what the strings are.\nThe caching and interning might be a problem, but you generally do not need to worry about it for immutable variables. Being immutable means that once the variables are created, their values cannot be changed. If we do change the value the variable gets a new place in memory. All variables we’ve encountered so far, ints, floats, and strs, are immutable. We will see encounter mutable data types in future lesson, in which case it really does matter practically to you as a programmer whether or not two variables are in the same location in memory.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#logical-operators",
    "href": "appendices/python/variables_operators_types.html#logical-operators",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.7 Logical operators",
    "text": "D.7 Logical operators\nLogical operators can be used to connect relational and identity operators. Python has three logical operators.\n\n\n\nLogic\nPython\n\n\n\n\nAND\nand\n\n\nOR\nor\n\n\nNOT\nnot\n\n\n\nThe and operator means that if both operands are True, return True. The or operator gives True if either of the operands are True. Finally, the not operator negates the logical result.\nThat might be as clear as mud to you. It is easier to learn this, as usual, by example.\n\nTrue and True\n\nTrue\n\n\n\nTrue and False\n\nFalse\n\n\n\nTrue or False\n\nTrue\n\n\n\nTrue or True\n\nTrue\n\n\n\nnot False and True\n\nTrue\n\n\n\nnot(False and True)\n\nTrue\n\n\n\nnot False or True\n\nTrue\n\n\n\nnot (False or True)\n\nFalse\n\n\n\n7 == 7 or 7.6 == 9.1\n\nTrue\n\n\n\n7 == 7 and 7.6 == 9.1\n\nFalse\n\n\nI think these examples will help you get the hang of it. Note that it is important to specify the ordering of your operations, particularly when using the not operator.\nNote also that\na &lt; b &lt; c\nis equivalent to\n(a &lt; b) and (b &lt; c)\nWith these new types of operators in hand, we can construct a more complete table of operator precedence.\n\n\n\nprecedence\noperators\n\n\n\n\n1\n**\n\n\n2\n*, /, //, %\n\n\n3\n+, -\n\n\n4\n&lt;, &gt;, &lt;=, &gt;=\n\n\n5\n==, !=\n\n\n6\n=, +=, -=, *=, /=, **=, %=, //=\n\n\n7\nis, is not\n\n\n8\nand, or, not",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#operators-we-left-out",
    "href": "appendices/python/variables_operators_types.html#operators-we-left-out",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.8 Operators we left out",
    "text": "D.8 Operators we left out\nWe have left out a few operators in Python. Two that we left out are the membership operators, in and not in, which we will visit in forthcoming sections of this appendix. The others we left out are bitwise operators and operators on sets, which we will not be covering.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#the-numerical-values-of-true-and-false",
    "href": "appendices/python/variables_operators_types.html#the-numerical-values-of-true-and-false",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.9 The numerical values of True and False",
    "text": "D.9 The numerical values of True and False\nAs we move to conditionals, it is important to take a moment to evaluate the numerical values of the keywords True and False. They have numerical values of 1 and 0, respectively.\n\nTrue == 1\n\nTrue\n\n\n\nFalse == 0\n\nTrue\n\n\nYou can do arithmetic on True and False, but you will get implicit type conversion.\n\nTrue + False\n\n1\n\n\n\ntype(True + False)\n\nint",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#conditionals",
    "href": "appendices/python/variables_operators_types.html#conditionals",
    "title": "Appendix D — Variables, operators, and types",
    "section": "D.10 Conditionals",
    "text": "D.10 Conditionals\nConditionals are used to tell your computer to do a set of instructions depending on whether or not a Boolean is True. In other words, we are telling the computer:\nif something is true:\n    do task a\notherwise:\n    do task b\nIn fact, the syntax in Python is almost exactly the same. As an example, let’s ask whether or not a codon is the canonical start codon (AUG).\n\ncodon = 'AUG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n\nThis codon is the start codon.\n\n\nThe syntax of the if statement is apparent in the above example. The Boolean expression, codon == 'AUG', is called the condition. If it is True, the indented statement below it is executed. This brings up a very important aspect of Python syntax.\n\nIndentation matters.\n\nAny lines with the same level of indentation will be evaluated together.\n\ncodon = 'AUG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n    print('Same level of intentation, so still printed!')\n\nThis codon is the start codon.\nSame level of intentation, so still printed!\n\n\nWhat happens if our codon is not the start codon?\n\ncodon = 'AGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n\nNothing is printed. This is because we did not tell Python what to do if the Boolean expression codon == 'AUG' evaluated False. We can add that with an else clause in the conditional.\n\ncodon = 'AGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelse:\n    print('This codon is not the start codon.')\n\nThis codon is not the start codon.\n\n\nGreat! Now, we have a construction that can choose which action to take depending on a value. So, if we’re zooming along an RNA sequence, we could pick out the start codon and infer where translation would start. Now, what if we want to know if we hit a canonical stop codon (UAA, UAG, or UGA)? We can nest the conditionals!\n\ncodon = 'UAG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelse:\n    if codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n        print('This codon is a stop codon.')\n    else:\n        print('This codon is neither a start nor stop codon.')\n\nThis codon is a stop codon.\n\n\nNotice that the indentation defines which clause the statement belongs to. E.g., the second if statement is executed as part of the first else clause.\nWhile this nesting is very nice, we can be more concise by using an elif clause.\n\ncodon = 'UGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/variables_operators_types.html#computing-environment",
    "href": "appendices/python/variables_operators_types.html#computing-environment",
    "title": "Appendix D — Variables, operators, and types",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.10.9\nIPython version      : 8.10.0\n\njupyterlab: 3.5.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html",
    "href": "appendices/python/lists_and_tuples.html",
    "title": "Appendix E — Lists and tuples",
    "section": "",
    "text": "E.1 Lists\n| Download notebook\nIn this tutorial, we will explore two important data types in Python, lists and tuples. They are both sequences of objects. Just like a string is a sequence (that is, an ordered collection) of characters, lists and tuples are sequences of arbitrary objects, called items or elements. They are a way to make a single object that contains many other objects. We will start our discussion with lists.\nAs usual, it is easiest to explore new topics by example. We’ll start by creating a list.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html#lists",
    "href": "appendices/python/lists_and_tuples.html#lists",
    "title": "Appendix E — Lists and tuples",
    "section": "",
    "text": "E.1.1 List creation\nWe create lists by putting Python values or expressions inside square brackets, separated by commas. For example:\n\nmy_list_1 = [1, 2, 3, 4]\ntype(my_list_1)\n\nlist\n\n\nWe observe here that although the elements of the list are ints, the type of the list is list. Actually, any Python expression can be inside a list (including another list!):\n\nmy_list_2 = [1, 2.4, 'a string', ['a string in another list', 5]]\nmy_list_2\n\n[1, 2.4, 'a string', ['a string in another list', 5]]\n\n\n\nmy_list_3 = [2+3, 5*3, 4**2]\nmy_list_3\n\n[5, 15, 16]\n\n\nmy_list_2 contains ints, a float, a string and another list. And our third list contains expressions that get evaluated when the list as a whole gets created.\nWe can also create a list by type conversion. For example, we can convert a string into a list of characters.\n\nmy_str = 'A string.'\nlist(my_str)\n\n['A', ' ', 's', 't', 'r', 'i', 'n', 'g', '.']\n\n\n\n\nE.1.2 List operators\nOperators on lists behave much like operators on strings. The + operator on lists means list concatenation.\n\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nThe * operator on lists means list replication and concatenation.\n\n[1, 2, 3] * 3\n\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\n\nE.1.2.1 Membership operators\nMembership operators are used to determine if an item is in a list. The two membership operators are:\n\n\n\nEnglish\noperator\n\n\n\n\nis a member of\nin\n\n\nis not a member of\nnot in\n\n\n\n\nThe result of the operator is True or False. Let’s look at my_list_2 again:\n\nmy_list_2 = [1, 2.4, 'a string', ['a string in another list', 5]]\n1 in my_list_2\n\nTrue\n\n\n\n['a string in another list', 5] in my_list_2\n\nTrue\n\n\n\n'a string in another list' in my_list_2\n\nFalse\n\n\n\n7 not in my_list_2\n\nTrue\n\n\nImportantly, we see that the string 'a string in another list' is not in my_list_2. This is because that string itself is not one of the four items of my_list_2. The string 'a string in another list' is in a list that is an item in my_list_2.\nNow, these membership operators offer a great convenience for conditionals. Remember our example about stop codons?\n\ncodon = 'UGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.\n\n\nWe can rewrite this much more cleanly, and with a lower chance of bugs, using a list and the in operator.\n\n# Make a list of stop codons\nstop_codons = ['UAA', 'UAG', 'UGA']\n\n# Specify codon\ncodon = 'UGG'\n\n# Check to see if it is a start or stop codon\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon in stop_codons:\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.\n\n\nThe simple expression\ncodon in stop_codons\nreplaced the more verbose\ncodon == 'UAA' or codon == 'UAG' or codon == 'UGA'\nMuch nicer!\n\n\n\nE.1.3 List indexing\nImagine that we would like to access an item in a list. Because a list is ordered, we can ask for the first item, the second item, the nth item, the last item, etc. This is done using a bracket notation. We first write the name of our list and then enclosed in square brackets we write the location (index) of the desired element:\n\nmy_list = [1, 2.4, 'a string', ['a string in another list', 5]]\n\nmy_list[1]\n\n2.4\n\n\nWait a minute! Shouldn’t my_list[1] give the first item in the list? It seems to give the second. This is because indexing in Python starts at zero. This is very important. (Historical note: Why Python uses 0-based indexing It is also worth reading Edsgar Dijkstra’s thoughts on the matter.)\n\nIndexing in Python starts at zero.\n\nNow that we know that, let’s look at the items in the list.\n\nprint(my_list[0])\nprint(my_list[1])\nprint(my_list[2])\nprint(my_list[3])\n\n1\n2.4\na string\n['a string in another list', 5]\n\n\nWe can also index the list that is within my_list by adding another set of brackets.\n\nmy_list[3][0]\n\n'a string in another list'\n\n\nSo, now we have the basics of list indexing. There are more ways to specify items in a list. We’ll look at some of these now, but in order to do it, it helps to have a simpler list. We’ll therefore create a list that goes from zero to ten.\n\nmy_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nmy_list[4]\n\n4\n\n\nWe already knew that would be the result. We can use negative indexing as well! This just means we start indexing from the last entry, starting at -1.\n\nmy_list[-1]\n\n10\n\n\n\nmy_list[-3]\n\n8\n\n\nThis is very convenient for indexing in reverse. Now make it more clear, here are the forward and backward indices for the list:\n\n\n\nValues\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nForward indices\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nReverse indices\n-11\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n\n\n\n\n\nE.1.4 List slicing\nNow, what if we want to pull out multiple items in a list, called slicing? We can use colons (:) for that.\n\nmy_list[0:5]\n\n[0, 1, 2, 3, 4]\n\n\nWe got elements 0 through 4. When using the colon indexing, my_list[i:j], we get items i through j-1. I.e., the range is inclusive of the first index and exclusive of the last. If the slice’s final index is larger than the length of the sequence, the slice ends at the last element.\n\nmy_list[3:1000]\n\n[3, 4, 5, 6, 7, 8, 9, 10]\n\n\nNow, we can also use negative indices with colons.\n\nmy_list[0:-3]\n\n[0, 1, 2, 3, 4, 5, 6, 7]\n\n\nAgain, note that we only went to index -4.\nWe can also specify a stride. The stride comes after a second colon. For example, if we only wanted the even numbers, we could do the following.\n\nmy_list[0::2]\n\n[0, 2, 4, 6, 8, 10]\n\n\nNotice that we did not enter anything for the end value of the slice. If the end is left blank, the default is to include the entire string. Similarly, we can leave out the start index, as its default is zero.\n\nmy_list[::2]\n\n[0, 2, 4, 6, 8, 10]\n\n\nSo, in general, the indexing scheme is:\n    my_list[start:end:stride]\n\nIf there are no colons, a single element is returned.\nIf there are any colons, we are slicing the list, and a list is returned.\nIf there is one colon, stride is assumed to be 1.\nIf start is not specified, it is assumed to be zero.\nIf end is not specified, the interpreted assumed you want the entire list.\nIf stride is not specified, it is assumed to be 1.\n\nWith this in hand, we do lots of crazy slicing. We can even use a negative stride, which results in reversing the list.\n\nmy_list[::-1]\n\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\nNote that the meaning of the “start” and “end” index is a bit ambiguous when you have a negative stride. When the stride is negative, we still slice from start to end, but then reverse the order.\nNow, let’s look at a few examples (inspired by Brett Slatkin).\n\nprint(my_list[2::2])\nprint(my_list[2:-1:2])\nprint(my_list[-2::-2])\nprint(my_list[-2:2:-2])\nprint(my_list[2:2:-2])\n\n[2, 4, 6, 8, 10]\n[2, 4, 6, 8]\n[9, 7, 5, 3, 1]\n[9, 7, 5, 3]\n[]\n\n\nYou can see that it takes a lot of thought to understand what the slices actually are. So, here is some good advice: Do not use start, end, and slice all at the same time (even though you can). Do the stride first and then the slice, on separate lines. For example, if we wanted just the even numbers, but not the first and last (this was the my_list[2:-1:2] example we just did), we would do\n\n# Extract evens\nevens = my_list[::2]\n\n# Cut off end values\nevens_without_end_values = evens[1:-1]\n\nevens_without_end_values\n\n[2, 4, 6, 8]\n\n\nThis is more verbose, but much easier to read and understand.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html#mutability",
    "href": "appendices/python/lists_and_tuples.html#mutability",
    "title": "Appendix E — Lists and tuples",
    "section": "E.2 Mutability",
    "text": "E.2 Mutability\nLists are mutable. That means that you can change their values without creating a new list. (You cannot change the data type or identity.) Let’s see this by example.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list[3] = 'four'\n\nmy_list\n\n[1, 2, 3, 'four', 5, 6, 7, 8, 9, 10]\n\n\nThe other data types we have encountered so far, ints, floats, and strs, are immutable. You cannot change their values without reassigning them. To see this, we’ll use the id() function, which tells us where in memory that the variable is stored. (Note: this identity is unique to the Python interpreter, and should not be considered an actual physical address in memory.)\n\na = 689\nprint(id(a))\n\na = 690\nprint(id(a))\n\n4400362064\n4400362832\n\n\nSo, we see that the identity of a, an integer, changed when we tried to change its value. So, we didn’t actually change its value; we made a new variable. With lists, though, this is not the case.\n\nprint(id(my_list))\n\nmy_list[0] = 'zero'\nprint(id(my_list))\n\n4402226560\n4402226560\n\n\nIt is still the same list! This is very important to consider when we do assignments.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html#pitfall-aliasing",
    "href": "appendices/python/lists_and_tuples.html#pitfall-aliasing",
    "title": "Appendix E — Lists and tuples",
    "section": "E.3 Pitfall: Aliasing",
    "text": "E.3 Pitfall: Aliasing\nAliasing is a subtle issue which can come up when assigning lists to variables. Let’s look at an example. We will make a list, then assign a new variable to the list (which we will momentarily erroneously think of as making a copy of the list) and then change a value of an entry in the “copied” list.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list_2 = my_list     # copy of my_list?\nmy_list_2[0] = 'a'\n\nmy_list_2\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nNow, let’s look at our original list to see what it looks like.\n\nmy_list\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nSo we see that assigning a list to a variable does not copy the list! Instead, you just get a new reference to the same value. This has the real potential to introduce a nasty bug that will bite you!\nThere is a way we can avoid this problem by using list slices. If both the slice’s starting index and the slice’s ending index of a list are left out, the slice is a copy of the entire list in a new hunk of memory.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list_2 = my_list[:]\nmy_list_2[0] = 'a'\n\nmy_list_2\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nmy_list\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nAnother option is to use a data type that is very much like a list, except it is immutable.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html#tuples",
    "href": "appendices/python/lists_and_tuples.html#tuples",
    "title": "Appendix E — Lists and tuples",
    "section": "E.4 Tuples",
    "text": "E.4 Tuples\nA tuple is just like a list, except it is immutable (basically a read-only list). (What I just said there is explosive, as described in this blog post. Tuples do have many other capabilities beyond what you would expect from just bring “a read-only list,” but for us just beginning now, we can think of it that way.) A tuple is created just like a list, except we use parentheses instead of brackets. The only watch-out is that a tuple with a single item needs to include a comma after the item.\n\nmy_tuple = (0,)\n\nnot_a_tuple = (0) # this is just the number 0 (normal use of parantheses)\n\ntype(my_tuple), type(not_a_tuple)\n\n(tuple, int)\n\n\nWe can also create a tuple by doing a type conversion. We can convert our list to a tuple.\n\nmy_list = [1, 2.4, 'a string', ['a sting in another list', 5]]\n\nmy_tuple = tuple(my_list)\n\nmy_tuple\n\n(1, 2.4, 'a string', ['a sting in another list', 5])\n\n\nNote that the list within my_list did not get converted to a tuple. It is still a list, and it is mutable.\n\nmy_tuple[3][0] = 'a string in a list in a tuple'\n\nmy_tuple\n\n(1, 2.4, 'a string', ['a string in a list in a tuple', 5])\n\n\nHowever, if we try to change an item in a tuple, we get an error.\n\nmy_tuple[1] = 7\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 my_tuple[1] = 7\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nEven though the list within the tuple is mutable, we still cannot change the identity of that list.\n\nmy_tuple[3] = ['a', 'new', 'list']\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[38], line 1\n----&gt; 1 my_tuple[3] = ['a', 'new', 'list']\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\nE.4.1 Slicing of tuples\nSlicing of tuples is the same as lists, except a tuple is returned from the slicing operation, not a list.\n\nmy_tuple = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Reverse\nmy_tuple[::-1]\n\n(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n\n\n# Odd numbers\nmy_tuple[1::2]\n\n(1, 3, 5, 7, 9)\n\n\n\n\nE.4.2 The + operator with tuples\nAs with lists we can concatenate tuples with the + operator.\n\nmy_tuple + (11, 12, 13, 14, 15)\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\n\n\n\n\nE.4.3 Membership operators with tuples\nMembership operators work the same as with lists.\n\n5 in my_tuple\n\nTrue\n\n\n\n'LeBron James' not in my_tuple\n\nTrue\n\n\n\n\nE.4.4 Tuple unpacking\nIt is like a multiple assignment statement that is best seen through example.\n\nmy_tuple = (1, 2, 3)\n(a, b, c) = my_tuple\n\na\n\n1\n\n\n\nb\n\n2\n\n\n\nc\n\n3\n\n\nThis is useful when we want to return more than one value from a function and further using the values as stored in different variables. We will make use of this as the bootcamp goes on; we’ll be writing functions in just a couple lessons!\nNote that the parentheses are dispensable.\n\na, b, c = my_tuple\n\nprint(a, b, c)\n\n1 2 3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html#wisdom-on-tuples-and-lists",
    "href": "appendices/python/lists_and_tuples.html#wisdom-on-tuples-and-lists",
    "title": "Appendix E — Lists and tuples",
    "section": "E.5 Wisdom on tuples and lists",
    "text": "E.5 Wisdom on tuples and lists\nAt face, tuples and lists are very similar, differing essentially only in mutability. The differences are actually more profound, as described in the aforementioned blog post. We will make extensive use of them in our programs.\n“When should I use a tuple and when should I use a list?” you ask. Here is my advice.\n\nAlways use tuples instead of lists unless you need mutability.\n\nThis keeps you out of trouble. It is very easy to inadvertently change one list, and then another list (that is actually the same, but with a different variable name) gets mangled. That said, mutability is often very useful, so you can use it to make your list and adjust it as you need. However, after you have finalized your list, you should convert it to a tuple so it cannot get mangled. We’ll come back to this later in the bootcamp.\nSo, I ask you, which is better?\n\n# Should it be a list?\nstop_codons = ['UAA', 'UAG', 'UGA']\n\n# or a tuple?\nstop_codons = ('UAA', 'UAG', 'UGA')",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/lists_and_tuples.html#computing-environment",
    "href": "appendices/python/lists_and_tuples.html#computing-environment",
    "title": "Appendix E — Lists and tuples",
    "section": "E.6 Computing environment",
    "text": "E.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python/iteration.html",
    "href": "appendices/python/iteration.html",
    "title": "Appendix F — Iteration",
    "section": "",
    "text": "F.1 Introducing the for loop\n| Download notebook\nWe often want a computer program to do a similar operation many times. For example, we may want to analyze many codons, one after another, and find the start and stop codons in order to determine the length of the open reading frame. Or, as a simpler example, we may wish to find the GC content of a specific sequence. We check each base to see if it is G or C and keep a count. Doing these repeated operations in a program is called iteration.\nThe first method of iteration we will discuss is the for loop. As an example of its use, we compute the GC content of an RNA sequence.\n# The sequence we want to analyze\nseq = 'GACAGACUCCAUGCACGUGGGUAUCUGUC'\n\n# Initialize GC counter\nn_gc = 0\n\n# Initialize sequence length\nlen_seq = 0\n\n# Loop through sequence and count G's and C's\nfor base in seq:\n    len_seq += 1\n    if base in 'GCgc':\n        n_gc += 1\n        \n# Divide to get GC content\nn_gc / len_seq\n\n0.5517241379310345\nLet’s look carefully at what we did here. We took a string containing a sequence of nucleotides and then we did something for each character (base) in that string (nucleic acid sequence). A string is a sequence in the sense of the programming language as well; just like a list or tuple, the string is an ordered collection of characters. (So as not to confuse between biological sequences and sequences as a part of the Python language, we will always write the latter in italics.)\nNow, let’s translate the new syntax in the above code to English.\nPython: for base in seq:\nEnglish: for each character in the string whose variable name is seq, do the following, with that character taking the name base\nThis exposes a general way of doing things repeatedly in Python. For every item in a sequence, we do something. That something follows the for clause and is contained in an indentation block. When we do this, we say are “looping over a sequence.” In the context of a for clause, the membership operator, in, means that we consider in order each item in the sequence or iterator (we’ll talk about iterators in a moment).\nNow, looking within the loop, the first thing we do is increment the length of the sequence. For each base we encounter in the loop, we add one to the sequence length. Makes sense!\nNext, we have an if statement. We use the membership operator again. We ask if the current base is a G or a C. To be safe, we also included lower case characters in case the sequence was entered that way. If the base is a G or a C, we increment the counter of GC bases by one.\nFinally, we get the fractional GC content by dividing the number of GC’s by the total length of the sequence.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python/iteration.html#introducing-the-for-loop",
    "href": "appendices/python/iteration.html#introducing-the-for-loop",
    "title": "Appendix F — Iteration",
    "section": "",
    "text": "F.1.1 Aside: the len() function\nNote in the last example that we determined the length of the RNA sequence by iterating the len_seq counter within the for loop. This works, but Python has a built-in function (like print() is a built-in function) to compute the length of a string (or list, tuple, or other sequence). To find out the length of a sequence, simply use it as an argument to the len() function.\n\nlen(seq)\n\n29\n\n\n\n\nF.1.2 Example and watchout: modifying a list\nLet’s look at another example of iterating through a list. Say we have a list of integers, and we want to change it by doubling each one. Let’s just do it.\n\n# We'll do one through 5\nmy_integers = [1, 2, 3, 4, 5]\n\n# Double each one\nfor n in my_integers:\n    n *= 2\n    \n# Check out the result\nmy_integers\n\n[1, 2, 3, 4, 5]\n\n\nWhoa! It didn’t seem to double any of the integers! This is because my_integers was converted to an iterator in the for clause, and the iterator returns a copy of the item in a list, not a reference to it. Therefore, the n inside the for block is not a view into the original list, and doubling it does nothing meaningful.\nWe’ve seen how to change individual list elements with indexing:\n\n# Don't do things this way\nmy_integers[0] *= 2\nmy_integers[1] *= 2\nmy_integers[2] *= 2\nmy_integers[3] *= 2\nmy_integers[4] *= 2\n\nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nBut we’d obviously like a better way to do this, with less typing and without knowing ahead of time the length of the list. Let’s look at a new concept that will help with this example.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python/iteration.html#iterators",
    "href": "appendices/python/iteration.html#iterators",
    "title": "Appendix F — Iteration",
    "section": "F.2 Iterators",
    "text": "F.2 Iterators\nIn the previous example, we iterated over a sequence. A sequence is one of many iterable objects, called iterables. Under the hood, the Python interpreter actually converts an iterable to an iterator. An iterator is a special object that gives values in succession. A major difference between a sequence and an iterator is that you cannot index an iterator. This seems like a trivial difference, but iterators make for more efficient computing than directly using a sequence with indexing.\nWe can explicitly convert a sequence to an iterator using the built-in function iter(), but we will not bother with that here because the Python interpreter automatically does this for you when you use a sequence in a loop. (This is incidentally why the previous example didn’t work; when the list is converted to an iterator, a copy is made of each element so the original list is unchanged.)\nInstead, we will now explore how we can create useful iterators using the range(), enumerate(), and zip() functions. I know we have not yet covered functions, but the syntax should not be so complicated that you cannot understand what these functions are doing, just like with the print() and len() functions.\n\nF.2.1 The range() function\nThe range() function gives an iterable that enables counting. Let’s look at an example.\n\nfor i in range(10):\n    print(i, end='  ')\n\n0  1  2  3  4  5  6  7  8  9  \n\n\nWe see that range(10) gives us ten numbers, from 0 to 9. As with indexing, range() inclusively starts at zero by default, and the ending is exclusive.\nIt turns out that the arguments of the range() function work much like indexing. If you have a single argument, you get that many integers, starting at 0 and incrementing by one. If you give two arguments, you start inclusively at the first and increment by one ending exclusively at the second argument. Finally, you can specify a stride with the third argument.\n\n# Print numbers 2 through 9\nfor i in range(2, 10):\n    print(i, end='  ')\n\n# Print a newline\nprint()\n    \n# Print even numbers, 2 through 9\nfor i in range(2, 10, 2):\n    print(i, end='     ')\n\n2  3  4  5  6  7  8  9  \n2     4     6     8     \n\n\nIt is often useful to make a list or tuple that has the same entries that a corresponding range object would have. We can do this with type conversion.\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nWe can use the range() function along with the len() function on lists to double the elements of our list from a bit ago:\n\nmy_integers = [1, 2, 3, 4, 5]\n\n# Since len(my_integers) = 5, this takes i from 0 to 4, \n# exactly the indices of my_integers\nfor i in range(len(my_integers)):\n    my_integers[i] *= 2\n    \nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nThis works, but there is an even better way to do this, with the next function.\n\n\nF.2.2 The enumerate() function\nLet’s say we want to print the indices of all G bases in a DNA sequence. We could do this by modifying our previous program.\n\n# Initialize GC counter\nn_gc = 0\n\n# Initialized sequence length\nlen_seq = 0\n\n# Loop through sequence and print index of G's\nfor base in seq:\n    if base in 'Gg':\n        print(len_seq, end='  ')\n    len_seq += 1\n\n0  4  12  16  18  19  20  26  \n\n\nThis is not so bad, but there is an easier way to do this. The enumerate() function gives an iterator that provides both the index and the item of a sequence. Again, this is best demonstrated in practice.\n\n# Loop through sequence and print index of G's\nfor i, base in enumerate(seq):\n    if base in 'Gg':\n        print(i, end='  ')\n\n0  4  12  16  18  19  20  26  \n\n\nThe enumerate() function allowed us to use an index and a base at the same time. To make it more clear, we can print the index and base type for each base in the sequence.\n\n# Print index and identity of bases\nfor i, base in enumerate(seq):\n    print(i, base)\n\n0 G\n1 A\n2 C\n3 A\n4 G\n5 A\n6 C\n7 U\n8 C\n9 C\n10 A\n11 U\n12 G\n13 C\n14 A\n15 C\n16 G\n17 U\n18 G\n19 G\n20 G\n21 U\n22 A\n23 U\n24 C\n25 U\n26 G\n27 U\n28 C\n\n\nThe enumerate() function is really useful and should be used in favor of just doing indexing. For example, many programmers, especially those first trained in lower-level languages, would write the above code similar to how we did the list doubling, with the range() and len() functions, but this is not good practice in Python.\nUsing enumerate(), the list doubling code looks like this:\n\nmy_integers = [1, 2, 3, 4, 5]\n\n# Double each one\nfor i, _ in enumerate(my_integers):\n    my_integers[i] *= 2\n    \n# Check out the result\nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nenumerate() is more generic and the overhead for returning a reference to an object isn’t an issue. The range(len()) construct will break on an object without support for len(). In addition, you are more likely to introduce bugs by imposing indexing on objects that are iterable but not unambiguously indexable. It is better to use the enumerate() function.\nNote that we used the underscore, _, as a throwaway variable that we do not use. There is no rule for this, but this is generally accepted Python syntax and helps signal that you are not going to use the variable.\nOne last gotcha: if we tried to do a similar technique with a string, we get a TypeError because a string is immutable. We’ll revisit examples like this in the lesson on string methods.\n\n# Try to convert capital G to lower g\nfor i, base in enumerate(seq):\n    if base == 'G':\n        seq[i] = 'g'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 4\n      2 for i, base in enumerate(seq):\n      3     if base == 'G':\n----&gt; 4         seq[i] = 'g'\n\nTypeError: 'str' object does not support item assignment\n\n\n\n\n\nF.2.3 The zip() function\nThe zip() function enables us to iterate over several iterables at once. In the example below we iterate over the jersey numbers, names, and positions of the LAFC players who scored in the 2022 MLS Cup Final.\n\nnames = ('Acosta', 'Murillo', 'Bale')\npositions = ('MF', 'D', 'F')\nnumbers = (23, 3, 11)\n\nfor num, pos, name in zip(numbers, positions, names):\n    print(num, name, pos)\n\n23 Acosta MF\n3 Murillo D\n11 Bale F\n\n\n\n\nF.2.4 The reversed() function\nThis function is useful for giving an iterator that goes in the reverse direction. We’ll see that this can be convenient in the next lesson. For now, let’s pretend we’re NASA and need to count down.\n\ncount_up = ('ignition', 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10)\n\nfor count in reversed(count_up):\n    print(count)\n\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nignition",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python/iteration.html#the-while-loop",
    "href": "appendices/python/iteration.html#the-while-loop",
    "title": "Appendix F — Iteration",
    "section": "F.3 The while loop",
    "text": "F.3 The while loop\nThe for loop is very powerful and allows us to construct iterative calculations. When we use a for loop, we need to set up an iterator. A while loop, on the other hand, allows iteration until a conditional expression evaluates False.\nAs an example of a while loop, we will calculate the length of a sequence before hitting a start codon.\n\n# Define start codon\nstart_codon = 'AUG'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon\nwhile seq[i:i+3] != start_codon:\n    i += 1\n    \n# Show the result\nprint('The start codon starts at index', i)\n\nThe start codon starts at index 10\n\n\nLet’s walk through the while loop. The value of i is changing with each iteration, being incremented by one. Each time we consider doing another iteration, the conditional is checked: do the next three bases match the start codon? We set up the conditional to evaluate to True when the bases are not the start codon, so the iteration continues. In other words, iteration continues in a while loop until the conditional returns False.\nNotice that we sliced the string the same way we sliced lists and tuples. In the case of strings, a slice gives another string, i.e., a sequential collection of characters.\nLet’s try looking for another codon…. But, let’s actually not do that. If you run the code below, it will run forever and nothing will get printed to the screen.\n# Define codon of interest\ncodon = 'GCC'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon, but DON'T DO THIS!!!!!\nwhile seq[i:i+3] != codon:\n    i += 1\n    \n# Show the result\nprint('The codon starts at index', i)\nThe reason this runs forever is that the conditional expression in the while statement never returns False. If we slice a string beyond the length of the string we get an empty string result.\n\nseq[100:103]\n\n''\n\n\nThis does not equal the codon we’re interested in, so the while loop keeps going. Forever. This is called an infinite loop, and you definitely to not want these in your code! We can fix it by making a conditional that will evaluate to False if we reach the end of the string.\n\n# Define codon of interest\ncodon = 'GCC'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon or the end of the sequence\nwhile seq[i:i+3] != codon and i &lt; len(seq):\n    i += 1\n    \n# Show the result\nif i == len(seq):\n    print('Codon not found in sequence.')\nelse:\n    print('The codon starts at index', i)\n\nCodon not found in sequence.\n\n\n\nF.3.1 for vs while\nMost anything that requires a loop can be done with either a for loop or a while loop, but there’s a general rule of thumb for which type of loop to use. If you know how many times you have to do something (or if your program knows), use a for loop. If you don’t know how many times the loop needs to run until you run it, use a while loop. For example, when we want to do something with each character in a string or each entry in a list, the program knows how long the sequence is and a for loop is more appropriate. In the previous examples, we don’t know how long it will be before we hit the start codon; it depends on the sequence you put into the program. That makes it more suited to a while loop.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python/iteration.html#the-break-and-else-keywords",
    "href": "appendices/python/iteration.html#the-break-and-else-keywords",
    "title": "Appendix F — Iteration",
    "section": "F.4 The break and else keywords",
    "text": "F.4 The break and else keywords\nIteration stops in a for loop when the iterator is exhausted. It stops in a while loop when the conditional evaluates to False. These is another way to stop iteration: the break keyword. Whenever break is encountered in a for or while loop, the iteration halts and execution continues outside the loop. As an example, we’ll do the calculation above with a for loop with a break instead of a while loop.\n\n# Define start codon\nstart_codon = 'AUG'\n\n# Scan sequence until we hit the start codon\nfor i in range(len(seq)):\n    if seq[i:i+3] == start_codon:\n        print('The start codon starts at index', i)\n        break\nelse:\n    print('Codon not found in sequence.')\n\nThe start codon starts at index 10\n\n\nNotice that we have an else block after the for loop. In Python, for and while loops can have an else statement after the code block to be evaluated in the loop. The contents of the else block are evaluated if the loop completes without encountering a break.\n\nF.4.1 A note about use of the word “codon”\nThe astute biologists among you will note that we have not really been using the word “codon” properly here. We are taking it to mean any three consecutive bases, but the more precise definition of a codon means that it is three consecutive bases that code for an amino acid. That means that for a three-base sequence to be a codon, it must be in-register with the start codon.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python/iteration.html#computing-environment",
    "href": "appendices/python/iteration.html#computing-environment",
    "title": "Appendix F — Iteration",
    "section": "F.5 Computing environment",
    "text": "F.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html",
    "href": "appendices/python/intro_to_functions.html",
    "title": "Appendix G — Introduction to functions",
    "section": "",
    "text": "G.1 Basic function syntax\n| Download notebook\nA function is a key element in writing programs. You can think of a function in a computing language in much the same way you think of a mathematical function. The function takes in arguments, performs some operation based on the identities of the arguments, and then returns a result. For example, the mathematical function\n\\[\\begin{align}\nf(x, y) = \\frac{x}{y}\n\\end{align}\\]\ntakes arguments \\(x\\) and \\(y\\) and then returns the ratio between the two, \\(x/y\\). In this lesson, we will learn how to construct functions in Python.\nFor our first example, we will translate the above function into Python. A function is defined using the def keyword. This is best seen by example.\ndef ratio(x, y):\n    \"\"\"The ratio of `x` to `y`.\"\"\"\n    return x / y\nFollowing the def keyword is a function signature which indicates the function’s name and its arguments. Just like in mathematics, the arguments are separated by commas and enclosed in parentheses. The indentation following the def line specifies what is part of the function. As soon as the indentation goes to the left again, aligned with def, the contents of the functions are complete.\nImmediately following the function definition is the doc string (short for documentation string), a brief description of the function. The first string after the function definition is always defined as the doc string. Usually, it is in triple quotes, as doc strings often span multiple lines.\nDoc strings are more than just comments for your code, the doc string is what is returned by the native python function help() when someone is looking to learn more about your function. For example:\nhelp(ratio)\n\nHelp on function ratio in module __main__:\n\nratio(x, y)\n    The ratio of `x` to `y`.\nThey are also printed out when you use the ? in a Jupyter notebook or JupyterLab console.\nratio?\n\n\nSignature: ratio(x, y)\nDocstring: The ratio of `x` to `y`.\nFile:      /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_7590/1007532851.py\nType:      function\nYou are free to type whatever you like in doc strings, or even omit them, but you should always have a doc string with some information about what your function is doing. True, this example of a function is kind of silly, since it is easier to type x / y than ratio(x, y), but it is still good form to have a doc string. This is worth saying explicitly.\nIn the next line of the function, we see a return keyword. Whatever is after the return statement is, you guessed it, returned by the function. Any code after the return is not executed because the function has already returned!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#basic-function-syntax",
    "href": "appendices/python/intro_to_functions.html#basic-function-syntax",
    "title": "Appendix G — Introduction to functions",
    "section": "",
    "text": "All functions should have doc strings.\n\n\n\nG.1.1 Calling a function\nNow that we have defined our function, we can call it.\n\nratio(5, 4)\n\n1.25\n\n\n\nratio(4, 2)\n\n2.0\n\n\n\nratio(90.0, 8.4)\n\n10.714285714285714\n\n\nIn each case, the function returns a float with the ratio of its arguments.\n\n\nG.1.2 Functions need not have arguments\nA function does not need arguments. As a silly example, let’s consider a function that just returns 42 every time. Of course, it does not matter what its arguments are, so we can define a function without arguments.\n\ndef answer_to_the_ultimate_question_of_life_the_universe_and_everything():\n    \"\"\"Simpler program than Deep Thought's, I bet.\"\"\"\n    return 42\n\nWe still needed the open and closed parentheses at the end of the function name. Similarly, even though it has no arguments, we still have to call it with parentheses.\n\nanswer_to_the_ultimate_question_of_life_the_universe_and_everything()\n\n42\n\n\n\n\nG.1.3 Functions need not return anything\nJust like they do not necessarily need arguments, functions also do not need to return anything. If a function does not have a return statement (or it is never encountered in the execution of the function), the function runs to completion and returns None by default. None is a special Python keyword which basically means “nothing.” For example, a function could simply print something to the screen.\n\ndef think_too_much():\n    \"\"\"Express Caesar's skepticism about Cassius\"\"\"\n    print(\"\"\"Yond Cassius has a lean and hungry look,\nHe thinks too much; such men are dangerous.\"\"\")\n\nWe call this function as all others, but we can show that the result it returns is None.\n\nreturn_val = think_too_much()\n\n# Print a blank line\nprint()\n\n# Print the return value\nprint(return_val)\n\nYond Cassius has a lean and hungry look,\nHe thinks too much; such men are dangerous.\n\nNone",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#built-in-functions-in-python",
    "href": "appendices/python/intro_to_functions.html#built-in-functions-in-python",
    "title": "Appendix G — Introduction to functions",
    "section": "G.2 Built-in functions in Python",
    "text": "G.2 Built-in functions in Python\nThe Python programming language has several built-in functions. We have already encountered print(), id(), ord(), len(), range(), enumerate(), zip(), and reversed(), in addition to type conversions such as list(). The complete set of built-in functions can be found here. A word of warning about these functions and naming your own.\n\nNever define a function or variable with the same name as a built-in function.\n\nAdditionally, Python has keywords (such as def, for, in, if, True, None, etc.), many of which we have already encountered. A complete list of them is here. The interpreter will throw an error if you try to define a function or variable with the same name as a keyword.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#an-example-function-reverse-complement",
    "href": "appendices/python/intro_to_functions.html#an-example-function-reverse-complement",
    "title": "Appendix G — Introduction to functions",
    "section": "G.3 An example function: reverse complement",
    "text": "G.3 An example function: reverse complement\nLet’s write a function that does not do something so trivial as computing ratios or giving us the Answer to the Ultimate Question of Life, the Universe, and Everything. We’ll write a function to compute the reverse complement of a sequence of DNA. Within the function, we’ll use some of our newly acquired iteration skills.\n\ndef complement_base(base):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    if base in 'Aa':\n        return 'T'\n    elif base in 'Tt':\n        return 'A'\n    elif base in 'Gg':\n        return 'C'\n    else:\n        return 'G'\n\n\ndef reverse_complement(seq):\n    \"\"\"Compute reverse complement of a sequence.\"\"\"\n    # Initialize reverse complement\n    rev_seq = ''\n    \n    # Loop through and populate list with reverse complement\n    for base in reversed(seq):\n        rev_seq += complement_base(base)\n        \n    return rev_seq\n\nNote that we do not have error checking here, which we should definitely do, but we’ll cover that in a future lesson. For now, let’s test it to see if it works.\n\nreverse_complement('GCAGTTGCA')\n\n'TGCAACTGC'\n\n\nIt looks good, but we might want to write yet another function to display the template strand (from 5\\('\\) to 3\\('\\)) above its reverse complement (from 3\\('\\) to 5\\('\\)). This makes it easier to verify.\n\ndef display_complements(seq):\n    \"\"\"Print sequence above its reverse complement.\"\"\"\n    # Compute the reverse complement\n    rev_comp = reverse_complement(seq)\n    \n    # Print template\n    print(seq)\n    \n    # Print \"base pairs\"\n    for base in seq:\n        print('|', end='')\n    \n    # Print final newline character after base pairs\n    print()\n            \n    # Print reverse complement\n    for base in reversed(rev_comp):\n        print(base, end='')\n        \n    # Print final newline character\n    print()\n\nLet’s call this function and display the input sequence and the reverse complement returned by the function.\n\nseq = 'GCAGTTGCA'\ndisplay_complements(seq)\n\nGCAGTTGCA\n|||||||||\nCGTCAACGT\n\n\nOk, now it’s clear that the result looks good! This example demonstrates an important programming principle regarding functions. We used three functions to compute and display the reverse complement.\n\ncomplement_base() gives the Watson-Crick complement of a given base.\nreverse_complement() computes the reverse complement.\ndisplay_complements() displays the sequence and the reverse complement.\n\nWe could very well have written a single function to compute the reverse complement with the if statements included within the for loop. Instead, we split this larger operation up into smaller functions. This is an example of modular programming, in which the desired functionality is split up into small, independent, interchangeable modules. This is a very, very important concept.\n\nWrite small functions that do single, simple tasks.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#pause-and-think-about-testing",
    "href": "appendices/python/intro_to_functions.html#pause-and-think-about-testing",
    "title": "Appendix G — Introduction to functions",
    "section": "G.4 Pause and think about testing",
    "text": "G.4 Pause and think about testing\nLet’s pause for a moment and think about what the complement_base() and reverse_complement() functions do. They do a well-defined operation on string inputs. If we’re doing some bioinformatics, we might use these functions over and over again. We should therefore thoroughly test the functions. For example, we should test that reverse_complement('GCAGTTGCA') returns 'TGCAACTGC'. For now, we will proceed without writing tests, but we will soon cover test-driven development, in which your functions are built around tests. For now, I will tell you this: If your functions are not thoroughly tested, you are entering a world of pain. A world of pain. Test your functions.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#keyword-arguments",
    "href": "appendices/python/intro_to_functions.html#keyword-arguments",
    "title": "Appendix G — Introduction to functions",
    "section": "G.5 Keyword arguments",
    "text": "G.5 Keyword arguments\nNow let’s say that instead of the reverse DNA complement, we want the reverse RNA complement. We could re-write the complement_base() function to do this. Better yet, let’s modify it.\n\ndef complement_base(base, material='DNA'):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    if base in 'Aa':\n        if material == 'DNA':\n            return 'T'\n        elif material == 'RNA':\n            return 'U'\n    elif base in 'TtUu':\n        return 'A'\n    elif base in 'Gg':\n        return 'C'\n    else:\n        return 'G'\n    \ndef reverse_complement(seq, material='DNA'):\n    \"\"\"Compute reverse complement of a sequence.\"\"\"\n    # Initialize reverse complement\n    rev_seq = ''\n    \n    # Loop through and populate list with reverse complement\n    for base in reversed(seq):\n        rev_seq += complement_base(base, material=material)\n        \n    return rev_seq\n\nWe have added a named keyword argument, also known as a named kwarg. The syntax for a named kwarg is\nkwarg_name=default_value\nin the def clause of the function definition. In this case, we say that the default material is DNA, but we could call the function with another material (RNA). Conveniently, when you call the function and omit the kwargs, they take on the default value within the function. So, if we wanted to use the default material of DNA, we don’t have to do anything different in the function call.\n\nreverse_complement('GCAGTTGCA')\n\n'TGCAACTGC'\n\n\nBut, if we want RNA, we can use the kwarg. We use the same syntax to call it that we did when defining it.\n\nreverse_complement('GCAGTTGCA', material='RNA')\n\n'UGCAACUGC'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#calling-a-function-with-a-splat",
    "href": "appendices/python/intro_to_functions.html#calling-a-function-with-a-splat",
    "title": "Appendix G — Introduction to functions",
    "section": "G.6 Calling a function with a splat",
    "text": "G.6 Calling a function with a splat\nPython offers another convenient way to call functions. Say a function takes three arguments, a, b, and c, taken to be the sides of a triangle, and determines whether or not the triangle is a right triangle. I.e., it checks to see if \\(a^2 + b^2 = c^2\\).\n\ndef is_almost_right(a, b, c):\n    \"\"\"\n    Checks to see if a triangle with side lengths\n    `a`, `b`, and `c` is right.\n    \"\"\"\n    # Use sorted(), which gives a sorted list\n    a, b, c = sorted([a, b, c])\n    \n    # Check to see if it is almost a right triangle\n    if abs(a**2 + b**2 - c**2) &lt; 1e-12:\n        return True\n    else:\n        return False\n\nRemember our warning from before: never use equality checks with floats. We therefore just check to see if the Pythagorean theorem almost holds. The function works as expected.\n\nis_almost_right(13, 5, 12)\n\nTrue\n\n\n\nis_almost_right(1, 1, 1.4)\n\nFalse\n\n\nNow, let’s say we had a tuple with the triangle side lengths in it.\n\nside_lengths = (13, 5, 12)\n\nWe can pass these all in separately by splitting the tuple but putting a * in front of it. A * before a tuple used in this way is referred an unpacking operator, and is referred to by some programmers as a “splat.”\n\nis_almost_right(*side_lengths)\n\nTrue\n\n\nThis can be very convenient, and we will definitely use this feature later in the bootcamp when we do some string formatting.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#anonymous-a.k.a.-lambda-functions",
    "href": "appendices/python/intro_to_functions.html#anonymous-a.k.a.-lambda-functions",
    "title": "Appendix G — Introduction to functions",
    "section": "G.7 Anonymous (a.k.a. lambda) functions",
    "text": "G.7 Anonymous (a.k.a. lambda) functions\nSo far, we have written functions using a def statement, followed by an indented block of code defining what will be executed when the function is called. Sometimes the functions are very short, like the ratio() function at the beginning of this lesson. We might wish to more succinctly define a function like this. Python’s lambda keyword enables this. As an example, let’s look at how we could define the ratio() function from before.\n\nf = lambda x, y: x / y\n\nf(3, 5)\n\n0.6\n\n\nThe syntax for defining an anonymous function, which must be on one line, is as follows.\n\nThe keyword lambda.\nThe arguments of the anonymous function, separated by commas.\nAn expression that is the return value of the function.\n\nYou may be thinking that anonymous functions run contrary to the idea that all functions should have doc strings. You’re right. Anonymous functions are typically only used when another function requires a function as an argument. In the is_almost_right() function above, we employed the sorted() function is used to sort a list. The sorted() function takes a key keyword argument that gives a function that is applied to each entry in the list and then the values returned by that function are used in the sorting. As an example, we can sort a list of names of goalscorers for LAFC in the epic 2022 MLS Cup final.\n\nsorted(['Kellyn Acosta', 'Jesus Murillo', 'Gareth Bale'])\n\n['Gareth Bale', 'Jesus Murillo', 'Kellyn Acosta']\n\n\nThis sorted by their first names, but we want to sort for their last names. As we will learn in the lesson on string methods, we can find the index of a space in a string using my_string.find(' '), so that the letter at the start of a last name for a player with string x is x[x.find(' ')+1]. We can use a lambda function to give this as the key and get a nicely sorted list.\n\nsorted(['Kellyn Acosta', 'Jesus Murillo', 'Gareth Bale'], key=lambda x: x[x.find(' ')+1])\n\n['Kellyn Acosta', 'Gareth Bale', 'Jesus Murillo']",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_functions.html#computing-environment",
    "href": "appendices/python/intro_to_functions.html#computing-environment",
    "title": "Appendix G — Introduction to functions",
    "section": "G.8 Computing environment",
    "text": "G.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html",
    "href": "appendices/python/string_methods.html",
    "title": "Appendix H — String methods",
    "section": "",
    "text": "H.1 Indexing and slicing of strings\n| Download notebook\nIn the last lesson, we wrote some functions to parse strings and compute things like reverse complements. This helped us practice using functions and the iteration skills we learned.\nYou might think, “Hey, replacing characters in strings sounds like it may be pretty common.” You would be right. You might also think, “I bet someone, possibly someone who is a really good programmer, already has written code to do this.” You would again be right.\nFor common tasks, there are often already methods written by someone smart, and working with strings is no different. In this lesson, we will explore some of the string processing tools that come with Python’s standard library.\nBefore getting into string methods, we pause to note that indexing and slicing of strings works just as it does for lists and tuples.\nmy_str = 'The Dude abides.'\n\nprint(my_str[5])\nprint(my_str[:6])\nprint(my_str[::2])\nprint(my_str[::-1])\n\nu\nThe Du\nTeDd bds\n.sediba eduD ehT",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html#revisiting-previous-examples-using-string-methods",
    "href": "appendices/python/string_methods.html#revisiting-previous-examples-using-string-methods",
    "title": "Appendix H — String methods",
    "section": "H.2 Revisiting previous examples using string methods",
    "text": "H.2 Revisiting previous examples using string methods\nWe’ll start by revisiting some of the examples we’ve seen so far.\n\nH.2.1 Computing GC content\nIf you remember from the iteration lesson, we started by computing the GC content of a nucleic acid sequence. We counted the occurrences of 'G' and 'C' in the string using a for loop. We can use the count() string method do to this.\n\n# Define sequence\nseq = 'GACAGACUCCAUGCACGUGGGUAUCAUGUC'\n\n# Count G's and C's\nseq.count('G') + seq.count('C')\n\n16\n\n\nThe seq.count() method enabled us to count the number times G and C occurred in the string seq. This notation is new. We have a variable, followed by a dot (.), and then a function. These functions are called methods in the language of object-oriented programming (OOP). If you have a string my_str, and you want to execute one of Python’s built-in string methods on it, the syntax is\nmy_str.string_method_of_choice(*args)\nIn general, the count method gives the number of times a substring appears in a string. We can learn more about its behavior by playing with it.\n\n# Substrings of more than one characater\nseq.count('GAC')\n\n2\n\n\n\n# Substrings cannot overlap\n'AAAAAAA'.count('AA')\n\n3\n\n\n\n# Something that's not there.\nseq.count('nonsense')\n\n0\n\n\n\n\nH.2.2 Finding the index of a start codon\nAnother task in the iteration lesson was to find the index of the start codon in an RNA sequence. Let’s do it with another string method.\n\nseq.find('AUG')\n\n10\n\n\nWow, that was easy. The find() method gives the index where the substring argument first appears. But, what if a substring is not in the string?\n\nseq.find('nonsense')\n\n-1\n\n\nIn this case, find() returns -1. This is not to be interpreted as index -1! find() always returns positive indices if it finds a substring. Note that you should not use find() to test if a substring is present. Use the in operator we already learned about.\n\n'AUG' in seq\n\nTrue\n\n\n\n\nH.2.3 Finding the last index of a substring\nLet’s say we wanted to find the last instance of the start codon. We basically want to search from the right. This is exactly what the rfind() method does.\n\nseq.rfind('AUG')\n\n25\n\n\n\n\nH.2.4 Finding the complementary base\nIn our lesson on functions, we wrote a function to compute a complementary base comparing against both the capital and lowercase letter. Here is that function implemented with some handy string methods.\n\ndef complement_base(base):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    # Convert to lowercase\n    base = base.lower()\n    \n    if base == 'a':\n        return 'T'\n    elif base == 't':\n        return 'A'\n    elif base == 'g':\n        return 'C'\n    else:\n        return 'G'\n\nWe were able to avoid all the “base in 'Tt'”-style operations by just converting the base to lowercase using the lower() method. In general, the lower() method takes a string and converts any capital letters to lower case. The upper() function works analogously.\n\n'LeBron James'.lower()\n\n'lebron james'\n\n\n\n'Make me aLl caPS.'.upper()\n\n'MAKE ME ALL CAPS.'\n\n\n\n\nH.2.5 Converting RNA to DNA\nWe also updated the complementary base function to account for RNA or DNA. Perhaps an easier way is just to replace all Us in an RNA sequence with Ts to get a DNA sequence. The replace() method makes this easy.\n\nseq.replace('U', 'T')\n\n'GACAGACTCCATGCACGTGGGTATCATGTC'\n\n\nNote that seq did not change. Remember, strings are immutable, so the replace() method returns a new string, as does lower(), upper(), and any other string method that returns a string. So, the characters stored in the variable seq are unchanged.\n\nseq\n\n'GACAGACUCCAUGCACGUGGGUAUCAUGUC'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html#the-join-method",
    "href": "appendices/python/string_methods.html#the-join-method",
    "title": "Appendix H — String methods",
    "section": "H.3 The join() method",
    "text": "H.3 The join() method\nOne of the most useful string methods is the join() method. Say we have a list of words that we want to craft into a sentence.\n\nword_tuple = ('The', 'Dude', 'abides.')\n\nNow, we would like to concatenate them into a single string. (This is sort of like the opposite of taking a string and making a list of its characters by doing a list() type conversion.) We need to know what we want to put between each word. In this case, we want a space. Here’s the nifty syntax to do that.\n\n' '.join(word_tuple)\n\n'The Dude abides.'\n\n\nWe now have a single string with the elements of the tuple, separated by spaces. The string before the dot (.) specifies what goes between the strings in the list or tuple (or other iterable). If we wanted “*” between each word, we could do that, too.\n\n' * '.join(word_tuple)\n\n'The * Dude * abides.'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html#the-format-method",
    "href": "appendices/python/string_methods.html#the-format-method",
    "title": "Appendix H — String methods",
    "section": "H.4 The format() method",
    "text": "H.4 The format() method\nThe format() method is very powerful. We not go over all use cases here, but I’ll show you what I think is most intuitive and commonly used. Again, this is best learned by example.\n\nmy_str = \"\"\"\nLet's do a Mad Lib!\nDuring this bootcamp, I feel {adjective}.\nThe instructors give us {plural_noun}.\n\"\"\".format(adjective='truculent', plural_noun='haircuts')\n\nprint(my_str)\n\n\nLet's do a Mad Lib!\nDuring this bootcamp, I feel truculent.\nThe instructors give us haircuts.\n\n\n\nSee the pattern? Given a string, the format() method takes kwargs that are themselves strings. Within the string, the name of the kwargs are given in braces. Then, the arguments in the format() method inserts the strings at the places delimited by braces.\nNow, what if we want to insert a number into a string? We could convert it to a string, but we should instead use string conversions. These are short directives that specify how the number should be represented in a string. A complete list is here. The table below shows some that are commonly used.\n\n\n\nconversion\ndescription\n\n\n\n\nd\ninteger\n\n\n04d\ninteger with four digits, possibly with leading zeros\n\n\nf\nfloat, default to six digits after decimal\n\n\n.8f\nfloat with 8 digits after the decimal\n\n\ne\nscientific notation, default to six digits after decimal\n\n\n.16e\nscientific notation with 16 digits after the decimal\n\n\ns\ndisplay as a string\n\n\n\nBelow are examples of all of these.\n\nprint('There are {n:d} states in the US.'.format(n=50))\nprint('Your file number is {n:d}.'.format(n=23))\nprint('π is approximately {pi:f}.'.format(pi=3.14))\nprint('e is approximately {e:.8f}.'.format(e=2.7182818284590451))\nprint(\"Avogadro's number is approximately {N_A:e}.\".format(N_A=6.022e23))\nprint('ε₀ is approximately {eps_0:.16e} F/m.'.format(eps_0=8.854187817e-12))\nprint('That {thing:s} really tied the room together.'.format(thing='rug'))\n\nThere are 50 states in the US.\nYour file number is 23.\nπ is approximately 3.140000.\ne is approximately 2.71828183.\nAvogadro's number is approximately 6.022000e+23.\nε₀ is approximately 8.8541878170000005e-12 F/m.\nThat rug really tied the room together.\n\n\nNote the syntax. In the braces, we specify the name of the kwarg, and then we put a colon followed by the string conversion. Note also that I used double quotes on the outside of the string containing Avogadro’s number so that I could include an apostrophe in the string. Finally, note that we got a subscript zero using the Unicode character, ₀.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html#f-strings",
    "href": "appendices/python/string_methods.html#f-strings",
    "title": "Appendix H — String methods",
    "section": "H.5 f-strings",
    "text": "H.5 f-strings\nf-strings are strings that are prefixed with an f or F that allow convenient insertion of entries into strings. Here are some examples.\n\nn_states = 50\nfile_number = 23\npi = 3.14\ne = 2.7182818284590451\nN_A = 6.022e23\neps_0=8.854187817e-12\nthing = 'rug'\n\nprint(f'There are {n_states} states in the US.')\nprint(f'Your file number is {file_number}.')\nprint(f'π is approximately {pi}.')\nprint(f'e is approximately {e:.8f}.')\nprint(f\"Avogadro's number is approximately {N_A}.\")\nprint(f'ε₀ is approximately {eps_0} F/m.')\nprint(f'That {thing} really tied the room together.')\n\nThere are 50 states in the US.\nYour file number is 23.\nπ is approximately 3.14.\ne is approximately 2.71828183.\nAvogadro's number is approximately 6.022e+23.\nε₀ is approximately 8.854187817e-12 F/m.\nThat rug really tied the room together.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html#there-are-many-more-string-methods",
    "href": "appendices/python/string_methods.html#there-are-many-more-string-methods",
    "title": "Appendix H — String methods",
    "section": "H.6 There are many more string methods",
    "text": "H.6 There are many more string methods\nYou can find a complete list of string methods from the Python doc pages. Various methods will come in handy when parsing strings going forward.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/string_methods.html#computing-environment",
    "href": "appendices/python/string_methods.html#computing-environment",
    "title": "Appendix H — String methods",
    "section": "H.7 Computing environment",
    "text": "H.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html",
    "href": "appendices/python/dictionaries.html",
    "title": "Appendix I — Dictionaries",
    "section": "",
    "text": "I.1 Mapping objects and dictionaries\n| Download notebook\nA mapping object allows an arbitrary collection of objects to be indexed by an arbitrary collection of values. That’s a mouthful. It is easier to understand instead by comparing to a sequence.\nLet’s take a sequence of two strings, say a tuple containing a first and last name.\nWe are restricted on how we reference the sequence. I.e., the first name is name[0], and the last name is name[1]. A mapping object could instead be indexed like name['first name'] and name['last name']. You can imagine this would be very useful! A classic example in biology might be looking up amino acids that are coded for by given codons. E.g., you might want\nto give you 'Leucine'.\nPython’s build-in mapping type is a dictionary. You might imagine that the Oxford English Dictionary might conveniently be stored as a dictionary (obviously). I.e., you would not want to store definitions that have to be referenced like\nRather, you would like to get definitions like this:\nImportantly, note that in Python 3.5 and older dictionaries have no sense of order. In Python 3.6, dictionaries were stored in insertion order as an implementation improvement. In Python 3.7 and beyond, dictionaries are guaranteed to be ordered in the order in which their entries were created. It is therefore advisable be cautious when relying on ordering in dictionaries. For safety’s sake, you may be better off assuming there is no sense of order.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#mapping-objects-and-dictionaries",
    "href": "appendices/python/dictionaries.html#mapping-objects-and-dictionaries",
    "title": "Appendix I — Dictionaries",
    "section": "",
    "text": "name = ('jeffrey', 'lebowski')\n\naa['CTT']\n\n\noed[103829]\n\noed['computer']",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#dictionary-syntax",
    "href": "appendices/python/dictionaries.html#dictionary-syntax",
    "title": "Appendix I — Dictionaries",
    "section": "I.2 Dictionary syntax",
    "text": "I.2 Dictionary syntax\nThe syntax for creating a dictionary, as usual, is best seen through example.\n\nmy_dict = {'a': 6, 'b': 7, 'c': 27.6}\nmy_dict\n\n{'a': 6, 'b': 7, 'c': 27.6}\n\n\nA dictionary is created using curly braces ({}). Each entry has a key, followed by a colon, and then the value associated with the key. In the example above, the keys are all strings, which is the most common use case. Note that the items can be of any type; in the above example, they are ints and a float.\nWe could also create the dictionary using the built-in dict() function, which can take a tuple of 2-tuples, each one containing a key-value pair.\n\ndict((('a', 6), ('b', 7), ('c', 27.6)))\n\n{'a': 6, 'b': 7, 'c': 27.6}\n\n\nFinally, we can make a dictionary with keyword arguments to the dict() function.\n\ndict(a='yes', b='no', c='maybe')\n\n{'a': 'yes', 'b': 'no', 'c': 'maybe'}\n\n\nWe do not need to have strings as the keys. In fact, any immutable object can be a key.\n\nmy_dict = {\n    0: 'zero',\n    1.7: [1, 2, 3],\n    (5, 6, 'dummy string'): 3.14,\n    'strings are immutable': 42\n}\n\nmy_dict\n\n{0: 'zero',\n 1.7: [1, 2, 3],\n (5, 6, 'dummy string'): 3.14,\n 'strings are immutable': 42}\n\n\nHowever, mutable objects cannot be used as keys.\n\nmy_dict = {\n    \"immutable is ok\": 1, \n    [\"mutable\", \"not\", \"ok\"]: 5\n}\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 my_dict = {\n      2     \"immutable is ok\": 1, \n      3     [\"mutable\", \"not\", \"ok\"]: 5\n      4 }\n\nTypeError: unhashable type: 'list'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#indexing-dictionaries",
    "href": "appendices/python/dictionaries.html#indexing-dictionaries",
    "title": "Appendix I — Dictionaries",
    "section": "I.3 Indexing dictionaries",
    "text": "I.3 Indexing dictionaries\nAs mentioned at the beginning of the lesson, we index dictionaries by key.\n\n# Make a dictionary\nmy_dict = dict(a='yes', b='no', c='maybe')\n\n# Pull out an entry\nmy_dict['b']\n\n'no'\n\n\nBecause the indexing of dictionaries is by key and not by sequential integers, they cannot be sliced; they must be accessed element-by-element. (Actually there are ways to slice keys of dictionaries using itertools, but we will not cover that.)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#useful-dictionaries-in-bioinformatics",
    "href": "appendices/python/dictionaries.html#useful-dictionaries-in-bioinformatics",
    "title": "Appendix I — Dictionaries",
    "section": "I.4 Useful dictionaries in bioinformatics",
    "text": "I.4 Useful dictionaries in bioinformatics\nIt might be useful to quickly look up 3-letter amino acid codes. Dictionaries are particularly useful for this.\n\naa_dict = {\n    \"A\": \"Ala\",\n    \"R\": \"Arg\",\n    \"N\": \"Asn\",\n    \"D\": \"Asp\",\n    \"C\": \"Cys\",\n    \"Q\": \"Gln\",\n    \"E\": \"Glu\",\n    \"G\": \"Gly\",\n    \"H\": \"His\",\n    \"I\": \"Ile\",\n    \"L\": \"Leu\",\n    \"K\": \"Lys\",\n    \"M\": \"Met\",\n    \"F\": \"Phe\",\n    \"P\": \"Pro\",\n    \"S\": \"Ser\",\n    \"T\": \"Thr\",\n    \"W\": \"Trp\",\n    \"Y\": \"Tyr\",\n    \"V\": \"Val\",\n}\n\nAnother useful dictionary would contain the set of codons and the amino acids they code for. This is built in the code below using the zip() function we learned before. To see the logic on how this is constructed, see the codon table here.\n\n# The set of DNA bases\nbases = ['T', 'C', 'A', 'G']\n\n# Build list of codons\ncodon_list = []\nfor first_base in bases:\n    for second_base in bases:\n        for third_base in bases:\n            codon_list += [first_base + second_base + third_base]\n\n# The amino acids that are coded for (* = STOP codon)\namino_acids = 'FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG'\n\n# Build dictionary from tuple of 2-tuples (technically an iterator, but it works)\ncodons = dict(zip(codon_list, amino_acids))\n\n# Show that we did it\nprint(codons)\n\n{'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L', 'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*', 'TGT': 'C', 'TGC': 'C', 'TGA': '*', 'TGG': 'W', 'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L', 'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P', 'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q', 'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M', 'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T', 'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K', 'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R', 'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V', 'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A', 'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E', 'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'}\n\n\nThese two dictionaries are particularly useful, so I put them in a little module which we will discuss in the lesson on packages and modules.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#a-dictionary-is-an-implementation-of-a-hash-table",
    "href": "appendices/python/dictionaries.html#a-dictionary-is-an-implementation-of-a-hash-table",
    "title": "Appendix I — Dictionaries",
    "section": "I.5 A dictionary is an implementation of a hash table",
    "text": "I.5 A dictionary is an implementation of a hash table\nIt is useful to stop and think about how a dictionary works. Let’s create a dictionary and look at where the values are stored in memory.\n\n# Create dictionary\nmy_dict = {'a': 6, 'b': 7, 'c':12.6}\n\n# Find where they are stored\nprint(id(my_dict))\nprint(id(my_dict['a']))\nprint(id(my_dict['b']))\nprint(id(my_dict['c']))\n\n4570286848\n4343739664\n4343739696\n4600140880\n\n\nSo, each entry in the dictionary is stored at a different location in memory. The dictionary itself also has its own address. So, when I index a dictionary with a key, how does the dictionary know which address in memory to use to fetch the value I am interested in?\nDictionaries use a hash function to do this. A hash function converts its input to an integer. For example, we can use Python’s built-in hash function to convert the keys to integers.\n\nhash('a'), hash('b'), hash('c')\n\n(199680081378453410, -6901413122848462881, -5789435826028719999)\n\n\nUnder the hood, Python then converts these integers to integers that could correspond to locations in memory. A collection of elements that can be indexed this way is called a hash table. This is a very common data structure in computing. Wikipedia has a pretty good discussion on them.\nGiven what you know about how dictionaries work, why do you think mutable objects are not acceptable as keys?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#dictionaries-are-mutable",
    "href": "appendices/python/dictionaries.html#dictionaries-are-mutable",
    "title": "Appendix I — Dictionaries",
    "section": "I.6 Dictionaries are mutable",
    "text": "I.6 Dictionaries are mutable\nDictionaries are mutable. This means that they can be changed in place. For example, if we want to add an element to a dictionary, we use simple syntax.\n\n# Remind ourselves what the dictionary is\nprint(my_dict)\n\n# Add an entry\nmy_dict['d'] = 'Bootcamp is so much fun!'\n\n# Look at dictionary again\nprint(my_dict)\n\n# Change an entry\nmy_dict['a'] = 'I was not satisfied with entry a.'\n\n# Look at it again\nprint(my_dict)\n\n{'a': 6, 'b': 7, 'c': 12.6}\n{'a': 6, 'b': 7, 'c': 12.6, 'd': 'Bootcamp is so much fun!'}\n{'a': 'I was not satisfied with entry a.', 'b': 7, 'c': 12.6, 'd': 'Bootcamp is so much fun!'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#membership-operators-with-dictionaries",
    "href": "appendices/python/dictionaries.html#membership-operators-with-dictionaries",
    "title": "Appendix I — Dictionaries",
    "section": "I.7 Membership operators with dictionaries",
    "text": "I.7 Membership operators with dictionaries\nThe in and not in operators work with dictionaries, but both only query keys and not values. We see this again by example.\n\n# Make a fresh my_dict\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\n\n# in works with keys\n'b' in my_dict, 'd' in my_dict, 'e' not in my_dict\n\n(True, False, True)\n\n\n\n# Try it with values\n2 in my_dict\n\nFalse\n\n\nYup! We get False. Why? Because 2 is not a key in my_dict. We can also iterate over the keys in a dictionary.\n\nfor key in my_dict:\n    print(key, ':', my_dict[key])\n\na : 1\nb : 2\nc : 3\n\n\nThe best, and preferred, method, is to iterate over key,value pairs in a dictionary using the items() method of a dictionary.\n\nfor key, value in my_dict.items():\n    print(key, ':', value)\n\na : 1\nb : 2\nc : 3\n\n\nNote, however, that like lists, the items that come out of the my_dict.items() iterator are not items in the dictionary, but copies of them. If you make changes within the for loop, you will not change entries in the dictionary.\n\nfor key, value in my_dict.items():\n    value = 'this string will not be in dictionary.'\n    \nmy_dict\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\nYou will, though, if you use the keys.\n\nfor key, _ in my_dict.items():\n    my_dict[key] = 'this will be in the dictionary.'\n    \nmy_dict\n\n{'a': 'this will be in the dictionary.',\n 'b': 'this will be in the dictionary.',\n 'c': 'this will be in the dictionary.'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#built-in-functions-for-dictionaries",
    "href": "appendices/python/dictionaries.html#built-in-functions-for-dictionaries",
    "title": "Appendix I — Dictionaries",
    "section": "I.8 Built-in functions for dictionaries",
    "text": "I.8 Built-in functions for dictionaries\nThe built-in len() function and del operation work on dictionaries.\n\nlen(d) gives the number of entries in dictionary d\ndel d[k] deletes entry with key k from dictionary d\n\nThis is the first time we’ve encountered the del keyword. This keyword is used to delete variables and their values from memory. The del keyword can also be to delete items from lists. Let’s see things in practice.\n\n# Create my_list and my_dict for reference\nmy_dict = dict(a=1, b=2, c=3, d=4)\nmy_list = [1, 2, 3, 4]\n\n# Print them\nprint('my_dict:', my_dict)\nprint('my_list:', my_list)\n\n# Get lengths\nprint('length of my_dict:', len(my_dict))\nprint('length of my_list:', len(my_list))\n\n# Delete a key from my_dict\ndel my_dict['b']\n\n# Delete entry from my_list\ndel my_list[1]\n\n# Show post-deleted objects\nprint('post-deleted my_dict:', my_dict)\nprint('post-deleted my_list:', my_list)\n\nmy_dict: {'a': 1, 'b': 2, 'c': 3, 'd': 4}\nmy_list: [1, 2, 3, 4]\nlength of my_dict: 4\nlength of my_list: 4\npost-deleted my_dict: {'a': 1, 'c': 3, 'd': 4}\npost-deleted my_list: [1, 3, 4]\n\n\nNote, though, that you cannot delete an item from a tuple, since it’s immutable.\n\nmy_tuple = (1, 2, 3, 4)\ndel my_tuple[1]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 my_tuple = (1, 2, 3, 4)\n----&gt; 2 del my_tuple[1]\n\nTypeError: 'tuple' object doesn't support item deletion",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#dictionary-methods",
    "href": "appendices/python/dictionaries.html#dictionary-methods",
    "title": "Appendix I — Dictionaries",
    "section": "I.9 Dictionary methods",
    "text": "I.9 Dictionary methods\nDictionaries have several built-in methods in addition to the items() you have already seen. Following are a few of them, assuming the dictionary is d.\n\n\n\n\n\n\n\nmethod\neffect\n\n\n\n\nd.keys()\nreturn keys\n\n\nd.pop(key)\nreturn value associated with key and delete key from d\n\n\nd.values()\nreturn the values in d\n\n\nd.get(key, None)\nFetch a value in d by key giving a default value (the second argument) if key is missing\n\n\n\nLet’s try these out.\n\nmy_dict = dict(a=1, b=2, c=3, d=4)\n\nmy_dict.keys()\n\ndict_keys(['a', 'b', 'c', 'd'])\n\n\nNote that this is a dict_keys object. We cannot index it. If, say, we wanted to sort the keys and have them index-able, we would have to convert them to a list.\n\nsorted(list(my_dict.keys()))\n\n['a', 'b', 'c', 'd']\n\n\nThis is not a usual use case, though, and be warned that doing then when this is not explicitly what you want can lead to bugs. Now let’s try popping an entry out of the dictionary.\n\nmy_dict.pop('c')\n\n3\n\n\n\nmy_dict\n\n{'a': 1, 'b': 2, 'd': 4}\n\n\n…and, as we expect, key 'c' is now deleted, and its value was returned in the call to my_dict.pop('c'). Now, let’s look at the values.\n\nmy_dict.values()\n\ndict_values([1, 2, 4])\n\n\nWe get a dict_values object, similar to the dict_keys object we got with the my_dict.keys() method. Finally, let’s consider get().\n\nmy_dict.get('d')\n\n4\n\n\nThis is the same as my_dict['d'], except that if the key 'd' is not there, it will return a default value. Let’s try using my_dict.get() with the deleted entry 'c'.\n\nmy_dict.get('c')\n\nNote that there was no error (there would be if we did my_dict['c']), and we got None. We could specify a default value.\n\nmy_dict.get('c', 3)\n\n3\n\n\nYou should think about what behavior you want when you attempt to get a value out of a dictionary by key. Do you want an error when the key is missing? Then use indexing. Do you want to have a (possibly None) default if the key is missing and no error? Then use my_dict.get().\nYou can get more information about build-in methods from the Python documentation.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#list-methods",
    "href": "appendices/python/dictionaries.html#list-methods",
    "title": "Appendix I — Dictionaries",
    "section": "I.10 List methods",
    "text": "I.10 List methods\nAs you may guess, the dictionary method pop() has an analog that works for lists. (Why don’t the dictionary key() and values() methods work for lists?) We take this opportunity to introduce a few more useful list methods. Imagine the list is called s.\n\n\n\nmethod\neffect\n\n\n\n\ns.pop(i)\nreturn value at index i and delete it from the list\n\n\ns.append(x)\nPut x at the end of the list\n\n\ns.insert(i, x)\nInsert x at index i in the list\n\n\ns.remove(x)\nRemove the first occurrence of x from the list\n\n\ns.reverse()\nReverse the order of items in the list",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#using-dictionaries-as-kwargs",
    "href": "appendices/python/dictionaries.html#using-dictionaries-as-kwargs",
    "title": "Appendix I — Dictionaries",
    "section": "I.11 Using dictionaries as kwargs",
    "text": "I.11 Using dictionaries as kwargs\nA nifty feature of dictionaries is that they can be passed into functions as keyword arguments. We covered named keyword arguments in the lesson on functions. In addition to the named keyword arguments, a function can take in arbitrary keyword arguments (not arbitrary non-keyword arguments). This is specified in the function definition by including a last argument with a double-asterisk, **. The kwargs with the double-asterisk get passed in as a dictionary.\n\ndef concatenate_sequences(a, b, **kwargs):\n    \"\"\"Concatenate (combine) 2 or more sequences.\"\"\"\n    seq = a + b\n\n    for key in kwargs:\n        seq += kwargs[key]\n        \n    return seq\n\nLet’s try it!\n\nconcatenate_sequences('TGACAC', 'CAGGGA', c='GGGGGGGGG', d='AAAATTTTT')\n\n'TGACACCAGGGAGGGGGGGGGAAAATTTTT'\n\n\nNow, imagine we have a dictionary that contains our values.\n\nmy_dict = {\"a\": \"TGACAC\", \"b\": \"CAGGGA\", \"c\": \"GGGGGGGGG\", \"d\": \"AAAATTTTT\"}\n\nWe can now pass this directly into the function by preceding it with a double asterisk.\n\nconcatenate_sequences(**my_dict)\n\n'TGACACCAGGGAGGGGGGGGGAAAATTTTT'\n\n\nBeautiful! This example is kind of trivial, but you can imagine that it can come in handy, e.g. with large sets of sequence fragments that you read in from a file. We will use **kwargs later in the bootcamp.\nQuestion: What is the risk in using a dictionary in this way to concatenate sequences?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#merging-dictionaries",
    "href": "appendices/python/dictionaries.html#merging-dictionaries",
    "title": "Appendix I — Dictionaries",
    "section": "I.12 Merging dictionaries",
    "text": "I.12 Merging dictionaries\nSaw I have two dictionaries that have no like keys and I want to merge them together. This might be like considering two volumes of an encyclopedia; they do not have and like keys, and we might like to consider them as a single volume. How can we accomplish this?\nThe dict() function, combined with the ** operator in function calls allows for this. We simple call dict() with ** before each dictionary argument.\n\nrestriction_dict = {\"KpnI\": \"GGTACC\", \"HindII\": \"AAGCTT\", \"ecoRI\": \"GAATTC\"}\n\ndict(**my_dict, **restriction_dict, another_seq=\"AGTGTAGTG\")\n\n{'a': 'TGACAC',\n 'b': 'CAGGGA',\n 'c': 'GGGGGGGGG',\n 'd': 'AAAATTTTT',\n 'KpnI': 'GGTACC',\n 'HindII': 'AAGCTT',\n 'ecoRI': 'GAATTC',\n 'another_seq': 'AGTGTAGTG'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/dictionaries.html#computing-environment",
    "href": "appendices/python/dictionaries.html#computing-environment",
    "title": "Appendix I — Dictionaries",
    "section": "I.13 Computing environment",
    "text": "I.13 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python/comprehensions.html",
    "href": "appendices/python/comprehensions.html",
    "title": "Appendix J — Comprehensions",
    "section": "",
    "text": "J.1 List comprehensions\n| Download notebook\nWe have learned how to build lists, tuples, arrays, etc. by constructing them directly. E.g., list(range(10)) gives us a list of all integers between 0 and 9 inclusive. But what if we want to build a list or array by iterating the contains something a bit more complicated. For example, let’s say we want to get a list of all prime numbers less than 1000. This could be a bit cumbersome, even with sympy’s lovely isprime() function.\nBecause we do not know a priori how many entries there are going to be, we have to keep appending to a list. Under the hood, this means that the Python interpreter has to keep allocating memory as it creates and grows lists. So, in addition to being syntactically clunky, the above way of creating a list is inefficient. It would be nice to have a more convenient way of doing this.\nEnter list comprehensions.\nAs is often the case, this is best seen by example. We will create the same Numpy array of primes using a list comprehension.\nprimes = [x for x in range(n_max) if sympy.isprime(x)]\n\n# Take a look\nprint(primes)\n\n[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]\nIn one line, we have made our list of primes! The list comprehension is enclosed in brackets. The first part, x, is an expression that will be inserted into the list. Next comes a for statement to produce the iterator. Finally, there is a conditional; if the conditional evaluates True, then the expression expression is included in the list.\nIf a condition is absent, all entries are put in the list. For example, if we didn’t want to just do list(range(100)) to get integers, we could use a list comprehension without a conditional.\n# Give same result as list(range(100))\nmy_list_of_ints = [i for i in range(100)]\n\nprint(my_list_of_ints)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python/comprehensions.html#list-comprehensions",
    "href": "appendices/python/comprehensions.html#list-comprehensions",
    "title": "Appendix J — Comprehensions",
    "section": "",
    "text": "J.1.1 Another example list comprehension\nLet’s say we wanted to build a list containing the information about the 2018 Nobel laureates. We have, in three separate arrays, their names, nationalities, and category for the prize.\n\nnames = (\n    \"Frances Arnold\",\n    \"George Smith\",\n    \"Gregory Winter\",\n    \"postponed\",\n    \"Denis Mukwege\",\n    \"Nadia Murad\",\n    \"Arthur Ashkin\",\n    \"Gérard Mourou\",\n    \"Donna Strickland\",\n    \"James Allison\",\n    \"Tasuku Honjo\",\n    \"William Nordhaus\",\n    \"Paul Romer\",\n)\n\nnationalities = (\n    \"USA\",\n    \"USA\",\n    \"UK\",\n    \"---\",\n    \"DRC\",\n    \"Iraq\",\n    \"USA\",\n    \"France\",\n    \"Canada\",\n    \"USA\",\n    \"Japan\",\n    \"USA\",\n    \"USA\",\n)\n\ncategories = (\n    \"Chemistry\",\n    \"Chemistry\",\n    \"Chemistry\",\n    \"Literature\",\n    \"Peace\",\n    \"Peace\",\n    \"Physics\",\n    \"Physics\",\n    \"Physics\",\n    \"Physiology or Medicine\",\n    \"Physiology or Medicine\",\n    \"Economics\",\n    \"Economics\",\n)\n\nWith these tuples in hand, we can use a list comprehension to build a nice list of tuples containing the information about the laureates.\n\n[(cat, name, nat) for name, nat, cat in zip(names, nationalities, categories)]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK'),\n ('Literature', 'postponed', '---'),\n ('Peace', 'Denis Mukwege', 'DRC'),\n ('Peace', 'Nadia Murad', 'Iraq'),\n ('Physics', 'Arthur Ashkin', 'USA'),\n ('Physics', 'Gérard Mourou', 'France'),\n ('Physics', 'Donna Strickland', 'Canada'),\n ('Physiology or Medicine', 'James Allison', 'USA'),\n ('Physiology or Medicine', 'Tasuku Honjo', 'Japan'),\n ('Economics', 'William Nordhaus', 'USA'),\n ('Economics', 'Paul Romer', 'USA')]\n\n\nNotice that I do not have to use range(); I can use any iterator, including one that puts out multiple values using zip().\nNow, let’s say we are really interested in the prize in chemistry. We can add an if statement to the comprehension like we did in the prime number example.\n\n[\n    (cat, name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Chemistry\"\n]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK')]\n\n\n(Note here that we split the list comprehension over many lines for readability, which is perfectly legal.) We can also nest iterators. For example, let’s say the the chemistry and medicine prize winners got together in Sweden and wanted to play against each other in basketball. There are three chemistry winners, but only two medicine winners. So, to play 2-on-2, we would have to choose only two chemistry laureates. So, let’s make a list of all possible pairs of chemistry winners.\n\n# First get list of chemistry laureates\nchem_names = [name for name, cat in zip(names, categories) if cat == \"Chemistry\"]\n\n# List of all possible pairs of chemistry laureates\n[\n    (n1, n2)\n    for i, n1 in enumerate(chem_names)\n    for j, n2 in enumerate(chem_names)\n    if i &lt; j\n]\n\n[('Frances Arnold', 'George Smith'),\n ('Frances Arnold', 'Gregory Winter'),\n ('George Smith', 'Gregory Winter')]\n\n\nTo summarize this structure of list comprehensions, borrowing from Dave Beazley’s explanation in Python Essential Reference, a list comprehension has the following structure.\n[expression_to_put_in_list for i_1 in iterable_1 if condition_1\n                           for i_2 in iterable_2 if condition_2\n                                     ...\n                           for i_n in iterable_n if condition_n]\nwhich is roughly equivalent to\nmy_list = []\nfor i_1 in iterable_1:\n    if condition_1:\n        for i_2 in iterable_2:\n            if condition_2:\n                ...\n                for i_n in iterable_n:\n                    if condition_n:\n                        my_list += [expression_to_put_in_list]\n\n\nJ.1.2 What if you want an else statement in a list comprehension?\nNow, let’s say that we deem “Physiology or Medicine” to be too long of a title for the category of the prize. We instead want to substitute that phrase with “Medicine” for brevity. We might construct the list like this:\n\n[\n    (\"Medicine\", name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Physiology or Medicine\"\n]\n\n[('Medicine', 'James Allison', 'USA'), ('Medicine', 'Tasuku Honjo', 'Japan')]\n\n\nThis leaves out all of the other prizes. So, we need an else statement. To include all prizes, we might try it like this.\n\n[\n    (\"Medicine\", name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Physiology or Medicine\" else (cat, name, nat)\n]\n\n\n  Cell In[10], line 4\n    if cat == \"Physiology or Medicine\" else (cat, name, nat)\n                                       ^\nSyntaxError: invalid syntax\n\n\n\n\nSyntax error! This structure of a list comprehension does not match the template shown above. In the conditional expression of list comprehensions, you cannot have an else block.\nHowever, the expression_to_put_in_list can be any valid Python expression. The following is a valid Python expression:\n(\"Medicine\", name, nat) if cat == \"Physiology or Medicine\" else (cat, name, nat)\nSo, we can still use a list comprehension to build the list.\n\n[\n    (\"Medicine\", name, nat) if cat == \"Physiology or Medicine\" else (cat, name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK'),\n ('Literature', 'postponed', '---'),\n ('Peace', 'Denis Mukwege', 'DRC'),\n ('Peace', 'Nadia Murad', 'Iraq'),\n ('Physics', 'Arthur Ashkin', 'USA'),\n ('Physics', 'Gérard Mourou', 'France'),\n ('Physics', 'Donna Strickland', 'Canada'),\n ('Medicine', 'James Allison', 'USA'),\n ('Medicine', 'Tasuku Honjo', 'Japan'),\n ('Economics', 'William Nordhaus', 'USA'),\n ('Economics', 'Paul Romer', 'USA')]\n\n\nTo be clear here, there is no conditional in the list comprehension; the conditional is in the expression to be added to the list, which we have called expression_to_put_in_list.\nList comprehensions will prove very useful, and most Pythonistas use them extensively.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python/comprehensions.html#dictionary-comprehensions",
    "href": "appendices/python/comprehensions.html#dictionary-comprehensions",
    "title": "Appendix J — Comprehensions",
    "section": "J.2 Dictionary comprehensions",
    "text": "J.2 Dictionary comprehensions\nIn addition to list comprehensions, Python also allows for dictionary comprehensions (and set comprehensions, but we will not discuss sets in the bootcamp). To demonstrate a dictionary comprehension, let’s use the name of the laureate as a key and the values in the dictionary are their nationality and category.\n\n{name: (cat, nat) for name, nat, cat in zip(names, nationalities, categories)}\n\n{'Frances Arnold': ('Chemistry', 'USA'),\n 'George Smith': ('Chemistry', 'USA'),\n 'Gregory Winter': ('Chemistry', 'UK'),\n 'postponed': ('Literature', '---'),\n 'Denis Mukwege': ('Peace', 'DRC'),\n 'Nadia Murad': ('Peace', 'Iraq'),\n 'Arthur Ashkin': ('Physics', 'USA'),\n 'Gérard Mourou': ('Physics', 'France'),\n 'Donna Strickland': ('Physics', 'Canada'),\n 'James Allison': ('Physiology or Medicine', 'USA'),\n 'Tasuku Honjo': ('Physiology or Medicine', 'Japan'),\n 'William Nordhaus': ('Economics', 'USA'),\n 'Paul Romer': ('Economics', 'USA')}\n\n\nAaaand we have our dictionary! This is quite a powerful way to construct this, and you may find dictionary comprehensions quite useful. I use them in specifying **kwargs and in creating dictionaries I want to convert to data frames.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python/comprehensions.html#paul-romer-and-jupyter-and-open-source-software",
    "href": "appendices/python/comprehensions.html#paul-romer-and-jupyter-and-open-source-software",
    "title": "Appendix J — Comprehensions",
    "section": "J.3 Paul Romer and Jupyter and open source software",
    "text": "J.3 Paul Romer and Jupyter and open source software\nCoincidentally, one of the laureates featured in this lesson, Paul Romer, is a big fan of Jupyter notebooks. I love this quote from this blog post of his:\n\nIn the larger contest between open and proprietary models, Mathematica versus Jupyter would be a draw if the only concern were their technical accomplishments. In the 1990s, Mathematica opened up an undeniable lead. Now, Jupyter is the unambiguous technical leader.\nThe tie-breaker is social, not technical. The more I learn about the open source community, the more I trust its members. The more I learn about proprietary software, the more I worry that objective truth might perish from the earth.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python/comprehensions.html#computing-environment",
    "href": "appendices/python/comprehensions.html#computing-environment",
    "title": "Appendix J — Comprehensions",
    "section": "J.4 Computing environment",
    "text": "J.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\nnumpy     : 1.26.4\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html",
    "href": "appendices/python/packages_and_modules.html",
    "title": "Appendix K — Packages and modules",
    "section": "",
    "text": "K.1 Example: I want to compute the mean and median of a list of numbers\n| Download notebook\nThe Python Standard Library has lots of built-in modules that contain useful functions and data types for doing specific tasks. You can also use modules from outside the standard library. And you will undoubtedly write your own modules!\nA module is contained in a file that ends with .py. This file can have classes, functions, and other objects. We will not discuss defining your own classes until much later in the bootcamp, so your modules will essentially just contain functions for now.\nA package contains several related modules that are all grouped together under one name. We will extensively use the NumPy, SciPy, Pandas, and Bokeh packages, among others, in the bootcamp, and I’m sure you will also use them beyond. As such, the first module we will consider is NumPy. We will talk a lot more about NumPy later in the bootcamp.\nSay I have a list of numbers and I want to compute the mean. This happens all the time; you repeat a measurement multiple times and you want to compute the mean. We could write a function to do this.\ndef mean(values):\n    \"\"\"Compute the mean of a sequence of numbers.\"\"\"\n    return sum(values) / len(values)\nAnd it works as expected.\nprint(mean([1, 2, 3, 4, 5]))\nprint(mean((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n3.275\nIn addition to the mean, we might also want to compute the median, the standard deviation, etc. These seem like really common tasks. Remember my advice: if you want to do something that seems really common, a good programmer (or a team of them) probably already wrote something to do that. Means, medians, standard deviations, and lots and lots and lots of other numerical things are included in the Numpy module. To get access to it, we have to import it.\nimport numpy\nThat’s it! We now have the numpy module available for use. Remember, in Python everything is an object, so if we want to access the methods and attributes available in the numpy module, we use dot syntax. In a Jupyter notebook or in the JupyterLab console, you can type\n(note the dot) and hit tab, and we will see what is available. For Numpy, there is a huge number of options!\nSo, let’s try to use Numpy’s numpy.mean() function to compute a mean.\nprint(numpy.mean([1, 2, 3, 4, 5]))\nprint(numpy.mean((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n3.275\nGreat! We get the same values! Now, we can use the numpy.median() function to compute the median.\nprint(numpy.median([1, 2, 3, 4, 5]))\nprint(numpy.median((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n2.85\nThis is nice. It gives the median, including when we have an even number of elements in the sequence of numbers, in which case it automatically interpolates. It is really important to know that it does this interpolation, since if you are not expecting it, it can give unexpected results. So, here is an important piece of advice:\nWe can access the doc string of the numpy.median() function in JupyterLab by typing\nand looking at the output. An important part of that output:\nThis is where the documentation tells you that the median will be reported as the average of two middle values when the number of elements is even. Note that you could also read the documentation here, which is a bit easier to read.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#example-i-want-to-compute-the-mean-and-median-of-a-list-of-numbers",
    "href": "appendices/python/packages_and_modules.html#example-i-want-to-compute-the-mean-and-median-of-a-list-of-numbers",
    "title": "Appendix K — Packages and modules",
    "section": "",
    "text": "numpy.\n\n\n\n\n\n\n\nAlways check the doc strings of functions.\n\n\nnumpy.median?\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#the-as-keyword",
    "href": "appendices/python/packages_and_modules.html#the-as-keyword",
    "title": "Appendix K — Packages and modules",
    "section": "K.2 The as keyword",
    "text": "K.2 The as keyword\nWe use Numpy all the time. Typing numpy over and over again can get annoying. So, it is common practice to use the as keyword to import a module with an alias. Numpy’s alias is traditionally np, and this is the only alias you should ever use for Numpy.\n\nimport numpy as np\n\nnp.median((4.5, 1.2, -1.6, 9.0))\n\n2.85\n\n\nI prefer to do things this way, though some purists differ. We will use traditional aliases for major packages like Numpy and Pandas throughout the bootcamp.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#third-party-packages",
    "href": "appendices/python/packages_and_modules.html#third-party-packages",
    "title": "Appendix K — Packages and modules",
    "section": "K.3 Third party packages",
    "text": "K.3 Third party packages\nStandard Python installations come with the standard library. Numpy and other useful packages are not in the standard library. Outside of the standard library, there are several packages available. Several. Ha! There are currently (June 12, 2023) about 470,000 packages available through the Python Package Index, PyPI. Usually, you can ask Google about what you are trying to do, and there is often a third party module to help you do it. The most useful (for scientific computing) and thoroughly tested packages and modules are available using conda. Others can be installed using pip, which we will do later in the bootcamp.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#writing-your-own-module",
    "href": "appendices/python/packages_and_modules.html#writing-your-own-module",
    "title": "Appendix K — Packages and modules",
    "section": "K.4 Writing your own module",
    "text": "K.4 Writing your own module\nTo write your own module, you need to create a .py file and save it. You can do this using the text editor in JupyterLab. Let’s call our module na_utils, for “nucleic acid utilities.” So, we create a file called na_utils.py. We’ll build this module to have two functions, based on things we’ve already written. We’ll have a function dna_to_rna(), which converts a DNA sequence to an RNA sequence (just changes T to U), and another function reverse_rna_complement(), which returns the reverse RNA complement of a DNA template. The contents of na_utils.py should look as follows.\n\"\"\"\nUtilities for parsing nucleic acid sequences.\n\"\"\"\n\ndef dna_to_rna(seq):\n    \"\"\"\n    Convert a DNA sequence to RNA.\n    \"\"\"\n    # Determine if original sequence was uppercase\n    seq_upper = seq.isupper()\n\n    # Convert to lowercase\n    seq = seq.lower()\n\n    # Swap out 't' for 'u'\n    seq = seq.replace('t', 'u')\n\n    # Return upper or lower case RNA sequence\n    if seq_upper:\n        return seq.upper()\n    else:\n        return seq\n\n\ndef reverse_rna_complement(seq):\n    \"\"\"\n    Convert a DNA sequence into its reverse complement as RNA.\n    \"\"\"\n    # Determine if original was uppercase\n    seq_upper = seq.isupper()\n\n    # Reverse sequence\n    seq = seq[::-1]\n\n    # Convert to upper\n    seq = seq.upper()\n\n    # Compute complement\n    seq = seq.replace('A', 'u')\n    seq = seq.replace('T', 'a')\n    seq = seq.replace('G', 'c')\n    seq = seq.replace('C', 'g')\n\n    # Return result\n    if seq_upper:\n        return seq.upper()\n    else:\n        return seq\nNote that the file starts with a doc string saying what the module contains.\nI then have my two functions, each with doc strings. We will now import the module and then use these functions. In order for the import to work, the file na_utils.py must be in your present working directory, since this is where the Python interpreter will look for your module. In general, if you execute the code\nimport my_module\nthe Python interpreter will look first in the pwd to find my_module.py.\n\nimport na_utils\n\n# Sequence\nseq = 'GACGATCTAGGCGACCGACTGGCATCG'\n\n# Convert to RNA\nna_utils.dna_to_rna(seq)\n\n'GACGAUCUAGGCGACCGACUGGCAUCG'\n\n\nWe can also compute the reverse RNA complement.\n\nna_utils.reverse_rna_complement(seq)\n\n'CGAUGCCAGUCGGUCGCCUAGAUCGUC'\n\n\nWonderful! You now have your own functioning module!\n\nK.4.1 A quick note on error checking\nThese functions have minimal error checking of the input. For example, the dna_to_rna() function will take gibberish in and give jibberish out.\n\nna_utils.dna_to_rna('You can observe a lot by just watching.')\n\n'you can observe a lou by jusu wauching.'\n\n\nIn general, checking input and handling errors is an essential part of writing functions, and we will cover that in a later lesson.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#importing-modules-in-your-.py-files-and-notebooks",
    "href": "appendices/python/packages_and_modules.html#importing-modules-in-your-.py-files-and-notebooks",
    "title": "Appendix K — Packages and modules",
    "section": "K.5 Importing modules in your .py files and notebooks",
    "text": "K.5 Importing modules in your .py files and notebooks\nAs our first foray into the glory of PEP 8, the Python style guide, we quote:\n\nImports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.\nImports should be grouped in the following order:\n\nstandard library imports\nrelated third party imports\nlocal application/library specific imports\n\nYou should put a blank line between each group of imports.\n\nYou should follow this guide. I generally do it for Jupyter notebooks as well, with my first code cell having all of the imports I need. Therefore, going forward all of our lessons will have all necessary imports at the top of the document. The only exception is when we are explicitly demonstrating a concept that requires an import.\n\nK.5.1 Imports and updates\nOnce you have imported a module or package, the interpreter stores its contents in memory. You cannot update the contents of the package and expect the interpreter to know about the changes. You will need to restart the kernel and then import the package again in a fresh instance.\nThis can seem annoying, but it is good design. It ensures that code you are running does not change as you go through executing a notebook. However, when developing modules, it is sometimes convenient to have an imported module be updated as you run through the notebook as you are editing. To enable this, you can use the autoreload extension. To activate it, run the following in a code cell.\n%load_ext autoreload\n%autoreload 2\nWhenever you run a cell, imported packages and modules will be automatically reloaded.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#shareable-reusable-packages",
    "href": "appendices/python/packages_and_modules.html#shareable-reusable-packages",
    "title": "Appendix K — Packages and modules",
    "section": "K.6 Shareable, reusable packages",
    "text": "K.6 Shareable, reusable packages\nWhen we wrote the na_utils module, we stored it in the directory that we were working in, or the pwd. But what if you write a module that you want to use regardless of what directory your are in? To allow this kind of usage, you can use the setuptools module of the standard library to manage your packages. You should read the documentation on Python packages and modules to understand the details of how this is done, but what we present here is sufficient to get simple packages running and installed.\n\nK.6.1 Package architecture\nIn order for the tools in setuptools to effectively install your modules for widespread use, you need to follow a specific architecture for your package. I made an example jb_bootcamp package that is present in the ~/git/bootcamp/modules folder.\nThe file structure is of the package is\n/jb_bootcamp\n  /jb_bootcamp\n    __init__.py\n    na_utils.py\n    bioinfo_dicts.py\n    ...\nsetup.py\nREADME.md\nThe ellipsis above signifies that there are other files in there that we are not going to use yet. I am trying to keep it simple for now to show how package management works.\nTo set up the package, you can use your command line skills to make the directories and use the JupyterLab text editor to make the files, in this case, four of them, __init__.py, na_utils.py, bioinfo_dicts.py (the little module we mentioned in the lesson on dictionaries), setup.py, and README.md.\nIt is essential that the name of the root directory be the name of the package, and that there be a subdirectory with the same name. That subdirectory must contain a file __init__.py. This file contains information about the package and how the modules of the package are imported, but it may be empty for simple modules. In this case, I included a string with the name and version of the package, as well as instructions to import appropriate modules. Here are the contents of __init__.py. The first two lines of code tell the interpreter what to import when running import jb_bootcamp.\n\"\"\"Top-level package for utilities for bootcamp.\"\"\"\n\nfrom .na_utils import *\nfrom .bioinfo_dicts import *\n\n__author__ = 'Justin Bois'\n__email__ = 'bois@caltech.edu'\n__version__ = '0.0.1'\nAlso within the subdirectory are the .py files containing the code of the package. In our case, we have, na_utils.py and bioinfo_dicts.py.\nIt is also good practice to have a README file (which I suggest you write in Markdown) that has information about the package and what it does. Since this little demo package is kind of trivial, the README is quite short. Here are the contents I made for README.md (shown in unrendered raw Markdown).\n# jb_bootcamp\n\nUtilities for use in the Introduction to Programming in the Biological Sciences Bootcamp.\nFinally, in the main directory, we need to have a file called setup.py, which contains the instructions for setuptools to install the package. We use the setuptools.setup() function to do the installation.\nimport setuptools\n\nwith open(\"README.md\", \"r\") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name='jb_bootcamp',\n    version='0.0.1',\n    author='Justin Bois',\n    author_email='bois@caltech.edu',\n    description='Utilities for use in bootcamp.',\n    long_description=long_description,\n    long_description_content_type='ext/markdown',\n    packages=setuptools.find_packages(),\n    classifiers=(\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n    ),\n)\nThis is a minimal setup.py function, but will be sufficient for most packages you write for your own use. For your use, you make obvious changes to the name, author, etc., fields.\n\n\nK.6.2 Installing your package\nYou will install your package after a future lesson, when we put it under version control.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#package-management",
    "href": "appendices/python/packages_and_modules.html#package-management",
    "title": "Appendix K — Packages and modules",
    "section": "K.7 Package management",
    "text": "K.7 Package management\nYour workflows may require many packages. For example, building this bootcamp requires 336 packages! These packages depend on each other in various ways. For example, Pandas, a package we will use extensively, requires NumPy, python-dateutil, and pytz, plus loads of optional dependencies. To make matters complicated, different versions of various packages depend on specific versions of their dependencies, which can form a tangled web of requirements. How can we handle this mess?\nPackage management systems solve this problem. The package management system we are using is Conda. When you want to install a new package, you can do so with a command like\nconda install the-package-i-want\nand Conda will make sure that all of the version numbers line up, updating or downgrading already installed packages to accommodate the new one. Conda also plays nicely with pip, which can also be used to install packages.\nThe smaller the set of packages you need to manage, the better. Therefore, Conda allows you to set up environments. Each environment contains a set of packages with versions in them. In the lesson in which you set up your computer, you set up an environment for this bootcamp that we have been using. Using environments for your projects, as opposed to a single monolithic base environment that has tons and tons of packages, is advantageous for several reasons.\n\nBy keeping the number of packages limited to those you need, you can avoid version clashes.\nProjects may require specific versions of packages, which can be explicitly installed. In other environments, you can use different versions.\nYou can encode your environment in a YAML file that you can share (as we did to set up the bootcamp environment). This allows your collaborators to readily set up environments mirroring yours and more easily share packages.\n\nPoint 3 is very useful for pedagogical applications. It is very convenient when all students and TAs have the same packages installed!\nI will not go through the details of how to use Conda (or Mamba, a related package manager) here, but rather refer you to Conda’s extensive documentation.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/packages_and_modules.html#computing-environment",
    "href": "appendices/python/packages_and_modules.html#computing-environment",
    "title": "Appendix K — Packages and modules",
    "section": "K.8 Computing environment",
    "text": "K.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.3\nIPython version      : 8.12.0\n\nnumpy     : 1.24.3\njupyterlab: 3.6.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python/exceptions_and_error_handling.html",
    "href": "appendices/python/exceptions_and_error_handling.html",
    "title": "Appendix L — Errors and exception handling",
    "section": "",
    "text": "L.1 Kinds of errors\n| Download notebook\nAnd now we’ll proceed to discussing errors and exception handling.\nSo far, we have encountered errors when we did something wrong. For example, when we tried to change a character in a string, we got a TypeError.\nIn this case, the TypeError indicates that we tried to do something that is legal in Python for some types, but we tried to do it to a type for which it is illegal (strings are immutable). In Python, an error detected during execution is called an exception. We say that the interpreter “raised an exception.” There are many kinds of built-in exceptions, and you can find a list of them, with descriptions here. You can write your own kinds of exceptions, but we will not cover that in bootcamp.\nIn this lesson, we will investigate how to handle errors in your code. Importantly, we will also touch on the different kinds of errors and how to avoid them. Or, more specifically, you will learn how to use exceptions to help you write better, more bug-free code.\nIn computer programs, we can break down errors into three types.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python/exceptions_and_error_handling.html#kinds-of-errors",
    "href": "appendices/python/exceptions_and_error_handling.html#kinds-of-errors",
    "title": "Appendix L — Errors and exception handling",
    "section": "",
    "text": "L.1.1 Syntax errors\nA syntax error means you wrote something nonsensical, something the Python interpreter cannot understand. An example of a syntax error in English would be the following.\n\nSir Tristram, violer d’amores, fr’over the short sea, had passen-core rearrived from North Armorica on this side the scraggy isthmus of Europe Minor to wielderfight his penisolate war: nor had topsawyer’s rocks by the stream Oconee exaggerated themselse to Laurens County’s gorgios while they went doublin their mumper all the time: nor avoice from afire bellowsed mishe mishe to tauftauf thuartpeatrick: not yet, though venissoon after, had a kidscad buttended a bland old isaac: not yet, though all’s fair in vanessy, were sosie sesthers wroth with twone nathandjoe.\n\nThis is recognizable as English. In fact, it is the second sentence of a very famous novel (Finnegans Wake by James Joyce). Clearly, many spelling and punctuation rules of English are violated here. To many of us, it is nonsensical, but I do know of some people who have read the book and understand it. So, English is fairly tolerant of syntax errors. A simpler example would be\n\nBoootcamp is fun!\n\nThis has a syntax error (“Boootcamp” is not in the English language), but we understand what it means. A syntax error in Python would be this:\nmy_list = [1, 2, 3\nWe know what this means. We are trying to create a list with three items, 1, 2, and 3. However, we forgot the closing bracket. Unlike users of the English language, the Python interpreter is not forgiving; it will raise a SyntaxError exception.\n\nmy_list = [1, 2, 3\n\n\n  Cell In[3], line 1\n    my_list = [1, 2, 3\n                      ^\nSyntaxError: incomplete input\n\n\n\n\nSyntax errors are often the easiest to deal with, since the program will not run at all if any are present.\n\n\nL.1.2 Runtime errors\nRuntime errors occur when a program is syntactically correct, so it can run, but the interpreter encountered something wrong. The example at the start of the tutorial, trying to change a character in a string, is an example of a runtime error. This particular one was a TypeError, which is a more specific type of runtime error. Python does have a RuntimeError, which just indicates a generic runtime (non-syntax) error.\nRuntime errors are more difficult to spot than syntax errors because it is possible that a program could run all the way through without encountering the error for some inputs, but for other inputs, you get an error. Let’s consider the example of a simple function meant to add two numbers.\n\ndef add_two_things(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\nSyntactically, this function is just fine. We can use it and it works.\n\nadd_two_things(6, 7)\n\n13\n\n\nWe can even add strings, even though it was meant to add two numbers.\n\nadd_two_things('Hello, ', 'world.')\n\n'Hello, world.'\n\n\nHowever, when we try to add a string and a number, we get a TypeError, the kind of runtime error we saw before.\n\nadd_two_things('a string', 5.7)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 add_two_things('a string', 5.7)\n\nCell In[4], line 3, in add_two_things(a, b)\n      1 def add_two_things(a, b):\n      2     \"\"\"Add two numbers.\"\"\"\n----&gt; 3     return a + b\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\n\n\nL.1.3 Semantic errors\nSemantic errors are perhaps the most nefarious. They occur when your program is syntactically correct, executes without runtime errors, and then produces the wrong result. These errors are the hardest to find and can do the most damage. After all, when your program does not do what you designed it to do, you want it to scream out with an exception!\nFollowing is a common example of a semantic error in which we change a mutable object within a function and then try to reuse it.\n\n# A function to append a list onto itself, with the intention of \n# returning a new list, but leaving the input unaltered\ndef double_list(in_list):\n    \"\"\"Append a list to itself.\"\"\"\n    in_list += in_list\n    return in_list\n\n# Make a list\nmy_list = [3, 2, 1]\n\n# Double it\nmy_list_double = double_list(my_list)\n\n# Later on in our program, we want a sorted my_list\nmy_list.sort()\n\n# Let's look at my_list:\nprint('We expect [1, 2, 3]')\nprint('We get   ', my_list)\n\nWe expect [1, 2, 3]\nWe get    [1, 1, 2, 2, 3, 3]\n\n\nYikes! We changed my_list within the function unintentionally. Question: How would you re-rewrite double_list() to avoid this issue?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python/exceptions_and_error_handling.html#handling-errors-in-your-code",
    "href": "appendices/python/exceptions_and_error_handling.html#handling-errors-in-your-code",
    "title": "Appendix L — Errors and exception handling",
    "section": "L.2 Handling errors in your code",
    "text": "L.2 Handling errors in your code\nIf you have a syntax error, your code will not even run. So, we will assume we are without syntax errors in this discussion on how to handle errors. So, how can we handle runtime errors? In most use cases, we just write our code and let the Python interpreter tell us about these exceptions. However, sometimes we want to use the fact that we know we might encounter a runtime error within our code. A common example of this is when importing modules that are convenient, but not essential, for your code to run. Errors are handled in your code using a try statement.\nLet’s try importing a module that computes GC content. This doesn’t exist, so we will get an ImportError.\n\nimport gc_content\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 import gc_content\n\nModuleNotFoundError: No module named 'gc_content'\n\n\n\nNow, if we had the gc_content module, we would like to use it. But if not, we will just hand-code a calculation of the GC content of a sequence. We use a try statement.\n\n# Try to get the gc_content module\ntry:\n    import gc_content\n    have_gc = True\nexcept ImportError as e:\n    have_gc = False\nfinally:\n    # Do whatever is necessary here, like close files\n    pass\n\nseq = 'ACGATCTACGATCAGCTGCGCGCATCG'\n    \nif have_gc:\n    print(gc_content(seq))\nelse:\n    print(seq.count('G') + seq.count('C'))\n\n16\n\n\nThe program now runs just fine! The try statement consists of an initial try clause. Everything under the try clause is attempted to be executed. If it succeeds, the rest of the try statement is skipped, and the interpreter goes to the seq = ... line.\nIf, however, there is an ImportError, the code within the except ImportError as e clause is executed. The exception does not halt the program. If there is some other kind of error other than an ImportError, the interpreter will raise an exception after it does whatever code is in the finally clause. The finally clause is useful to tidy things up, like closing open file handles. While it is possible for a try statement to handle any generic exception by not specifying ImportError as e, it is good practice to explicitly specify the exception(s) that you anticipate in try statements as shown here. In this case, we only want to have control over ImportErrors. We want the interpreter to scream at us for any other, unanticipated errors.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python/exceptions_and_error_handling.html#issuing-warnings",
    "href": "appendices/python/exceptions_and_error_handling.html#issuing-warnings",
    "title": "Appendix L — Errors and exception handling",
    "section": "L.3 Issuing warnings",
    "text": "L.3 Issuing warnings\nWe may want to issue a warning instead of silently continuing. For this, the warnings module from the standard library is useful. We use the warnings.warn() method to issue the warning.\n\n# Try to get the gc_content module\ntry:\n    import gc_content\n\n    have_gc = True\nexcept ImportError as e:\n    have_gc = False\n    warnings.warn(\n        \"Failed to load gc_content. Using custom function.\", UserWarning\n    )\nfinally:\n    pass\n\nseq = \"ACGATCTACGATCAGCTGCGCGCATCG\"\n\nif have_gc:\n    print(gc_content(seq))\nelse:\n    print(seq.count(\"G\") + seq.count(\"C\"))\n\n16\n\n\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_16676/3620265156.py:8: UserWarning: Failed to load gc_content. Using custom function.\n  warnings.warn(\n\n\nNormally, we would use an ImportWarning, but those are ignored by default, so we have used a UserWarning.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python/exceptions_and_error_handling.html#checking-input",
    "href": "appendices/python/exceptions_and_error_handling.html#checking-input",
    "title": "Appendix L — Errors and exception handling",
    "section": "L.4 Checking input",
    "text": "L.4 Checking input\nIt is often the case that you want to check the input of a function to ensure that it works properly. In other words, you want to anticipate errors that the user (or you) might make in running your function, and you want to give descriptive error messages. For example, let’s say you are writing a code that processes protein sequences that contain only the 20 naturally occurring amino acids represented by their one-letter abbreviation. You may wish to check that the amino acid sequence is legitimate. In particular, the letters B, J, O, U, X, and Z, are not valid abbreviations for standard amino acids. (We will not use the ambiguity code, e.g. B for aspartic acid or asparagine, Z for glutamine or glutamic acid, or X for any amino acid.)\nTo illustrate the point, we will write a simple function that converts the sequence of one-letter amino acids to the three-letter abbreviation. We’ll use the dictionary that converts single-letter amino acid codes to triple letter that we encountered in the lesson on dictionaries that is now included in the bootcamp_utils package.\n\ndef one_to_three(seq):\n    \"\"\"\n    Converts a protein sequence using one-letter abbreviations\n    to one using three-letter abbreviations.\n    \"\"\"\n    # Convert seq to upper case\n    seq = seq.upper()\n\n    aa_list = []\n    for amino_acid in seq:\n        # Check if the `amino_acid` is in our dictionary `bootcamp_utils.aa`\n        if amino_acid not in bootcamp_utils.aa.keys():\n            raise RuntimeError(f\"{amino_acid} is not a valid amino acid\")\n        # Add the `amino_acid` to our aa_list\n        aa_list.append(bootcamp_utils.aa[amino_acid])\n\n    # Return the amino acids, joined together, with a dash as a separator.\n    return \"-\".join(aa_list)\n\nSo, if we put in a legitimate amino acid sequence, the function works as expected.\n\none_to_three('waeifnsdfklnsae')\n\n'Trp-Ala-Glu-Ile-Phe-Asn-Ser-Asp-Phe-Lys-Leu-Asn-Ser-Ala-Glu'\n\n\nBut, it we put in an improper amino acid, we will get a descriptive error.\n\none_to_three('waeifnsdfzklnsae')\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 one_to_three('waeifnsdfzklnsae')\n\nCell In[12], line 13, in one_to_three(seq)\n     10 for amino_acid in seq:\n     11     # Check if the `amino_acid` is in our dictionary `bootcamp_utils.aa`\n     12     if amino_acid not in bootcamp_utils.aa.keys():\n---&gt; 13         raise RuntimeError(f\"{amino_acid} is not a valid amino acid\")\n     14     # Add the `amino_acid` to our aa_list\n     15     aa_list.append(bootcamp_utils.aa[amino_acid])\n\nRuntimeError: Z is not a valid amino acid\n\n\n\nGood code checks for errors and gives useful error messages. We will use exception handling extensively when we go over test driven development in future lessons.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python/exceptions_and_error_handling.html#computing-environment",
    "href": "appendices/python/exceptions_and_error_handling.html#computing-environment",
    "title": "Appendix L — Errors and exception handling",
    "section": "L.5 Computing environment",
    "text": "L.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p bootcamp_utils,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\nbootcamp_utils: 0.0.7\njupyterlab    : 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python/file_io.html",
    "href": "appendices/python/file_io.html",
    "title": "Appendix M — File I/O",
    "section": "",
    "text": "M.1 File objects\n| Download notebook\nReading data in from files and then writing your results out again is one of the most common practices in scientific computing. In this tutorial, we will learn about some of Python’s File I/O capabilities. We will use a PDB file as an example. The PDB file contains the crystal structure for the tetramerization domain of p53.It is stored in the file ~/git/bootcamp/data/1OLG.pdb. (Make sure you launch your notebook from the ~/git/bootcamp/ directory.) Note that 1OLG is its unique Protein Databank identifier.\nTo open a file, we use the built-in open() function. When opening files, we should do this using context management. I will demonstrate how to open a file and then describe the syntax.\nwith open('data/1OLG.pdb', 'r') as f:\n    print(type(f))\n\n&lt;class '_io.TextIOWrapper'&gt;\nPython has a wonderful keyword, with. This keyword enables context management. Upon entry into a with block, variables have certain meaning. In this case, the variable f has the meaning of an open file, an instance of the _io.TextIOWrapper class. Upon exit, certain operations take place. For file objects created by opening them, the file is automatically closed upon exit, even if there is an error. This is important. If your program raises an exception before you have a chance to close the file, it won’t get closed and you could be in trouble. If you use context management, the file will still get closed. So here is an important tip:\nLet’s focus for a moment on the variable f in the above code cell. It is a Python file object, which has methods and attributes, just like any other object. We’ll explore those in a moment, but first, let’s look at how we opened the file. The first argument to open() is a string that has the name of the file, with the full path if necessary. The second argument is a string that says what we will be doing with the file. I.e., are we reading or writing to the file? The possible strings for this second argument are\nWe will mostly be working with text files in the bootcamp, so the first three are the most useful. A big warning, though….",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python/file_io.html#file-objects",
    "href": "appendices/python/file_io.html#file-objects",
    "title": "Appendix M — File I/O",
    "section": "",
    "text": "Use context management using with when working with files.\n\n\n\n\n\n\n\n\n\nstring\nmeaning\n\n\n\n\n'r'\nopen a text file for reading\n\n\n'w'\ncreate and open a text file for writing\n\n\n'a'\nappend an existing text file\n\n\n'r+'\nopen a text file for reading and writing\n\n\nappend 'b' to any of the above\nsame as above, except for binary files\n\n\n\n\n\nTrying to open an existing file with ‘w’ will wipe it out and create a new file.\n\n\nM.1.1 Reading data out of the file with file object methods\nWe will focus on the methods f.read() and f.readlines(). What do they do?\n\n\n\n\n\n\n\nmethod\ntask\n\n\n\n\nf.read()\nRead the entire contents of the file into a string\n\n\nf.readlines()\nRead the entire file into a list with each item being a string representing a line\n\n\n\nFirst, we’ll try using the first method to get a single string with the entire contents of the file.\n\n# Read file into string\nwith open('data/1OLG.pdb', 'r') as f:\n    f_str = f.read()\n\n# Let's look at the first 1000 characters\nf_str[:1000]\n\n'HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG              \\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION             \\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR                               \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);             \\nCOMPND   3 CHAIN: A, B, C, D;                                                   \\nCOMPND   4 ENGINEERED: YES                                                      \\nSOURCE    MOL_ID: 1;                                                            \\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;                                   \\nSOURCE   3 ORGANISM_COMMON: HUMAN;                                              \\nSOURCE   4 ORGANISM_TAXID: 9606                                                 \\nKEYWDS    ANTI-ONCOGENE                                                         \\nEXPDTA    SOLUTION NMR      '\n\n\nWe see lots of \\n, which signifies a new line. The backslash is known as an escape character, meaning that the n after it does not signify the letter n, but that \\n together means a new line.\nNow, let’s try reading it in as a list.\n\n# Read contents of the file in as a list\nwith open('data/1OLG.pdb', 'r') as f:\n    f_list = f.readlines()\n\n# Look at the list (first ten entries)\nf_list[:10]\n\n['HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG              \\n',\n 'TITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION             \\n',\n 'TITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR                               \\n',\n 'COMPND    MOL_ID: 1;                                                            \\n',\n 'COMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);             \\n',\n 'COMPND   3 CHAIN: A, B, C, D;                                                   \\n',\n 'COMPND   4 ENGINEERED: YES                                                      \\n',\n 'SOURCE    MOL_ID: 1;                                                            \\n',\n 'SOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;                                   \\n',\n 'SOURCE   3 ORGANISM_COMMON: HUMAN;                                              \\n']\n\n\nWe see that each entry is a line, including the newline character. To look at lines in files, the rstrip() method for strings can come it handy. It strips all whitespace, including newlines, from the end of a string.\n\nf_list[0].rstrip()\n\n'HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG'\n\n\n\n\nM.1.2 Reading line-by-line\nWhat if we do not want to read the entire file into a list? For example, if a file is several gigabytes, we do not want to spend all of our RAM storing a list. Instead, we can read it line-by-line. Conveniently, the file object can be used as an iterator.\n\n# Print the first ten lines of the file\nwith open('data/1OLG.pdb', 'r') as f:\n    for i, line in enumerate(f):\n        print(line.rstrip())\n        if i &gt;= 10:\n            break\n\nHEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR\nCOMPND    MOL_ID: 1;\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);\nCOMPND   3 CHAIN: A, B, C, D;\nCOMPND   4 ENGINEERED: YES\nSOURCE    MOL_ID: 1;\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;\nSOURCE   3 ORGANISM_COMMON: HUMAN;\nSOURCE   4 ORGANISM_TAXID: 9606\n\n\nAlternatively, we can use the method f.readline() to read a single line in the file and return it as a string.\n\n# Print the first ten lines of the file\nwith open('data/1OLG.pdb', 'r') as f:\n    i = 0\n    while i &lt; 10:\n        print(f.readline().rstrip())\n        i += 1\n\nHEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR\nCOMPND    MOL_ID: 1;\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);\nCOMPND   3 CHAIN: A, B, C, D;\nCOMPND   4 ENGINEERED: YES\nSOURCE    MOL_ID: 1;\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;\nSOURCE   3 ORGANISM_COMMON: HUMAN;\n\n\nEach subsequent call to f.readline() reads in the next line of the file. (As we read through a file, we keep moving forward in the bytes of the file and we have to use f.seek() to rewind.)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python/file_io.html#writing-to-a-file",
    "href": "appendices/python/file_io.html#writing-to-a-file",
    "title": "Appendix M — File I/O",
    "section": "M.2 Writing to a file",
    "text": "M.2 Writing to a file\nWriting to a file has similar syntax. We already saw how to open a file for writing. Again, context management is useful. However, before trying to open a file, we should check to make sure a file of the same name does not exist before opening it. The os.path module is useful. The function os.path.isfile() function checks to see if a file exists.\n\nos.path.isfile('data/1OLG.pdb')\n\nTrue\n\n\nNow that we know how to check existence of a file so we do not overwrite it, we can open and write a file.\n\nif os.path.isfile('yogi.txt'):\n    raise RuntimeError('File yogi.txt already exists.')\n\nwith open('yogi.txt', 'w') as f:\n    f.write('When you come to a fork in the road, take it.')\n    f.write('You can observe a lot by just watching.')\n    f.write('I never said most of the things I said.')\n\nNote that we can use the f.write() method to write strings to a file. Let’s look at the file contents.\n\n!cat yogi.txt\n\nWhen you come to a fork in the road, take it.You can observe a lot by just watching.I never said most of the things I said.\n\n\nAh! There are no newlines! When writing to a file, unlike when you use the print() function, you must include the newline characters. Let’s try again, intentionally obliterating our first attempt.\n\nwith open('yogi.txt', 'w') as f:\n    f.write('When you come to a fork in the road, take it.\\n')\n    f.write('You can observe a lot by just watching.\\n')\n    f.write('I never said most of the things I said.\\n')\n    \n!cat yogi.txt\n\nWhen you come to a fork in the road, take it.\nYou can observe a lot by just watching.\nI never said most of the things I said.\n\n\nThat’s better. Note also that f.write() only takes strings as arguments. You cannot pass numbers. They must be converted to strings first.\n\n# This will result in an exception\nwith open('gimme_phi.txt', 'w') as f:\n    f.write('The golden ratio is φ = ')\n    f.write(1.61803398875)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 4\n      2 with open('gimme_phi.txt', 'w') as f:\n      3     f.write('The golden ratio is φ = ')\n----&gt; 4     f.write(1.61803398875)\n\nTypeError: write() argument must be str, not float\n\n\n\nYup. It must be a string. Let’s try again.\n\nwith open('gimme_phi.txt', 'w') as f:\n    f.write('The golden ratio is φ = ')\n    f.write('{phi:.8f}'.format(phi=1.61803398875))\n\n!cat gimme_phi.txt\n\nThe golden ratio is φ = 1.61803399\n\n\nThat works!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python/file_io.html#an-exercise-extract-atomic-coordinates-for-first-chain-in-tetramer",
    "href": "appendices/python/file_io.html#an-exercise-extract-atomic-coordinates-for-first-chain-in-tetramer",
    "title": "Appendix M — File I/O",
    "section": "M.3 An exercise: extract atomic coordinates for first chain in tetramer",
    "text": "M.3 An exercise: extract atomic coordinates for first chain in tetramer\nAs an example on how to do file I/O, we will take the PDB file and extract only the ATOM records for the first chain of the tetramer and write only those entries to a new file.\nIt is useful to know that according to the PDB format specification, column 21 in the ATOM entry gives the ID of the chain.\nWe also conveniently use the fact that we can have multiple files open in our with block, separating them with commas.\n\nwith open('data/1OLG.pdb', 'r') as f, open('atoms_chain_A.txt', 'w') as f_out:\n    # Put the ATOM lines from chain A in new file\n    for line in f:\n        if len(line) &gt; 21 and line[:4] == 'ATOM' and line[21] == 'A':\n            f_out.write(line)\n\nLet’s see how we did!\n\n!head -10 atoms_chain_A.txt\n\nATOM      1  N   LYS A 319      18.634  25.437  10.685  1.00  4.81           N  \nATOM      2  CA  LYS A 319      17.984  25.295   9.354  1.00  4.32           C  \nATOM      3  C   LYS A 319      18.160  23.876   8.818  1.00  3.74           C  \nATOM      4  O   LYS A 319      19.259  23.441   8.537  1.00  3.67           O  \nATOM      5  CB  LYS A 319      18.609  26.282   8.371  1.00  4.67           C  \nATOM      6  CG  LYS A 319      18.003  26.056   6.986  1.00  5.15           C  \nATOM      7  CD  LYS A 319      16.476  26.057   7.091  1.00  5.90           C  \nATOM      8  CE  LYS A 319      16.014  27.341   7.784  1.00  6.51           C  \nATOM      9  NZ  LYS A 319      16.388  28.518   6.952  1.00  7.33           N  \nATOM     10  H1  LYS A 319      18.414  24.606  11.281  1.00  5.09           H  \n\n\n\n!tail -10 atoms_chain_A.txt\n\nATOM    689  HD2 PRO A 359       0.183  25.663  13.542  1.00  4.71           H  \nATOM    690  HD3 PRO A 359       0.246  23.956  13.062  1.00  4.53           H  \nATOM    691  N   GLY A 360      -3.984  26.791  10.832  1.00  5.45           N  \nATOM    692  CA  GLY A 360      -4.489  28.138  10.445  1.00  5.95           C  \nATOM    693  C   GLY A 360      -5.981  28.236  10.765  1.00  6.77           C  \nATOM    694  O   GLY A 360      -6.401  27.621  11.732  1.00  7.24           O  \nATOM    695  OXT GLY A 360      -6.679  28.924  10.039  1.00  7.15           O  \nATOM    696  H   GLY A 360      -4.589  26.020  10.828  1.00  5.72           H  \nATOM    697  HA2 GLY A 360      -3.950  28.896  10.995  1.00  5.99           H  \nATOM    698  HA3 GLY A 360      -4.341  28.288   9.386  1.00  6.05           H  \n\n\nNice!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python/file_io.html#finding-files-and-with-glob",
    "href": "appendices/python/file_io.html#finding-files-and-with-glob",
    "title": "Appendix M — File I/O",
    "section": "M.4 Finding files and with glob",
    "text": "M.4 Finding files and with glob\nIn the above snippet of code, we extracted all atom records from a PDB file. We might want to do this (or some other operation) for many files. For example, the directory ~/git/data/ has four PDB files in it. For the present discussion, let’s say we want to pull the sequence of chain A out of each PDB file.\nThe glob module from the standard library enables us to get a list of all files that match a pattern. In our case, we want all files matching data/*.pdb, where * is a wild card character, meaning that any matches of characters where * appears are allowed. Let’s see what glob.glob() gives us.\n\nfile_list = glob.glob('data/*.pdb')\n\nfile_list\n\n['data/1OLG.pdb', 'data/1J6Z.pdb', 'data/1FAG.pdb', 'data/2ERK.pdb']\n\n\nWe have the four PDB files. We can now loop over them and pull out the sequences.\n\n# Dictionary to hold sequences\nseqs = {}\n\n# Loop through all matching files\nfor file_name in file_list:\n    # Extract PDB ID\n    pdb_id = file_name[file_name.find('/')+1:file_name.rfind('.')]\n    \n    # Initialize sequence string, which we build as we go along\n    seq = ''\n    with open(file_name, 'r') as f:\n        for line in f:\n            if len(line) &gt; 11 and line[:6] == 'SEQRES' and line[11] == 'A':\n                seq += line[19:].rstrip() + ' '\n\n    # Build sequence with dash-joined three letter codes\n    seq = '-'.join(seq.split())\n\n    # Store in the dictionary\n    seqs[pdb_id] = seq\n\nLet’s take a look at what we got. We’ll look at actin.\n\nseqs['1J6Z']\n\n'ASP-GLU-ASP-GLU-THR-THR-ALA-LEU-VAL-CYS-ASP-ASN-GLY-SER-GLY-LEU-VAL-LYS-ALA-GLY-PHE-ALA-GLY-ASP-ASP-ALA-PRO-ARG-ALA-VAL-PHE-PRO-SER-ILE-VAL-GLY-ARG-PRO-ARG-HIS-GLN-GLY-VAL-MET-VAL-GLY-MET-GLY-GLN-LYS-ASP-SER-TYR-VAL-GLY-ASP-GLU-ALA-GLN-SER-LYS-ARG-GLY-ILE-LEU-THR-LEU-LYS-TYR-PRO-ILE-GLU-HIC-GLY-ILE-ILE-THR-ASN-TRP-ASP-ASP-MET-GLU-LYS-ILE-TRP-HIS-HIS-THR-PHE-TYR-ASN-GLU-LEU-ARG-VAL-ALA-PRO-GLU-GLU-HIS-PRO-THR-LEU-LEU-THR-GLU-ALA-PRO-LEU-ASN-PRO-LYS-ALA-ASN-ARG-GLU-LYS-MET-THR-GLN-ILE-MET-PHE-GLU-THR-PHE-ASN-VAL-PRO-ALA-MET-TYR-VAL-ALA-ILE-GLN-ALA-VAL-LEU-SER-LEU-TYR-ALA-SER-GLY-ARG-THR-THR-GLY-ILE-VAL-LEU-ASP-SER-GLY-ASP-GLY-VAL-THR-HIS-ASN-VAL-PRO-ILE-TYR-GLU-GLY-TYR-ALA-LEU-PRO-HIS-ALA-ILE-MET-ARG-LEU-ASP-LEU-ALA-GLY-ARG-ASP-LEU-THR-ASP-TYR-LEU-MET-LYS-ILE-LEU-THR-GLU-ARG-GLY-TYR-SER-PHE-VAL-THR-THR-ALA-GLU-ARG-GLU-ILE-VAL-ARG-ASP-ILE-LYS-GLU-LYS-LEU-CYS-TYR-VAL-ALA-LEU-ASP-PHE-GLU-ASN-GLU-MET-ALA-THR-ALA-ALA-SER-SER-SER-SER-LEU-GLU-LYS-SER-TYR-GLU-LEU-PRO-ASP-GLY-GLN-VAL-ILE-THR-ILE-GLY-ASN-GLU-ARG-PHE-ARG-CYS-PRO-GLU-THR-LEU-PHE-GLN-PRO-SER-PHE-ILE-GLY-MET-GLU-SER-ALA-GLY-ILE-HIS-GLU-THR-THR-TYR-ASN-SER-ILE-MET-LYS-CYS-ASP-ILE-ASP-ILE-ARG-LYS-ASP-LEU-TYR-ALA-ASN-ASN-VAL-MET-SER-GLY-GLY-THR-THR-MET-TYR-PRO-GLY-ILE-ALA-ASP-ARG-MET-GLN-LYS-GLU-ILE-THR-ALA-LEU-ALA-PRO-SER-THR-MET-LYS-ILE-LYS-ILE-ILE-ALA-PRO-PRO-GLU-ARG-LYS-TYR-SER-VAL-TRP-ILE-GLY-GLY-SER-ILE-LEU-ALA-SER-LEU-SER-THR-PHE-GLN-GLN-MET-TRP-ILE-THR-LYS-GLN-GLU-TYR-ASP-GLU-ALA-GLY-PRO-SER-ILE-VAL-HIS-ARG-LYS-CYS-PHE'\n\n\nExcellent! Our function worked, and we now have the protein sequence.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python/file_io.html#computing-environment",
    "href": "appendices/python/file_io.html#computing-environment",
    "title": "Appendix M — File I/O",
    "section": "M.5 Computing environment",
    "text": "M.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html",
    "href": "appendices/python/intro_to_numpy_and_scipy.html",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "",
    "text": "N.1 A very brief introduction to NumPy arrays\n| Download notebook\nHere you will learn about NumPy, arguably the most important package for scientific computing, and SciPy, a package containing lots of goodies for scientific computing, like special functions and numerical integrators.\nThe central object for NumPy and SciPy is the ndarray, commonly referred to as a “NumPy array.” This is an array object that is convenient for scientific computing. We will go over it in depth in the next lesson, but for now, let’s just create some NumPy arrays and see how operators work on them.\nJust like with type conversions with lists, tuples, and other data types we’ve looked at, we can convert a list to a NumPy array using\nNote that above we imported the NumPy package with the np alias. This is for convenience; it allow us to use np as a prefix instead of numpy. NumPy is in very widespread use, and the convention is to use the np abbreviation.\n# Create a NumPy array from a list\nmy_ar = np.array([1, 2, 3, 4])\n\n# Look at it\nmy_ar\n\narray([1, 2, 3, 4])\nWe see that the list has been converted, and it is explicitly shown as an array. It has several attributes and lots of methods. The most important attributes are probably the data type of its elements and the shape of the array.\n# The data type of stored entries\nmy_ar.dtype\n\ndtype('int64')\n# The shape of the array\nmy_ar.shape\n\n(4,)\nThere are also lots of methods. The one we use most often is astype(), which converts the data type of the array.\nmy_ar.astype(float)\n\narray([1., 2., 3., 4.])\nThere are many others. For example, we can compute summary statistics about the entries in the array, very similar to what we have see with Pandas.\nprint(my_ar.max())\nprint(my_ar.min())\nprint(my_ar.sum())\nprint(my_ar.mean())\nprint(my_ar.std())\n\n4\n1\n10\n2.5\n1.118033988749895\nImportantly, NumPy arrays can be arguments to NumPy functions. In this case, these functions do the same operations as the methods we just looked at.\nprint(np.max(my_ar))\nprint(np.min(my_ar))\nprint(np.sum(my_ar))\nprint(np.mean(my_ar))\nprint(np.std(my_ar))\n\n4\n1\n10\n2.5\n1.118033988749895",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#a-very-brief-introduction-to-numpy-arrays",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#a-very-brief-introduction-to-numpy-arrays",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "",
    "text": "np.array()",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#other-ways-to-make-numpy-arrays",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#other-ways-to-make-numpy-arrays",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.2 Other ways to make NumPy arrays",
    "text": "N.2 Other ways to make NumPy arrays\nThere are many other ways to make NumPy arrays besides just converting lists or tuples. Below are some examples.\n\n# How long our arrays will be\nn = 10\n\n# Make a NumPy array of length n filled with zeros\nnp.zeros(n)\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n# Make a NumPy array of length n filled with ones\nnp.ones(n)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n# Make an empty NumPy array of length n without initializing entries\n# (while it initially holds whatever values were previously in the memory\n# locations assigned, ones will be displayed)\nnp.empty(n)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n# Make a NumPy array filled with zeros the same shape as another NumPy array\nmy_ar = np.array([[1, 2], [3, 4]])\nnp.zeros_like(my_ar)\n\narray([[0, 0],\n       [0, 0]])\n\n\nAs we work through the rest of this exercise, we will use more interesting arrays (not just zeroes, ones, and counting).\n\nx = np.array(\n    [1683, 2061, 1792, 1852, 2091, 1781, 1912, 1802, 1751, 1731, 1892,\n     1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833, 1683,\n     1671, 1680, 1692, 1800, 1821, 1882, 1642, 1749, 1712, 1661, 1701,\n     2141, 1863, 1752, 1740, 1721, 1660, 1930, 2030, 1851, 2131, 1828])\n\ny = np.array(\n    [1840, 2090, 2169, 1988, 2212, 2339, 1989, 2144, 2290, 1920, 2280,\n     1809, 2158, 1800, 2133, 2060, 2160, 2001, 2030, 2088, 1951, 2460])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#slicing-numpy-arrays",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#slicing-numpy-arrays",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.3 Slicing NumPy arrays",
    "text": "N.3 Slicing NumPy arrays\nWe can slice NumPy arrays like lists and tuples. Here are a few examples.\n\n# Reversed array\nx[::-1]\n\narray([1828, 2131, 1851, 2030, 1930, 1660, 1721, 1740, 1752, 1863, 2141,\n       1701, 1661, 1712, 1749, 1642, 1882, 1821, 1800, 1692, 1680, 1671,\n       1683, 1833, 1800, 1930, 1910, 1821, 1840, 1787, 1683, 1809, 1951,\n       1892, 1731, 1751, 1802, 1912, 1781, 2091, 1852, 1792, 2061, 1683])\n\n\n\n# Every 5th element, starting at index 3\nx[3::5]\n\narray([1852, 1751, 1683, 1930, 1680, 1642, 2141, 1660, 1828])\n\n\n\n# Entries 10 to 20\nx[10:21]\n\narray([1892, 1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833])\n\n\n\nN.3.1 Fancy indexing\nNumPy arrays also allow fancy indexing, where we can slice out specific values. For example, say we wanted indices 1, 19, and 6 (in that order) from x. We just index with a list of the indices we want.\n\nx[[1, 19, 6]]\n\narray([2061, 1800, 1912])\n\n\nInstead of a list, we could also use a NumPy array.\n\nx[np.array([1, 19, 6])]\n\narray([2061, 1800, 1912])\n\n\nAs a very nice feature, we can use Boolean indexing with Numpy arrays. Say we only want entries greater than 2000.\n\n# Just slice out the big ones\nx[x &gt; 2000]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nIf we want to know the indices where the values are high, we can use the np.where() function.\n\nnp.where(x &gt; 2000)\n\n(array([ 1,  4, 33, 40, 42]),)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#numpy-arrays-are-mutable",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#numpy-arrays-are-mutable",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.4 NumPy arrays are mutable",
    "text": "N.4 NumPy arrays are mutable\nYes, NumPy arrays are mutable. Let’s look at some consequences.\n\n# Make an array\nmy_ar = np.array([1, 2, 3, 4])\n\n# Change an element\nmy_ar[2] = 6\n\n# See the result\nmy_ar\n\narray([1, 2, 6, 4])\n\n\nNow, let’s try attaching another variable to the NumPy array.\n\n# Attach a new variable\nmy_ar2 = my_ar\n\n# Set an entry using the new variable\nmy_ar2[3] = 9\n\n# Does the original change? (yes.)\nmy_ar\n\narray([1, 2, 6, 9])\n\n\nLet’s see how messing with NumPy in functions affects things.\n\n# Re-instantiate my_ar\nmy_ar = np.array([1, 2, 3, 4]).astype(float)\n\n# Function to normalize x (note that /= works with mutable objects)\ndef normalize(x):\n    x /= np.sum(x)\n\n# Pass it through a function\nnormalize(my_ar)\n\n# Is it normalized even though we didn't return anything? (Yes.)\nmy_ar\n\narray([0.1, 0.2, 0.3, 0.4])\n\n\nSo, be careful when writing functions. What you do to your NumPy array inside the function will happen outside of the function as well. Always remember that:\n\nNumPy arrays are mutable.\n\n\nN.4.1 Slices of NumPy arrays are views, not copies\nA very important distinction between NumPy arrays and lists is that slices of NumPy arrays are views into the original NumPy array, NOT copies. To illustrate this, we will again use out 1, 2, 3, 4 array for simplicity and clarity.\n\n# Make list and array\nmy_list = [1, 2, 3, 4]\nmy_ar = np.array(my_list)\n\n# Slice out of each\nmy_list_slice = my_list[1:-1]\nmy_ar_slice = my_ar[1:-1]\n\n# Mess with the slices\nmy_list_slice[0] = 9\nmy_ar_slice[0] = 9\n\n# Look at originals\nprint(my_list)\nprint(my_ar)\n\n[1, 2, 3, 4]\n[1 9 3 4]\n\n\nMessing with an element of a slice of a NumPy array messes with that element in the original! This is not the case with lists. Let’s issue a warning.\n\nSlices of NumPy arrays are views, not copies.\n\nFortunately, you can make a copy of an array using the np.copy() function.\n\n# Make a copy\nx_copy = np.copy(x)\n\n# Mess with an entry\nx_copy[10] = 2000\n\n# Check equality\nnp.allclose(x, x_copy)\n\nFalse\n\n\nSo, messing with an entry in the copy did not affect the original.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#mathematical-operations-with-arrays",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#mathematical-operations-with-arrays",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.5 Mathematical operations with arrays",
    "text": "N.5 Mathematical operations with arrays\nMathematical operations on arrays are done elementwise to all elements.\n\n# Divide one array be another\nnp.array([5, 6, 7, 8]) / np.array([1, 2, 3, 4])\n\narray([5.        , 3.        , 2.33333333, 2.        ])\n\n\n\n# Multiply by scalar\n-4 * x\n\narray([-6732, -8244, -7168, -7408, -8364, -7124, -7648, -7208, -7004,\n       -6924, -7568, -7804, -7236, -6732, -7148, -7360, -7284, -7640,\n       -7720, -7200, -7332, -6732, -6684, -6720, -6768, -7200, -7284,\n       -7528, -6568, -6996, -6848, -6644, -6804, -8564, -7452, -7008,\n       -6960, -6884, -6640, -7720, -8120, -7404, -8524, -7312])\n\n\n\n# Raise to power\nx**2\n\narray([2832489, 4247721, 3211264, 3429904, 4372281, 3171961, 3655744,\n       3247204, 3066001, 2996361, 3579664, 3806401, 3272481, 2832489,\n       3193369, 3385600, 3316041, 3648100, 3724900, 3240000, 3359889,\n       2832489, 2792241, 2822400, 2862864, 3240000, 3316041, 3541924,\n       2696164, 3059001, 2930944, 2758921, 2893401, 4583881, 3470769,\n       3069504, 3027600, 2961841, 2755600, 3724900, 4120900, 3426201,\n       4541161, 3341584])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#indexing-2d-numpy-arrays",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#indexing-2d-numpy-arrays",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.6 Indexing 2D NumPy arrays",
    "text": "N.6 Indexing 2D NumPy arrays\nNumPy arrays need not be one-dimensional. We’ll create a two-dimensional NumPy array by reshaping our x array from having shape (44,) to having shape (11, 4). That is, it will become an array with 11 rows and 4 columns. (The 2D nature of this array has no meaning in this case; it’s just meant for demonstration.)\n\n# New 2D array using the reshape() method\nmy_ar = x.reshape((11, 4))\n\n# Look at it\nmy_ar\n\narray([[1683, 2061, 1792, 1852],\n       [2091, 1781, 1912, 1802],\n       [1751, 1731, 1892, 1951],\n       [1809, 1683, 1787, 1840],\n       [1821, 1910, 1930, 1800],\n       [1833, 1683, 1671, 1680],\n       [1692, 1800, 1821, 1882],\n       [1642, 1749, 1712, 1661],\n       [1701, 2141, 1863, 1752],\n       [1740, 1721, 1660, 1930],\n       [2030, 1851, 2131, 1828]])\n\n\nNotice that it is represented as an array made out of a list of lists. If we had a list of lists, we would index it like this:\nlist_of_lists[i][j]\n\n# Make list of lists\nlist_of_lists = [[1, 2], [3, 4]]\n\n# Pull out value in first row, second column\nlist_of_lists[0][1]\n\n2\n\n\nThough this will work with NumPy arrays, this is not how NumPy arrays are indexed. They are indexed much more conveniently.\n\nmy_ar[0, 1]\n\n2061\n\n\nWe essentially have a tuple in the indexing brackets. Now, say we wanted the second row (indexing starting at 0).\n\nmy_ar[2, :]\n\narray([1751, 1731, 1892, 1951])\n\n\nWe can use Boolean indexing as before.\n\nmy_ar[my_ar &gt; 2000]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nNote that this gives a one-dimensional array of the entries greater than 2000. If we wanted indices where this is the case, we can again use np.where().\n\nnp.where(my_ar &gt; 2000)\n\n(array([ 0,  1,  8, 10, 10]), array([1, 0, 1, 0, 2]))\n\n\nThis tuple of NumPy arrays is how we would index using fancy indexing to pull those values out using fancy indexing.\n\nmy_ar[(np.array([ 0,  1,  8, 10, 10]), np.array([1, 0, 1, 0, 2]))]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nNumPy arrays can be of arbitrary integer dimension, and these principles extrapolate to 3D, 4D, etc., arrays.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#concatenating-arrays",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#concatenating-arrays",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.7 Concatenating arrays",
    "text": "N.7 Concatenating arrays\nLet’s say we want to study all cross sectional areas and don’t care if the mother was well-fed or not. We would want to concatenate our arrays. The np.concatenate() function accomplishes this. We simply have to pass it a tuple containing the NumPy arrays we want to concatenate.\n\ncombined = np.concatenate((x, y))\n\n# Look at it\ncombined\n\narray([1683, 2061, 1792, 1852, 2091, 1781, 1912, 1802, 1751, 1731, 1892,\n       1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833, 1683,\n       1671, 1680, 1692, 1800, 1821, 1882, 1642, 1749, 1712, 1661, 1701,\n       2141, 1863, 1752, 1740, 1721, 1660, 1930, 2030, 1851, 2131, 1828,\n       1840, 2090, 2169, 1988, 2212, 2339, 1989, 2144, 2290, 1920, 2280,\n       1809, 2158, 1800, 2133, 2060, 2160, 2001, 2030, 2088, 1951, 2460])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#numpy-has-useful-mathematical-functions",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#numpy-has-useful-mathematical-functions",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.8 NumPy has useful mathematical functions",
    "text": "N.8 NumPy has useful mathematical functions\nSo far, we have not done much mathematics with Python. We have done some adding and division, but nothing like computing a logarithm or cosine. The NumPy functions also work elementwise on the arrays when it is intuitive to do so. That is, they apply the function to each entry in the array. Check it out.\n\n# Exponential\nnp.exp(x / 1000)\n\narray([5.38167681, 7.8538197 , 6.00144336, 6.37255189, 8.09300412,\n       5.93578924, 6.76660849, 6.06175887, 5.76036016, 5.64629738,\n       6.63262067, 7.03571978, 6.10434004, 5.38167681, 5.97151103,\n       6.29653826, 6.1780334 , 6.7530888 , 6.88951024, 6.04964746,\n       6.2526164 , 5.38167681, 5.31748262, 5.36555597, 5.43033051,\n       6.04964746, 6.1780334 , 6.56662499, 5.16549017, 5.74885095,\n       5.54003047, 5.26457279, 5.47942408, 8.50794132, 6.44303692,\n       5.7661234 , 5.69734342, 5.59011579, 5.25931084, 6.88951024,\n       7.61408636, 6.36618252, 8.42328589, 6.22143134])\n\n\n\n# Cosine\nnp.cos(x)\n\narray([ 0.62656192,  0.9933696 ,  0.27501843,  0.03112568,  0.26681725,\n       -0.96021239, -0.33430744,  0.29228295, -0.42404251, -0.99984597,\n        0.72399324, -0.99748325,  0.84865001,  0.62656192, -0.84393482,\n        0.56257847,  0.43231386,  0.99610114,  0.48702972, -0.99122275,\n       -0.11903049,  0.62656192,  0.94691648, -0.73027654, -0.24968607,\n       -0.99122275,  0.43231386, -0.98275172, -0.49500319, -0.64703425,\n       -0.98592179, -0.61963892, -0.17156886,  0.00460656, -0.99936794,\n        0.53296056,  0.90375673,  0.82939405,  0.3256673 ,  0.48702972,\n        0.86222727, -0.824246  ,  0.5401501 ,  0.91834245])\n\n\n\n# Square root\nnp.sqrt(x)\n\narray([41.02438299, 45.39823785, 42.33202098, 43.03486958, 45.72745346,\n       42.20189569, 43.72642222, 42.44997055, 41.84495191, 41.60528813,\n       43.49712634, 44.17012565, 42.53234064, 41.02438299, 42.27292278,\n       42.89522118, 42.67317659, 43.70354677, 43.93176527, 42.42640687,\n       42.81354926, 41.02438299, 40.87786687, 40.98780306, 41.1339276 ,\n       42.42640687, 42.67317659, 43.38202393, 40.52159918, 41.82104733,\n       41.37632173, 40.75536774, 41.24318125, 46.27094121, 43.16248371,\n       41.85689907, 41.71330723, 41.48493703, 40.74309757, 43.93176527,\n       45.0555213 , 43.02324953, 46.16275555, 42.75511665])\n\n\nWe can even do some matrix operations (which are obviously not done elementwise), like dot products.\n\nnp.dot(x, x)\n\n146360195\n\n\nNumPy also has useful attributes, like np.pi.\n\nnp.pi\n\n3.141592653589793",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#scipy-has-even-more-useful-functions-in-modules",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#scipy-has-even-more-useful-functions-in-modules",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.9 SciPy has even more useful functions (in modules)",
    "text": "N.9 SciPy has even more useful functions (in modules)\nSciPy actually began life as a library of special functions that operate on NumPy arrays. For example, we can compute an error function using the scipy.special module, which contains lots of special functions. Note that you often have to individually import the SciPy module you want to use, for example with\nimport scipy.special\n\nscipy.special.erf(x / 2000)\n\narray([0.76597747, 0.8549794 , 0.7948931 , 0.80965587, 0.86074212,\n       0.79209865, 0.8236209 , 0.79740973, 0.78433732, 0.77904847,\n       0.81905337, 0.83227948, 0.79915793, 0.76597747, 0.7936263 ,\n       0.80676772, 0.8021292 , 0.82316805, 0.8276577 , 0.79690821,\n       0.80506817, 0.76597747, 0.76262579, 0.76514271, 0.76846912,\n       0.79690821, 0.8021292 , 0.81673693, 0.7543863 , 0.78381257,\n       0.77393853, 0.75980693, 0.77094188, 0.86995276, 0.81227529,\n       0.78459935, 0.78143985, 0.77636944, 0.75952376, 0.8276577 ,\n       0.84883448, 0.80941641, 0.86814949, 0.80384751])\n\n\nThere are many SciPy submodules which give plenty or rich functionality for scientific computing. You can check out the SciPy docs to learn about all of the functionality. Particularly useful modules that have come up in our work in systems biology include:\n\nscipy.special: Special functions.\nscipy.stats: Functions for statistical analysis.\nscipy.optimize: Numerical optimization.\nscipy.integrate: Numerical solutions to differential equations.\nscipy.interpolate: Smooth interpolation of functions.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#numpy-and-scipy-are-highly-optimized",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#numpy-and-scipy-are-highly-optimized",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.10 NumPy and SciPy are highly optimized",
    "text": "N.10 NumPy and SciPy are highly optimized\nImportantly, NumPy and SciPy routines are often fast. To understand why, we need to think a bit about how your computer actually runs code you write.\n\nN.10.1 Interpreted and compiled languages\nWe have touched on the fact that Python is an interpreted language. This means that the Python interpreter reads through your code, line by line, translates the commands into instructions that your computer’s processor can execute, and then these are executed. It also does garbage collection, which manages memory usage in your programs for you. As an interpreted language, code is often much easier to write, and development time is much shorter. It is often easier to debug. By contrast, with compiled languages (the dominant ones being Fortran, C, and C++), your entire source code is translated into machine code before you ever run it. When you execute your program, it is already in machine code. As a result, compiled code is often much faster than interpreted code. The speed difference depends largely on the task at hand, but there is often over a 100-fold difference.\nFirst, we’ll demonstrate the difference between compiled and interpreted languages by looking at a function to sum the elements of an array. Note that Python is dynamically typed, so the function below works for multiple data types, but the C function works only for double precision floating point numbers.\n\n# Python code to sum an array and print the result to the screen\nprint(sum(my_ar))\n\n[19793 20111 20171 19978]\n\n\n/* C code to sum an array and print the result to the screen */\n\n#include &lt;stdio.h&gt;\n\nvoid sum_array(double a[], int n);\n\nvoid sum_array(double a[], int n) {\n   int i; \n   double sum=0;\n   for (i = 0; i &lt; n; i++){\n       sum += a[i];\n   }\n   printf(\"%g\\n\", sum);\n}\nThe C code won’t even execute without another function called main to call it. You should notice the difference in complexity of the code. Interpreted code is very often much easier to write!\n\n\nN.10.2 NumPy and SciPy use compiled code!\nUnder the hood, when you call a NumPy or SciPy function, or use one of the methods, the Python interpreter passes the arrays into pre-compiled functions. (They are usually C or Fortran functions.) That means that you get to use an interpreted language with near-compiled speed! We can demonstrate the speed by comparing an explicit sum of elements of an array using a Python for loop versus NumPy. We will use the np.random module to generate a large array of random numbers (we will visit random number generation in a coming section). We then use the %timeit magic function of IPython to time the execution of the sum of the elements of the array.\n\n# Make array of 10,000 random numbers\nrng = np.random.default_rng()\nx = rng.random(10000)\n\n# Sum with Python for loop\ndef python_sum(x):\n    x_sum = 0.0\n    for y in x:\n        x_sum += y\n    return x_sum\n\n# Test speed\n%timeit python_sum(x)\n\n877 µs ± 9.01 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nNow we’ll do the same test with the NumPy implementation.\n\n%timeit np.sum(x)\n\n7.84 µs ± 99.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWow! We went from a millisecond to microseconds!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#word-of-advice-use-numpy-and-scipy",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#word-of-advice-use-numpy-and-scipy",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.11 Word of advice: use NumPy and SciPy",
    "text": "N.11 Word of advice: use NumPy and SciPy\nIf you are writing code and you think to yourself, “This seems like a pretty common things to do,” there is a good chance the someone really smart has written code to do it. If it’s something numerical, there is a good chance it is in NumPy or SciPy. Use these packages. Do not reinvent the wheel. It is very rare you can beat them for performance, error checking, etc.\nFurthermore, NumPy and SciPy are very well tested. In general, you do not need to write unit tests for well-established packages. Obviously, if you use NumPy or SciPy within your own functions, you still need to test what you wrote.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python/intro_to_numpy_and_scipy.html#computing-environment",
    "href": "appendices/python/intro_to_numpy_and_scipy.html#computing-environment",
    "title": "Appendix N — Introduction to Numpy and Scipy",
    "section": "N.12 Computing environment",
    "text": "N.12 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.10.9\nIPython version      : 8.10.0\n\nnumpy     : 1.23.5\nscipy     : 1.10.0\njupyterlab: 3.5.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  }
]